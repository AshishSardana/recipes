{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPTCache and Weaviate ♻️\n",
    "\n",
    "This notebook shows how to configure GPTCache to use Weaviate as the set vector store."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gptcache import cache\n",
    "from gptcache.manager import get_data_manager, CacheBase, VectorBase\n",
    "from gptcache.similarity_evaluation.distance import SearchDistanceEvaluation\n",
    "from gptcache.embedding import OpenAI\n",
    "import weaviate\n",
    "import os\n",
    "import sqlite3\n",
    "from gptcache.adapter import openai\n",
    "import timeit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use OpenAI for the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_embedding_fn = OpenAI().to_embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SQLite to cache the requests/responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/gptcache/manager/scalar_data/sqlalchemy.py:20: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "cache_base = CacheBase(\"sqlite\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See what is currently in the SQLite database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_cache_content():\n",
    "    cursor = sqlite3.connect(\"sqlite.db\").cursor()\n",
    "    tables = cursor.execute('SELECT * FROM sqlite_master WHERE type=\"table\"').fetchall()\n",
    "\n",
    "    for table in tables:\n",
    "        # Print the table name as a delimiter\n",
    "        print(f\"Results for table {table[1]}:\")\n",
    "        print(\"------------------------\")\n",
    "\n",
    "        # Execute a SELECT * query for the table\n",
    "        cursor.execute(f\"SELECT * FROM {table[1]}\")\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Print the results\n",
    "        for row in results:\n",
    "            print(row)\n",
    "\n",
    "        # Print a blank line to separate the output for each table\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for table gptcache:\n",
      "------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The database is currently empty\n",
    "\n",
    "dump_cache_content()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = os.getenv(\"WEAVIATE_URL\") # URL to your Weaviate instance\n",
    "api_key = os.getenv(\"WEAVIATE_API_KEY\") # authentication key -- ignore if you don't have this configured\n",
    "auth_config = weaviate.AuthApiKey(api_key=api_key)\n",
    "vector_base = VectorBase(\"weaviate\", url=url, auth_client_secret=auth_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Weaviate client to query the database outside of GPTCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client = weaviate.Client(url=url, auth_client_secret=auth_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create class and test connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_class = \"GPTCache\"\n",
    "weaviate_client.schema.get(class_name=weaviate_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm the Weaviate database is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaviate_client.data_object.get(class_name=weaviate_class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_manager = get_data_manager(cache_base, vector_base)\n",
    "\n",
    "cache.init(\n",
    "    embedding_func=openai_embedding_fn,\n",
    "    data_manager=data_manager,\n",
    "    similarity_evaluation=SearchDistanceEvaluation(max_distance=1)\n",
    ")\n",
    "\n",
    "cache.set_openai_key()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "In `similarity_evaluation`, we set `max_distance=1` to make the similarity threshold calculation \"work\" using this evaluation metric and cosine similarity (Weaviate's default similarity metric).\n",
    "\n",
    "References:\n",
    "\n",
    "1. [Calculating rank threshold](https://github.com/zilliztech/GPTCache/blob/03a059704443961ae5b6ca243e3edc2dc15aeb2a/gptcache/adapter/adapter.py#L98C1-L107C10)\n",
    "\n",
    "2. [Applying the rank threshold](https://github.com/zilliztech/GPTCache/blob/03a059704443961ae5b6ca243e3edc2dc15aeb2a/gptcache/adapter/adapter.py#L158C1-L176C18)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the time it takes to query the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeit_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Time the execution of the function\n",
    "        start_time = timeit.default_timer()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = timeit.default_timer()\n",
    "\n",
    "        # Print the time taken\n",
    "        print(f\"Time taken to run {func.__name__}: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timeit_decorator\n",
    "def get_openai_response(question):\n",
    "    # Call the OpenAI API to get a response\n",
    "    result = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": question}],\n",
    "    )\n",
    "    # Extract the response from the API result\n",
    "    response = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    # Return the response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first ask \"Who is Barrack Obama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who is Barrack Obama?\"\n",
    "\n",
    "get_openai_response(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's ask the same question again and note how long the response takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_openai_response(question)\n",
    "\n",
    "# notice how it went from 4.12 seconds to 0.56 seconds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's rephrase the same question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrase_question = \"Tell me more about Barrak Obama\"\n",
    "get_openai_response(rephrase_question)\n",
    "\n",
    "# This question is very similar to the above question and the response time is still very quick"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's look at the content stored in the SQLite database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_cache_content()\n",
    "\n",
    "# The question and answered is stored in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weaviate_client.data_object.get(class_name=weaviate_class, with_vector=True)\n",
    "\n",
    "weaviate_client.data_object.get(class_name=weaviate_class)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples that didn't perform very well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Starting with non-fictional characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_question = \"Who is Joe Biden?\"\n",
    "\n",
    "get_openai_response(new_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_new_question = \"Who is Taylor Swift?\"\n",
    "\n",
    "get_openai_response(another_new_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_openai_response(\"Who is Miley Cyrus?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trying with fictional characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_openai_response(\"Who is Antman?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_openai_response(\"Who is Spiderman?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why are Barrak Obama, Joe Biden, Taylor Swift, and Miley Cyrus semantically similar?\n",
    "\n",
    "\n",
    "2. How can we tweak semantic caching so that \"Who is Barrack Obama\" and \"Who is Joe Biden\" are semantically distinct? Do we need a more sophisticated distance evaluation metric?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
