[
    {
        "8lyA3mf7FjY": "thank you hey everyone thank you so much for checking out a new weviate release podcast releasing webiate version 1.15 this release is shot with all sorts of exciting new things from cloud native backups improved stability for high memory setups faster Imports for ordered objects more efficient filter aggregations two new distance metrics two new Eva modules and then smaller improvements and Bug fixes so it's a long list of exciting new things to me I think it's so interesting seeing these Concepts in computer science like red black trees and how do they impact database systems how has it been added to alleviate and all these other exciting things Eddie's written this awesome article on the go mem limit and how that's improving the stability for high memory setups and then this Cloud native backups so Eddie and thank you so much for coming on the web podcast to discuss uh version 1.15 hey Connor thanks for having me yeah it's a super exciting release um so many cool new features when we initially set out to to build this 1.15 release uh the the main plan basically was around backups but now we have this long list the features and I think every single one of them is a great reason to to upgrade but yeah let's maybe start with backups uh there the the kind of Point yeah everyone needs backups right I mean it's kind of a requirement to run your your database in production or any state full loads such as database and uh you could do backups before you could kind of do them before because you could do them at an infrastructure level basically manually deviate writes its files on disk so you could take a snapshot of that particular disk and store it somewhere dude with your your cloud provider but that that's not a really smooth process this would basically would give you a vendor lock-in for that specific vendor so you couldn't really migrate you would be kind of left or you would leave vv8 in this kind of weird state where it wasn't really ever prepared to to take copies of files or unrestore you would get these like weird error messages did vv8 crash instead trying to recover um yeah so it was It was kind of possible but it just wasn't a very good user experience and deviate is all about user experience so now we've kind of done the exact same opposite and now we have a backup feature with the user experience that we're absolutely proud of with the the 1.15 release you can backup and restore it to any a cloud provider or to your local file system so we have support for AWS S3 or any S3 implementation actually doesn't have to be the AWS one it could also be an open source one for example um for for Google Cloud GCS for a local file system and basically these can all be be plugged in using v8's module system and then you can just do a single command we have support for this and all of our our clients and all of our language clients or you could just send the raw HTTP request to start a backup and and that's it basically in bb8 will do all the cool things in the background and it's been engineered to be minimally intrusive basically so you can keep using Vivian in fact we encourage you to to do this in production and basically keep running vv8 in production send requests have your users use the machine they can even send right requests in the way that it's architect architected without any kind of impact and yeah the backups will just run in the in the background and then you can restore them either onto the same machine or even onto a different machine if you like yeah and I think that the ux the design and the documentation that shows you how to do it is so well communicated and really you know massive credit to how you've written the how how we V8 stays running all the time how you can select certain classes and I want to get right back to the ux but this little story about not backing up your data I really like this uh this HBO show Silicon Valley I imagine like a lot of our listeners maybe have seen this show and enjoyed the show and there's a scene where where Richard is he hasn't backed up the data they have like this insurance client Dan Melcher and and they're all panicking that we you know we have no backup of our data in them and it like they're they try to like migrate their server to Stanford and it falls apart but then the systems back itself up on the refrigerators and it's this really funny scene in that show but kind of communicating that urgency of like you need to have your data back exactly you do and nvv it gives you a proper way to do that now no refrigerators involved at all yeah and so one thing about the ux is I was curious about this notion of backing up specific classes so I imagine I have like 60 classes and I only want to back up some of them and the ux makes it so clear of how you include certain classes and can you tell me about the design decisions behind that part of it yeah yeah absolutely so this is actually uh mainly based on user feedback so the entire feature was uh developed around users input like of course you you ask your users before you write the first line of code um and then you try and try and get feedback and this is something that that was very very heavily influenced from Community feedback so basically the idea behind the the backup system is I I think there are so many use cases that you can do like you can even use it to to migrate between environments and these kind of things but generally I see two main use cases for a backup like one is the disaster recovery case where basically the disk is lost or or someone deletes it or this kind of basically to to take all the data be very agnostic of what data is on there and just take the whole thing basically back it up and restore it that's one option the other option is basically more at a yeah logical level where you know what your specific classes mean and you maybe you want to create a backup not so much to protect against Hardware failure or or cloud provider failure but against user errors so for example um a user could accidentally delete data just through the API so if you want to protect against that then you would do a sort of uh yeah I would also do a backup but it would be more to to protect against user actions and more this kind of application Level backup and there you might not want to back up the entire database for example of course vv8 can be used for for all kinds of different use cases and then I know of some that use classes basically as their their isolation unit for multi-tenancy so they would have their own service that that runs for their customers and each customer would produce one or or not Direct but basically their application would would create one or more classes inside VBA that belonged to exactly one customer so maybe they have a requirement maybe for for um sock 2 or gdpr or something that customer data needs to store be stored in a specific way so that would be one option for example to just create a backup for a specific class and the API is designed in a way that gives you three options basically you can provide no information at all this way it would just take a backup of the entire instance with everything that's that's on there or you could set an explicit list of classes so then basically only what's included in the class would be backed up or you could do the other way around where everything except basically we have this exclude field and you can say like okay um don't exclude this or don't include this class basically exclude this class because it's sensitive for something and then it would create a backup of all the other classes um so see I said like different classes might be you need more secure more like let's back this up every hour or so and then other things it's like well we need to back this up put it behind some security because this is particularly sensitive yeah I think that's really interesting great Point yeah think about all different the timing timing plays a bit role like you could have a class that rarely changes but is maybe very large why would you back it up all the time or you could have another class that yeah that frequently changes and you need much more frequent updates so yeah that's also a nice way to decouple this a bit basically and just give you all the flexibility to to Really um yeah from a technical perspective it's just a single call but you can do a lot with it you can customize it in whatever way you want uh one thing I'm kind of struggling to wrap my head around is um so when you back it up what is the kind of state look like and does it differ from like S3 because S3 is just like a file storage thing right or uh so when it's backed up is it like uh Json or file of data what what is the backup thing the the backup is almost an exact copy of um what you would have on your disk in VBA normally so it's basically the the binary file formats for for the LSM trees for the the hnsw index for other index types that we have so so for bm25 for example there's a a simple index that tracks a property length because you need these like average property lengths for the vm25 uh combination and and other basically metadata that we need for for maintenance so it's almost an exact copy of those files and then there is a snapshot I think it's actually called snapshot Json but that's that's basically just an implementation detail that gives you a bit of info about the the backup itself so it's like what what kind of machine was it running on um when was the backup created what's the the ID of the backup and these kind of things and that's basically the the metadata that that makes this backup a sort of self-contained backup I would say and and what that what I mean with that is basically you could create so let's say you have two instances one is your production instance um and the other one is let's say your local instance which is completely empty and now you want to migrate basically using using backups your data from the reduction instance to the cloud provider and then basically pull back the the um the backup from the cloud provider then all you have to do is send the request from the production instance put it into let's say S3 in this S3 bucket and then on your local machine as long as it's configured to to point to the same S3 bucket you can restore that exact same um backup just by specifying its ID and the the local instance doesn't need to know anything thing about your production instance so you don't have to do any like pre-configuring basically the the backup is completely self-contained with everything that it needs to to be runnable this so one thing I'm very curious about is with say the collection of demos and on uh GitHub we've yet examples we have things like you know uh clip demo with the with the UI and searching through movies searching through wines all these examples and I love this idea of getting wevia demos of here's a graphql API that starts through Wikipedia search through archive maybe complete with the UI also with with this backup because I'm thinking about how I think right now you clone it locally and then spin it up and have it running would instead just kind of restoring from a backup be an easier way to run the demo yourself yeah absolutely so so basically because the the backup and restore is yeah it is just copying of files that also means any kind of index building that you have to do that's already that's already contained um so let's say you have this this massive instance where you have hundreds of millions of objects and took you maybe over a day to index it if you create the backup the backup is just copying the files expensive we have all the the index structures that have already been built and then if you want to restore it so let's say you want to yeah run a demo case and and we've basically just put the demo data set into a backup and you restore from that that would yeah that would make it a super fast process because you you basically just wait for the time it takes to copy the files and once they're there you can start it up and you can use it yeah wow that's incredible then you avoid the import time and just back up wow so I think that was a great coverage of backups and for people listening we have you know these seven different things that I outlined in the beginning that are going to be chunked up in chapters and Dirk is going to be coming on in the second half to discuss uh the red black trees in this thing but the next Topic at Ian could we talk about the go mem limit you've written this great article how does this help leave you yeah thank you for for bringing up the the article um the the go mem limit is one of those features where we really benefit from the goal language Community being active and improving go itself so a lot of the improvements that we do in viviate they're typically code that we ride where we've done something that maybe wasn't ideal before and now we've we found a better way or we've just improved it in in general so so for example the the um memory uh improvements or allocation improvements for filtered aggregations that you're talking about with Dirk later um they are are something where basically we've just written better code but go mem limit is one of the those few things basically where we didn't have to do much other than um compile the binaries for um for the new 1.15 version with the latest version of go because this is a new feature that go actually introduced and go memed is basically a soft memory limit and soft means that that go cannot prevent your memory from growing beyond that limit so it's not a hard limit basically the kernel has a heart limited does an out of memory kill if the memory is exceeded and that's basically that's the hard limit but it's it's a Target and a Target is super important for a um for a garbage collected language because by default in go the target would always be twice of what you have so to put that in simple terms let's say your your application currently uses two gigabytes of memory and and you want it to use to give us like that's not accidental or or temporary allocations but that is let's say you want to keep your vectors in memory and this is two gigabytes of vectors then with that Target being to double so it's called 100 in in Gold language basically it's a go GC equals 100 and that would mean that your instance can grow all the way up to four gigabytes and that's relatively small numbers so that's that might still be okay you might have a four gigabyte instance but now let's say you have this massive setup and your stable memory is 200 gigabytes most likely you don't have a 400 gigabyte instance just to have a buffer for for temporary allocations you would have let's say maybe 20 overhead and you would have a 240 gigabyte machine now if you set your your um uh go GC instead of to double to 20 that would work for this particular case but a you have to know exactly where you end up which is the 200 gigabyte you have to know exactly how much you have and that means that to get there you already had a basically very aggressive garbage collector because it was set to to 20 which means you've spent a lot of time on garbage collection which might not be something that you you would want to to do and go mem limit basically is the the missing turning knob here it's actually not turning up to maybe sort of sort of a knob that turns itself because it basically turns the the other knob the gochi Cena which basically in a um in a single sentence the closer your memory usage gets to the Limit the more aggressive it makes the garbage collector so in the beginning on your 240 gigabyte machine you have all the memory in the world there's no reason to save memory so go can for example double the Heap every time that's fine but at 220 gigabytes or 200 gigabytes with 240 gigabyte limit you can't double anymore and this is exactly what go limit does in the background it would then make the the go garbage collector quite aggressive and then you would never sort of run out of memory again when you think you shouldn't have because you've calculated your stable memory requirements and it was all fine and um yeah long story short all you have to do is set a single configuration environment variable which is this go mem limit and you just put it to however memory you have and if there is a way that the go runtime can can prevent it by basically making the garbage collector more aggressive it's got your back and it will will take care of not going over there what it can do of course is if you if you keep on importing and you have vectors that require 260 gigabytes of memory on a 200 gigabyte a 40 gigabyte machine that will still run out of memory but for those cases where you kind of accidentally ran out of memory it will prevent those and I think that's that's super cool for having to do nothing but but yeah upgrading to the latest version and setting a new environment variable yeah super interesting I I'm not going to claim to be a knowledgeable about go really but I've seen like these Cuda om errors where you try to put a big gradient through the thing and add a memory programs crashing and so I can I can Reason by analogy there where you're trying to import too much data and then it crashes compared to this thing that can manage it and I I feel like that's such a huge thing that things would say um like gradient accumulation in deep learning is just kind of an analogy that I'm going to be using to reason about this like the way that it prevents you from the oom error has been such a game changer with training big models with you know big batch sizes and it sounds like a very similar thing with this and so from my understanding it sounds like um you know with the aggressive garbage collector it might slow it down when you get to that limit maybe because it's uh so I think it's what you do is maybe you over you have like a 32-bit about like placeholder right the temporary allocation is trying to kill those to make them for more data yeah yeah so the slowing down is is kind of an effect what it does is basically it speeds up the garbage collector itself so so basically the garbage collector the or the point of the garbage collector is to to for for any kind of memory that was used temporarily um in a garbage collected language is not freed immediately basically the garbage collector just runs in intervals and collects all that memory so that it can be reused again and the longer it it doesn't run basically so the longer the pass between two cycles the more memory piles up and if you're getting close to your memory limit then it basically has to run more aggressively and that kind of automatically does that trade-off where you only have this many CPUs and if your CPUs do more garbage collection that means they do less other things so yes you you kind of trade off a bit of of compute uh Power for more garbage collection power and and therefore prevent running out of memory if it's preventable so does this help with say uh like large-scale load tests get a billion vectors into Eva does this kind of thing facilitate that yeah yeah exactly so so the the higher your your regular Heap use or I think I use the the term uh long lift versus versus temporary so basically the the higher your long lift Heap or memory usage isn't that is exactly what happens in in such a billion scale case because those billion vectors they will use I don't know including the index something like one or two terabytes probably depending on the dimensionality so I think that the Sif 1B data set is something that we've played around with and I think it's it's a billion uh vectors at 128 Dimensions um yeah and that's that's four bytes for per Dimension so that is not going to calculate them in my head on the podcast but that is a lot and I think if you take into account the the um the space for the index itself we ended up with something like one terabyte or 1.2 terabytes and yeah if you have these kind of kind of massive limits then it really helps if you can set your go mem limit so that the garbage collector doesn't need this massive overhead because even just 10 is 120 gigabytes at 1.2 terabytes and that's a lot wow yes super interesting you've got three main topics to discuss on improving the performance of Eva so Dirk thank you so much for uh coming on the podcast to explain these new ideas yeah thank you for inviting me happy to be here so could we start with uh what is a red black tree how's it different from a regular binary tree and how does this help with vva's performance uh I would just stop by suspending what the binary tree is so let's say you have a bunch of numbers and you want to find out if a number is on your list and um like classical approach would be you just have a long list and you go through it and check everyone at every number and that's slow because you might have to check each entry before you know it and what you can do is you um sort this into a tree so let's say you have numbers from 0 to 100 you pick a middle note let's say 50 as the root note and then if you want to check if 7 is in your in your tree you go in and say okay yeah fifty Seven is smaller than that so I take the left entry and the left entry might be 33 and we go okay seven is smaller than 33 I go again in the left three until you are at the bottom and they're found to seven or not and that lets you check if the number is inside with a lot less steps then compared to just going through um to a list now um if you create these trees you can have problems um if you enter objects in order let's say the tree is empty and you start entering entering numbers in it and let's say you start with with one then one is your first object the root of the tree then you enter two then okay two is larger than one so you go to the right and the right child of one is now the two and now if the three comes you go in three is larger than zero and so larger than one go to the right note which is two three is larger than two you go to the right and enter it there and you can continue on that until you're at whatever numbers you are and then your binary tree is basically a list attitude to find any object you start at the first and you're the right to write the right to write the right right until you're there and so it doesn't improve the uh performance when when looking up things and also when writing objects into the street and now the uh well black tree is a self-balanced tree so there are a bunch of rules associated with the tree and then you rebalance the tree when certain things happen as of course the example for example the example I just gave it's you have then you enter the tree so you have done three nodes on the right and zero on the left so you're just like this rotate it around that the two is the new roots and then you have on the right the three and on the left the one and then you have a gaps of one instead of a depth of two and that makes it then faster than entering and reading notes from it super cool and yeah so around the 1.15 release I will have a blog post it'll probably be an in description of this video and it's released that'll visually describe this concept of how you rotate the trees the red red black coloring and maybe you're aware of things like visual algo these these ideas of um you know the red black coloring how that lets you balance a binary tree before we go a little deeper into how binary search trees help with databases to look into IDs can you tell me about your experience implementing this in weediate um like how what what do you change what kind of like in the code base the kind of Open Source database engine this kind of topic um so we already have a binary tree in there where you when you insert the objects at first and um basically the change was was limited to the existing binary tree implementation that we already have so basically I implemented checks are the rules um I just can't think afterwards is any of the rules are not developed anymore and if yes do the appropriate rotation changing values around um to to have it balanced so the red red reflectory itself is an idea I think from the 70s or the 80s so it's around since quite a while um but it works great so yeah so all this is implemented in go have you been how long have you been programming and go what was your journey like learning it two months so I started I started working on really AIDS um yeah early July and I I started I don't know maybe two three weeks before it was my first girl college um but but it's just surprisingly fast like I was um answer it first I'll just put the government like it's really quick that you get you took out that's a great experience super cool I think that's really interesting that it takes took you two months to learn go and this kind of Open Source database you know upgrading little Parts like binary tree to red black tree these kind of updates can we can we maybe step a bit into the motivation of the red of the binary tree uh so so we say we have these uuids they uniquely identify every item in our database could be images you know passages paragraphs whatever you put in your vector in the web Vector database so how does so the uuids they get sorted in in the tree is that the key idea for when you like how you use this kind of structure um would be an example let's say you have a uid that is um just a counter that goes up and then if you insert it you would get exacted behaviors I described earlier that you insert the first object that is your root insert the object that's your right side that object it's your text right child and so on and that just really degrades the performance when you add objects and makes it makes it very slow because every time you use an object you have to go through all the objects you added before before you can write it and for the next object it's even longer so it gets really slow and yeah so I kind of have two things I want to take this in but first so so say um you know if you're looping through your data in on the client side say you've got python code and you're looping through the data items and you're using that Loop counter to construct the uuid is that the kind of idea where your one two three four and such that you get this right except for anything going on the right now does this also play with say we have an inverted index and we have uh I don't know age uh or let's say some numeric value with a broader range than that let's say maybe uh um maybe we have uh income and some database I don't know we have a big set of numbers in the inverted index does the binary tree also help with that kind of lookup uh I'm sorry I don't know that like as I said I just started so I I don't fully understand the journey sorry let me set the stage better that was a bad example so so with the inverted index it's like um those the foundational idea is say you have a textbook and you're looking for biology you go to the back of the textbook it says biology is on page 99 page 305 inverted index so um so I think with numeric values we can also kind of have this uh inverted index where say let's say it's age and it's one to a hundred you know you'd say 80 and then your database is indexed where you have 80 and then the customers or whatever it is that have that value 80. could we imagine numeric values like that his age say it's like zero to 100 I think you need something like a key that identifies an object so you can like you can store as a value whatever you want so you could have 50 values with a given key um but I'm I'm not sure if you could use it for for what you have in mind so in general is you have a bunch of objects that haven't have a unique key and you want to insert or get them out of a structure quickly that's I think that that's where you use these these binary trees and red black trees are then if you have um ordered rights helps you to keep the performance up right yeah I can certainly imagine how to say um with wevia when you're doing a near object search and you pass in the ID of the object to reference the vector of that object having this kind of structure to quickly get it grab the vector search with it obviously sounds super useful yeah I think like I'm not totally sure about this but there is this I've got what is it called uh h n s w and did I get that right um it works kind of into the same direct obviously it's a lot more complicated but in I get I would say the idea behind it is similar but obviously it's more complex and um helps you to to find find it but this is like the implementation where I work on the 3D to to you get objects into vv8 you first store them and then you do other things later it's I don't think it takes part in the in the search or in the indexing data yeah that's such a fascinating connection between say tree structures for fast search or the proximity graphs hierarchical proximity graph structure of hsw the way that we have these like symbolic structures to organize data for fast lookup super interesting and I guess my understanding of it is you know if you do a near object search where you pass in an ID you can use the balanced red black balance binary binary search tree to quickly get that ID quickly get the vector then you take that Vector on the road to the hnsw index to now do that approximate nearest neighbor but so kind of stepping out of this topic can we now talk about the second topic on our agenda um fewer memory allocations on filtered aggregations could maybe begin with the filtered aggregations and and kind of understanding the current memory overhead and then kind of what's been done to help with that yeah um yeah so maybe we do it for the aggregation so we're looking for for a value that's for example Nia another like sorry an object entry in the database it's near another entry and um there was a lot of overhead involved in temporary allocations while doing this um I think something like so we we did a benchmark where we added one million objects to bb8 and then aggregation rate for example would just count how often is this property there and with this we had I think 200 gigabytes of just temporary allocations that would happen while looking for by checking how many other and um yeah there were a lot of different inefficiencies I would say in the code that that you could optimize um in general like this the the stack and the Heap um where you can store memory and go well in general and um if you start on this you can store things on the stack where you a know how big it is and B it doesn't escape so let's say you have a function you allocate some memory there and then you return it up the stack to whatever column you don't know what's happening afterwards so you need to have it at the place where it can dynamically grow or Shrink or the the lifetime is not limited and so it goes onto the Heap uh if you have a variable that you create in a function you only use it in that function and afterwards it's gone you can put it on the stack where it's much cheaper to create and to uh read and um so I would say there were three or four different patterns that I noticed the first one is um we the the objects we store them in Long byte areas so it's just a long area of bytes and then if you for example want to check or I want to read all the properties from it um then there was a library that was used it was called second binary.read I think and this just created lots of allocations for this process yeah so if you say oh I want to have um this is the the buffer from which I'm reading where the object is stored in I want to read the first 20 bytes because they mean the the ID of the object then you could tell that to the to that library and it would give you those first 20 bytes um and would create a bunch of temporary allocations by doing that um it's nice to use but that makes makes it slow and so I basically created a little low level library that just um reads from underlying buffer how many bytes you want without any temporary allocations and that's yeah then use that library at different places and and make it yeah somewhat user friendly um [Music] yeah yeah so I'm sorry to be you know I'll be re-watching this and hopefully wrap my head around the full the idea but um maybe so a filtered aggregation is say we're we have a hundred million passages in Wikipedia say we're trying to uh average out the word counts of them or something like that right is that it so that's a filtered aggregation uh yeah so so each Wikipedia Passage uh with its respective properties like word count is stored as like a byte string yes so you can better hash out the thing that you're trying to filter on um so basically you know do you have to hold object like the complete object is one long bite string and there you have to to get to the to the right play so you you basically get that long thing and then you know okay the first eight bytes are the ID of the object then there's a bunch of other things and then there is um let's say the length of the of the key of the object so you first you first need to go to the place where the key is that you read the key then you know okay the next 53 bytes are the key if you need it you read those if not you just jump over it and then you do that until you're at the place where the information is that you're looking for and basically with the old library you had to read everything and every read created many allocations and now this new library basically allows you okay um I want to read like I know the next value like the next thing I'm reading is let's say an unsigned in 64. so you do reads unsigned in 64. you get that value back and the internal pointer the points where in the buffer eyes jumps forward and you do that until you are at the right place in the buffer where the information is that you need super cool so it sounds like it would be especially useful if you have say a lot of properties like data objects that have several properties but especially benefit from this kind of technique and maybe one more thing I'm missing in my understanding is so so the temporary allocations so is that um so you copy the whole thing could you take me through it a little a little more sorry to be okay okay let's start from a bit let's say we have a an object that and the the buffer where everything is stored is 10 000 entries long and then what was done before is those 10 000 entries were given to that binary dot read library and they would say Okay read me the first four bytes read between the next 20 by three million next five bytes and so on and each of these calls the library created in the library so not in our code but in the library code a bunch of temporary allocations where they just created buffers for whatever reason like I haven't looked at their code so I don't know it and um basically what's happening now is you have two choices a is move the position inside the buffer forward without reading anything so if you know all the first 80 bytes don't matter because it is information I don't need you just say move position forward and the internal number that says omx by 15 is just moved 80 by 80 entries forward if you then say Oh I know now I'm reading uh answer integer 64. you call the read answer integer 64 call and it then reads the next uh eight bytes interprets them in a way that doesn't use any temporary allocation and returns you the value so having that Precision on um knowing that you're going to read a 32-bit value that that reduces I think that makes sense I so that reduces the uh because otherwise you need to allocate the 64-bit no do you always no um you always need to know what comes next so you have a you have a format and you know in in my long long white area that describes the um the the object like the first eight bytes are some ID then some other thing comes then this comes and then um the length of the t comes which isn't unsigned 32-bit value and so on and um so you always know what to read it's just are you doing it in efficient way or unefficient way basically and this this library was doing in an unefficient way I don't know why and like I haven't looked in that code but it um yeah it probably does a bunch of other things and for example it works for files and for any buffer which makes it really nice to use you can use the same functions on uh like reading a file and reading some byte area probably all's Network also I don't know that and now I basically I created some specialized function which does a subset of the the spinal.read library does but for us it does what we need at this place and it does it more efficiently wow so so I guess the high level takeaway is that you can you know do these aggregations with less memory requirements but yes I'm so fascinated at how you're able to kind of go into the database code and make these modifications even with you know two months ago and replace this library with this thing it's really opening my thinking of about this kind of like open source database and the different components of the databases that you can improve on you know from The Balancing trees for the sequential rights to the memory allocations on these aggregations is super interesting uh so we're not going to do our third topic um so with less memory construct consumption with importing many parallel bashes I think um you know just importing data I love this topic trying to better understand it myself so I'm so excited to learn about what's new with this yeah so um what we were doing up to now is uh like you have your clients and then you for ads data in batches to to vv8 and you for example do it over Network so you just have five clients that you run in parallel that each send their batches to deviate and that can processed there and uh what was done up to now is that for each batch that gets imported a number of go routines are started they do the actual importing so if the first batch arrives um we will start basically a number of CPU cores go routines and those girl routines then work through the to the objects that are in the batch and and add them to the actual database and the same happens for the second and the third the fourth and so on so if you have 10 batches in parallel you start 10 times your number of course goal routines that work in parallel importing those objects and um each worker has a memory cost associated with it so every worker that started um has certain certain things it does and does it memory so if you have then 10 batches you have that 100 times and um so I've got that number from etn I haven't measured it myself but like he tried to import many objects I think he like I think he said the billion but I'm not totally sure if it was a billion and at some point on a really big machine um like 40 of all memory that's used was from these workers and at a certain amount of time a certain point like the import doesn't get faster if you add more gold routine so you just add more memory that gets allocated and used but the total throughput doesn't get higher anymore yeah wow this is incredible the I mean the thinking around if I want to import a billion vectors into vva that I can distribute it across say 10 like uploaders could we talk about that idea a bit more so um because it would speed it up it kind of reminds me almost of like distributed training with deep neural networks and the same kind of you know distributed data uploading what else goes into a distributed data uploading with I mean imagine the red black tree balancing it isn't it probably needs to be some kind of synchronization step right like I mean the the that's a different point so first you you kind of Center the objects to vv8 and then you um put each object into vb8 and um and you have clients let's say the python client and that just has overhead and you you can't send the data from one client fast enough to to saturate like a big server but you have BBI running and so if you instead of just having one client sending data you have two or three clients then um you come to a point where you really use up all the resources that the the um the the server has so in my local test on my machine there is no real point in having two or three in parallel that doesn't make it like maybe a little bit faster up to a point but then it gets lower but if you have this this remote setup so you're sending over over Network you have latency you have throughput issues then it can help to have multiple batches in parallel so so maybe we could um Step through so the the memory overhead of each of the clients what what is it that the clients need to store that creates the overhead oh it's not the clients but um it's the these workers that take the object and actually add it to um to to vv8 and they have some temporary structures where they where they uh like that's one of the parts I have looked at so I I don't know it yet but um they have some temporary errors where they check oh um I already looked at a bunch of other objects and now the new object is different but yeah I can't really expanded the story about that one and um so basically what we've done is now instead of starting a number of go routines with each batch that comes there's a shared a pool of girl routines that that works on the object of all batches so let's say you have 10 clients sending data you're sending batches in then all the objects from these batches get added to one queue and then the shared workers just take the first increase at the database as soon as one is done it takes the next one so you always have the same number of of workers adding objects and it doesn't like the number of broadcast doesn't get up uh doesn't scale up if you have more batches and so you you are limited in the growth of of memory usage so if you have many batches because you don't have this many parallel operations that are going on could you tell me Limited in the um in the memory of the of the patch can you explain that one a little more yes um so each worker that adds the objects to the database create some temporary structures as I said before I'm not totally sure what they contain but they they need some memory to do their work so if you have 10 you have 10 times those temporary structures if you have 100 you have 100 times those temporary structures so um if you know say okay I'm limited it to 10 or 20 workers that go in parallel you limit the amount of memory they use for while importing it and um probably that as kind of a trade-off so if you have more goal routines up to a point it gets faster let's say you would change just one worker one gold routine work in parallel then it would be much lower then you have five if you have a five core machine but if you are going to 50 then on your five call machine the 50 is going routine won't help you adding things faster but it will still consume memory but you can't use any other things so the the client is Freer to go get the next day so the network part is faster and so when you have 50 that um no like the the clients stay the same the networks stay the same what's different is how deviate like in deviate how vv8 accepts the objects and then adds it to the database so vva test instead of having a hundred goal routines working paralleling the object that only has 10 or 15 or whatever you put in uh okay because it needs less temporary structure yes you have less gold routines working parallel and deviate so you have less of these temporary objects adding uh use so you have less over memory consumption so then as a result can you add even more data to it like um it scales a bit better if you have um if you have many batches then it's a bit faster not too much but a little bit I think the most important thing is if you're adding lots of objects in parallel then just your memory is is growing so much that you're running out of memory at a certain point and like this is this is what goes away or it's lower so you basically you can import more objects with the same amount of memory super cool so so I think that was a great coverage of the three topics of you know self-balancing red black trees to improve the sequential rights fewer memory allocations and filtered aggregations and as we just finished up on less memory consumption when importing many parallel batches um so quickly touching on a couple other uh topics in the 1.15 release we have two new distance metrics uh hemming distance in Manhattan distance uh Eddie could you quickly explain what they are and how community members have contributed this yeah yeah so so first of all the fact that Community members have contributed I'm super proud of that fact because for an open source product it's so nice to get these kind of outside contributions and um yeah users having those use cases and knowing that they they need them and then um they don't just have to put in a feature request but they can also just do it themselves that's really nice and it's really cool to to see that that kind of community usage uh yeah so so Manhattan distance that that's one that I haven't actually used myself before um but I like that I like the naming I think an alternative names like taxi cap distance and the idea is that you you have a grid or like you City grit as you would have in Manhattan and you can't just walk straight through a block so basically the only way to navigate from point A to point B is to actually walk the the uh the grid basically in the grid then would be your your axes in the coordinate system and that's basically what the distance is so instead of taking the diagonals you would in in euclidean distance you just take the sort of walk along the axes and um yeah I have no idea what use cases are are there for what models use them but apparently there are some because community members like them um then Hamming distance is one that I'm a bit more familiar with because that's also also used for binary passage retrieval um we're basically Hamming distance just gives you the different uh bits or bytes or depending on on what it runs on if it runs on numbers then gives you the difference between the numbers of each vector and um yeah on binary passage retrieval the the general ideas that you compress this entire Vector into this kind of binary thing and then instead of let's say with cosine calculating the entire angle basically you just compare you just walk through all the bits and see are they the same are they not the same and if all the bits are the same then the distance would be zero the vectors or the vectors the binary vectors would be identical and basically the the worst possible score that you could get is that every bit is different um so I guess it's in computer science terms it's basically an xor kind of calculation between the the two um vectors and uh yeah if all are different than your maximum distance basically the number of bits or if it's on a number the the vector Dimension and then that would be like the the most farthest apart vectors um yeah um binary passenger retrievable Ascent would be one of the the use cases yeah the new distance metrics are so interesting I think so I think we've covered five things now with uh you know cosine similarity being the foundational one and then kind of dot product where you remove the normalization from cosine similarity and euclidean and Manhattan which are similar kind of analogs of each other and now this Hamming distance thing which I think is pretty unique I remember doing bioinformatics classes where you look at like the RNA of mutated coronaviruses and so you'd have this edit distance with the acgt vocabulary and so the edit distance and yeah the maybe the binary passage retrieval may be looking for these connections and you know Eric has been really studying these distance metrics and trying to prescribe things for webia users like you know when to use which one when and so looking at things like accuracy and speed I think you have the little reasoning around the speed where the Manhattan should be faster because you just have the differences you don't need to square them some of the dot product you don't need to normalize them so it should be like a little bit faster but generally I think the prescription is uh you know just try it see and see what your feeling is our current thinking so we also have these uh two new hugging face modules or two new modules that I think integrate with hiking face correct me if I'm wrong and saying that they're kind of hugging face modules but can you explain what I knew about the new Wii date modules uh yeah yeah so one of those is actually uh completely hugging face specific so hugging face has an API where you can basically dissipate service from from hugging face and uh basically they host the model for you and they do the I think it's called the the hugging face inference API and that's exactly exactly what it does um so with vv8 modules or before all that you could do with with eBay modules basically spin the module up yourself but if you maybe already have a hugging face a subscription you just want to integrate with that because you're yeah you're already using that and you're happy with it um the new module basic features gives you an integration point to their API so this is we call this from the mediates perspective it's basically a third-party integration um because it's it's from a technical perspective it's just a network called to um this third party API um but yeah it gives you completely new flexibility because every model that is supported on unhiking face um you can run it through vb8 and um you can basically make use of all of the optimizations that the hug and face team has been been doing already so if you are for example self-hosting vv8 but don't want to self-host the inference part that would be one option and of course also with the VBA cloud service um you can integrate it as well so you can if you want a fully managed uh option then you can integrate as well and have basically the database part being managed by semi and the um inference part being managed by hugging phase and then the other module that we have which is yet another Community contribution which I'm I'm super proud of um it's a summarization module which basically it runs at at runtime or at query time so similar to our q a module for example um which I think we call the category readers and generators um and yeah basically it takes your results and if you want to it can summarize results for you into something into a shorter segment or so so if you um run your search results for example on very large documents but you want to preview them let's say in in yeah your search results page maybe that that's a very simple and very much search related example you could just generate such a summary on the Fly and display it awesome well thank you so much Eddie and everyone we have again this recap of cloud native backups improved stability for high memory setups faster Imports for ordered objects with this self-balancing red black tree more efficient filter aggregations and then the two new distance metrics and two new Eva modules uh so I'm like a kind of like a MC of a show now so following this this is the conclusion of our version 1.4 15 release uh now following is going to be some more information about Dirk uh how did he become working with edian on these performance improvements what is his background like what led him to be thinking about these things so I I hope you'll be sticking around to watch that part as well thanks for having me Dirk I'm really curious like how how is your kind of career development been that you you know you can identify these performance bottlenecks and develop things like this um I'm I was a physicist uh so the the physics Mazda PhD individual postdoc and that basically wrote um simulations about various barriers topics and it's just natural to to look for these performance optimization when you write the simulations and then my my last job was at Deutsche Barn which is like a train company in Germany and uh it's part of a team that wrote the simulation for microscopic train simulations so um yeah I did a lot more more architecture and general design things there was always this performance thing in the back or a part of of the work can you tell me a little more about uh like simulations simulation code and and kind of what what the experiments look like for that uh it really depends so let's let's I would I would go to university again because I think I'm not allowed to tell too much about my last job um so I did there was like cell simulation so how to sell moves how do they interact and so on and um in one of these projects for example we had a lot of of noise how the cells would move so that like we we were looking at the collisions between cells and um you know you have them on a 1D stripe and they collide with each other and we had a little bit of noise so they don't always go head-on but like this sometimes like this sometimes so they rotate a little bit and so you need a lot of um with the same parameter sets you need many runs so you can do some statistics if you just the wallet could be random the outcome but if you do a thousand for this parameter setting you know then oh this place happens 20 times this case happens 50 times then um you can put a statistic and you can compare different parameter sets so your code needs to be fast enough to like explore the relevant parameter space and have enough runs so you can build up statistics and so yeah it needs to be fast enough for that well yeah can I ask about um what inspired your interest in we get uh it's a cool project so I I talked to this with Etienne about it and it sounded really interesting and um I thought like I did simulations for 10 years 12 years something like that and it's time to move on and do something different and um like I was very like close to machine learning in my last project like didn't do it myself but with the simulation it was then used with machine learning and so I feel like I'm keeping this closeness to machine learning but not doing it myself and yeah also very interesting going from the from the big corporate company to a small startup oh yeah I'd love to talk about that as well if I could stay on one more thing with the what is the role of machine learning in those simulations that you describe um so University there was no machine learning so I did experiment a little bit with like detecting the outcomes but it was just like very very very simple um and then my last job basically the the goal of the project was to automate scheduling like long-term planning and short-term planning um so long-term planning sense of the next schedule how all of our trains drive to the network for the next half a year and we have these requirements of training companies that try from here to there from data here and so on and get a schedule where everything fits in and the second part short term let's say three Falls over you can't drive all those tracks anymore we need to reroute everything how can we do it um by staying as close as possible to the original schedule and making our passengers arrive as as close to original time as possible and so I was building with my team um this micro microscopic train simulations and then there was another team that used reinforcement learning to try to learn with the simulation how to create conflict-free schedules and to react to risk disruptions okay is there any scenario in which that would be like the the vva data uploading would be so chaotic that it would be an analogous kind of system or is it I thought a bit about it but I I I don't think like I I don't think there's a like a direct connection um I think possible learning like not too much into it but it's it's I think very Niche and special like I think their profit rate fits great and many way it doesn't um yeah so sorry so it's like these really complex scheduling algorithms is I guess I'm curious if like leviated they say there's like a million clients uploading data and then you got to synchronize it all get in the databases that maybe analogous or is it I don't think so no because um but basically the the the the uh the problems you have in the with the train system is that trains cannot easily overtake each other so the the um the order trains are in at a certain point in time is really important and you can have a train like I'm not using German cities but let's say one train starting in Munich another train starting in Berlin which is 500 kilometers away and they would need to use the same piece of drag in five hours so the the the order which will derived at that point like if the high speed train comes first so if the slow cargo train comes first really can mess up your schedule because if the high speed train is behind the the cargo train which drives really slow and can't be overtaken then you have to ISP train behind it going really slow forward and um that that's a real problem that can't be solved with analytical methods so that's why we went the the AI route I think this vv8 is uploading is you have a bunch of data you need to get it into the database and the order doesn't really matter and you just need to make it efficient and um yeah what do you think about this idea say we're we have like video data right this video data could be like you know a 30 second video is bullet train like it's you know it gets ready compared to like a three hour video where right would that kind of thing maybe be similar um I don't I don't I don't see a connection there sorry um yeah I um I think this this like this scheduling is so difficult because you have everything affecting every everything else so each decision you do at one point can have a day later an effect somewhere else that you don't know immediately and with with like adding things to bb8 the objects don't depend on each other you need to get them in without having bugs in your code like without overwriting anything else but besides that they're independent from each other and it doesn't really matter if you add a 30 minutes video or a 30 seconds video like the the process of writing it will probably take longer because you have more data but um I think besides that that it's not to pick off a connection maybe with the cross-referencing there could be some kind of because I think with the cross referencing the current way that you need to do it is you kind of have like a parent-child import where first you import uh you know say it's articles and then they have their paragraphs you need to first get the articles in there and then you reference the paragraph So maybe with these graph structures there's some kind of scheduling to how you orchestrate the referencing with uploading that could be like if it's really complex and you have like circular things that you need to break up and there it could be um well maybe let's say you're you're creating objects on the Fly and then you need to uh make sure that everything is in that you need and then a new object appears and then you need to reorder everything if you have that then it might be but um I'm not sure how realistic like how often that happens maybe but super cool so can you tell me about your experience at semi-technologies and you know just kind of earlier topic we previewed of working in a big Corporation compared to the startup yeah um so I was like in the in the startup in the big company but then the or should I say the big company came into the startup and and so yeah meetings corporate overhead and that was sometimes a bit too much and now I would say it's it's very lean you really work most of your time obviously can't you talk to your colleagues sometimes when you're there but you really have have time to work so that's that's a um a very nice nice change and like you still have a feeling that you can that you know everyone so like in the old like the crew like I think when I started with 60 people even over there left it was 250 so I kind of you lose you don't know everyone in the company anymore and now I have to play I know all the names and all the faces um that's the difference if I ask one more question about um sort of your your motivation and kind of do you do you maybe have like an application of we V8 like personally I love the idea of searching through scientific papers and I I find every little grounding every little thing I'm working on in that application to be very motivating uh do you have that kind of thing or is it just about the technical of you know where can I find a performance like I like personally I really like to to coach and to complicated problems I I guess um now when you set statistic papers if if you have a lot of results from science like you do experiments or you do simulations and you have just a bunch of of results that you can't really look at anymore you'll have a possibility to just like send it to bb8 and do some some basic analytics in there like as I said with my my simulations where you have um where you have the cells crushing at on and I really I look at a lot of videos and I wrote like by hand the script that oh if the cells are after colliding at this far away in this case and yeah that was a bit annoying so if you could automate that and happy read in there that would be really cool and I I'm sure in larger projects I I did it I mostly worked alone with one or two colleagues so we didn't add that much data but if you're in a big collaboration let's say um at 7 or something where they have billions of terabytes if you could help there I think that would be something that would really fascinate me yeah well it does not as thing I thought about but that kind of yeah like how cell simulations I've heard of like maybe like docking simulations I I don't really know too much about this but this kind of you're simulating a bunch of like um is it like electromagnetic interactions like that kind of thing so we read it like effective interaction so we didn't go into the the like the physical details of the electrons that that um don't want to be the same place but basically if the one cell is here and the other side can't be here and they have some chemical attraction to each other and um yeah repulsed each other at certain places so it was very um how do you say like high level so we didn't go into into the details because it's already like the the uh the models already complex enough so you really need to think about what can you put in what's like the the minimum level of detail that you need to have in there to to kind of simulate what's happening in the experiment um wow so you have like uh like a big molecule and some kind of chemical score to it rather than like an atom level now we did um like real like biological cells so and um we we simulate them as a as a blob and we used it's called phase field approach so it's basically a field that's between one and zero where it's one there is the cell where it's zero that's not and it goes like it's one one one one one and then it goes down to zero in like very small small um area and in this in this fuzzy in between phase that like it's the boundary of the cell and um then the these places from different cells can get close to each other and when kind of their boundaries touch you have like physical interactions so they they repulse each other and attract each other in a certain way and then um based off some chemicals instead of the cell you have additional chemical interactions and then if you let them Collide you have based on these interactions you have then the cells behave differently and and there were like interesting experiments about it and that's what we try to to reproduce those experiments yeah well that sounds super interesting like that like hierarchy of biology and the um that's the whole thing is super interesting well Dirk thank you so much for uh describing the new performance changes in we V8 version 1.15 and I really enjoyed getting to learn more about um you know your background and the things that interest you I really enjoyed this podcast thank you very much which I did too"
    },
    {
        "nSCUk5pHXlo": "hey everyone I'm super excited to host Eddie and dylocker and Parker Duckworth for the weevier 1.17 release podcast these releases are always so great it feels like such a celebration of weeviate and the hard work of the team to bring these new features uh into weviate uh so today we're mainly talking about replication and hybrid search and uh we're also welcoming uh Parker Duckworth for the first time on the weba podcast so we'll talk about ref devec as well a little bit uh so firstly Parker thank you so much for joining the wevia podcast yeah happy to be here awesome I think this release is just so special because uh you know we all got together in Italy and these having everyone in the semi-technologies team in the same room as our up on the Whiteboard at the slides and presenting these new features edian can you tell us about your experience with that yeah it's absolutely crazy so so we're a completely remote company but being remote doesn't mean that you can't meet up occasionally and this is exactly what we did so we had uh 28 I think uh or 27 or 28 folks in in Italy in the same room and uh we had a demo session planned for the last day of the week to to show our progress and I think by that point it had been about a week that I had last synced up with with Parker and red one who were working on the replication and and I was just sitting there and enjoying the demo and I almost get a bit emotional looking at the kind of progress that we've made because I mean like building a distributed database is the first for me it's probably a first for most people to do that and um seeing that come together and sort of seeing the the seed that that we originally planted when it was a super tiny team and see the the team grow and and come this feature come together and it I mean it's like it's such a such a serious feature in the end basically like when we started I would think like okay once we have replication that is really then we're in the big league then we have these these high availability and failover scenarios these kind of things and yeah seeing that happen absolutely unique experience can absolutely recommend it go to Italy with your company yeah it's just an absolutely incredible experience and and yeah the end at the last day uh Parker and red one gave this incredible lecture about a replication so uh Parker could kind of take it away and uh tell us about replication yeah absolutely so uh when I first started at the company we had um a milestone it was like a the last Milestone of the road map which was replication and it seemed so far away uh such a Monumental task at the time right we didn't really have anything um to to support that at the time that I joined so you know over time we've built towards replication um for example starting with backups uh first we introduced the ability to back up whatever's in your weviate instance um maybe on a single node and then that evolved into distributed backups which allows you to like backup a cluster and all of this was working towards the ability to automatically replicate or to support replication and finally we were able to build on uh the back of all the work that we had done with backups and introduce replication so it's something that we had in mind the whole time throughout the planning and development of backups we knew that we were building towards replication and so we wanted to yeah just build it up incrementally until we we got to this point so uh the really fun and interesting thing is that really the Capstone of the replication work uh I guess you could say was done in Italy so up to the point um up to that point you know we I hadn't met anyone in the team in person you know I'm based in the US and uh the rest of the core team is based um in Europe and other other places and so uh getting to sit uh specifically next to a red one another core team member and work with him in person to finalize this replication um that we wanted to build was super super uh interesting and and um they just made the whole experience so great so the the way that we decided to implement um replication we first looked at you know the cap theorem like what trade-offs do we want to make here you know do we want to prioritize consistency or availability or partition tolerance and so after discussing um many times with the core team and and um yeah red one and I discussing for for a while we um decided to make the trade-off for partition tolerance and availability uh similar to Cassandra um the thing with with weeviate is that um it's super read heavy so oftentimes the use case uh will be where um we'll insert like a large amount of of data up front and maybe there will be more inserts in the future but we want to prioritize uh read um the availability um so that being said um we decided to follow a uh leaderless um yeah replication algorithm so the idea is that a request will come into a cluster of Eva nodes and the node which happens to receive the requests will be um uh promoted I I guess you could say as the coordinator for that request and so um this coordinator uh also considers itself one of the participant nodes or one of the um yeah other nodes that it needs to relay like this this replicated data to so um the coordinator will uh participate in like a two-phase commit with the rest of the nodes including itself so a request comes in let's say a write request for um yeah a piece of data and then the coordinator receives the request and sends out um a broadcast basically like asking every node that is part of the replication replica set I guess you could say um to acknowledge the request has come in uh once that acknowledge one else comes comes back then um yeah the coordinator will actually send out the rest uh of the notes the data that it needs to actually commit or write to disk so um the advantage to going with this uh leaderless um algorithm is that it's more flexible in the case of node failure we don't have like a single point of failure with like a leader follower um algorithm so um yeah just uh makes things more flexible in the event of like node outages and things like that yeah that's so interesting to hear about I I love learning about these new database features in webia for example when the backups came out I had so much use out of that with my research on search features because as we're building out this beer benchmarks and we're going to talk about this later with hybrid search and our evaluation miracle and how we're evaluating this these bench these uh backups sorry have been so useful in I can upload NF Corpus arguana sci-fi all these beer data sets and then just back it up restore it run the tests and then you know we know our hybrid surge performance or whatever we're evaluating them so can you help me understand further uh the use cases of replication um when when people are going to need to use it yeah so the the biggest one is for reliability so typically with replication you want some kind of redundancy so for example if a note goes down and this is a Parker already said uh that from the perspective of the cap theorem where um prioritizing availability and partition tolerance and partition tolerance in a distributed cluster is basically a given so partition tolerance of this means in in this case means a connection to another node could go down or the node itself could go down and like from the perspective of a node the other node is down doesn't matter if it's actually down or if it just can't reach it so kind of a partition tolerance is a given and um then availability would mean that use cases can still continue and the cool thing with our application is that the consistency level that the user wants is tunable so um some of that is going to be part of 1.17 some of it is only going to be part of 1.18 so we're rolling this out in in phases basically but in phases that we believe make sense so it's not that one net 1.17 is like a half finished feature and then it's only complete in one not 18 but basically what you get in 1.17 is is complete and usable and then you get new features that still fall under the umbrella of of replication in 1.18. um so yeah with tunable consistency you can basically say um how much priority do I give to the availability of reads versus uh right so with 1.17 and every ride is replicated to all nodes that participate in that particular class so in in a sort of consistency level terms that's the replication level all and then every read or request that comes in um at least for for searches uh it is the with done with the consistency level of one so in other words to write data into the cluster all nodes need to be available but to read data any node could go down basically as long as you still have one and that's that's a specific configuration so in this kind of setup this would work well for um for example for uh search on an e-commerce application so you would say like hey products are updated only once a day so we only need to be able to write data once every 24 hours but users need to be able to search 24 hours and if something goes down they still need to be able to search so that will be a great use case for for the kind of uh read with uh sorry write with high consistency and read flow consistency kind of cases but there are others you could for example say if I ride with a quorum and read with a quorum then um you could tolerate node failures on on both cases or you could say I both write and read with very little a consistency or minimal consistency basically and then you could tolerate a lot of node failures but also you could risk that data is out of sync so basically you have this eventual consistency kind of aspect where you say okay for my use case I can tolerate it if some data is not there at some point but it needs to be there later on so this is the the high availability kind of use case which from my perspective is the the most requested reason or the most the biggest motivator in in replication but there are others so for example what you can also do is you can use replication to scale your read throughput so again to stick with e-commerce uh Black Friday let's say the kind of traffic that you expect on on Black Friday is five times the kind of traffic that you would expect during during regular day or regular week you don't want to provision your cluster for that Peak load that you have one day of the year you want to have your cluster sized appropriately for the rest of the the year and then scale it up basically just for for that one or two or three day period And this is some things you can do with replication so you could say my replication factor is three for most of the year so I have some redundancy but maybe now I scaled up to five or or 8 or 10 or 15 because then I have 15 identical replicas that could serve the traffic and basically this this gets linearly so 15 nodes could serve five times the traffic that three nodes could serve and finally there's another one and this is sort of more on the roadmap but that's the the multi-data center kind of replication so for regional proximity so you could have two places on on Earth so maybe I think we've used that before in agent SW graph example so let's stick with that we have Frankfurt Germany and um I don't know Boston uh USA so um if you had a data center somewhere in the middle it's kind of a bad example right now because we're in the middle of the Atlantic but that's okay let's let's assume there was Data Center in the Atlantic um then the kind of latencies that users would get would be pretty much the same for the for the users in Boston and for the users in Frankfurt because they have the same sort of regional distance to the data center but no one has a real good latency because no one is close to the data center so we could say let's move the data center to Boston super for the Boston users not so great for the Frankfurt users but now with multi-dc replication which you could also do is you could have a data center in Boston you can have a data Center in in Frankfurt and each of those users contacts the data center that's close to them so they have good latencies but then of course the data centers need to stay in sync as well and this is basically the multi-data center replication this is something that's not yet present in 1.17 also not going to be part of 1.18 yet but it's something that the kind of architecture that we've chosen in vv8 that is supported by the architecture so this is something that if you want to have it I think we have a feature request ticket for it on GitHub already so upload it and then we'll build it yeah I love how you gave the example of kind of the um rewrite trade-offs with consistency levels when when you want to use each thing when I think that's always a super important thing to understand yeah the the distributed systems it's so interesting to learn about it reminds me of our podcast people are you know for a binge on wevia podcast we did another podcast with Eric Bernice and on a running Vector search in production and it reminds me of that topic of yeah what it takes to you know scale out your e-commerce store so you can handle Black Friday I think in general it's so interesting um so also quickly I want to touch on this um this iterative release and Eddie and you've recently written a very popular blog post on product engineering I think now would be a great time to kind of touch on product engineering and you're thinking around these iterative releases and the general kind of philosophy behind how you think about these things yeah so so uh for those of you who haven't heard of product engineering yet it's kind of the the merger of product decision and Engineering decision so it's kind of blurring the lines between traditionally you might have a product department or a product team and then you have the engineering department and these these parties hate each other and they don't talk to each other and don't collaborate um and the in product engineering the ideas that you you soften these boundaries and you have collaboration so in a startup to me this makes a lot of sense because you have small teams and if you have a team with like three developers you can't afford to have a dedicated product manager and maybe a dedicated lead developer and then who's going to do the work basically they have like a a manager to engineer ratio that just doesn't make sense so something that naturally involves in those kind of settings is that yeah you blur the lines and you maybe have an engineer who takes over a couple of product responsibilities or maybe you have a product manager who has an engineering background and can do the the kind of prototyping themselves and um yeah you have easier communication more collaboration and have something that I would say yeah sort of is is is more productive and and feels more natural and you don't have these kind of artificial boundaries between the two um for our replication release um I mentioned that before already we're really really interested in the feedback that we're getting and we're really trying to to provide value early but value but really value like not just sort of give you something half finished so like hey we went for a minimal uh implementation so that you could have it sooner but don't use it because it's not really meant for production and that's kind of not what we're trying to do but instead return to say like okay what is a combination of features that you would use or what you would need to use it in in production and then maybe this only works for 80 of use cases maybe it doesn't work for the remaining 20 yet that's fine because the remaining 20 they'll be covered later on but that's no reason for us to say to the the first 80 hey you're gonna have to wait because we're only going to release it when we once we have 100 covered so that's kind of the idea um of doing that in iterations and of course we get the feedback when something is out we well yeah it gives us the ability to still pivot and for this to to sort of tie this back into product engineering it's super important to have that that feedback cycle and not have like artificial steps in between where someone has to relay a message from like one Department to another department then in the end you have Engineers that never talk to users um but it's the complete opposite basically like everyone in in uh the UVA core team or in other teams can be a member of our or not can be a member but typically is a member of our public slacks so they communicate with Engineers right away and then if if new ideas pop up through that we we discuss the ideas internally and it's not like well no the product manager said we're not going to do that but it's more way more collaborate yeah incredible and I think this is a great transition to start talking about hybrid search and our philosophy and the overall how we've developed it and so on and so maybe let me set the stage by describing what hybrid search is so uh hybrid search describes combining keyword scoring methods with Vector search methods so let's I think we're all pretty familiar with the vector surge part that's where we encode data with machine learning models build up a vector index and search for the approximate nearest neighbors and that but now let's kind of focused a little more on the keyword scoring methods so sort of the foundational algorithm is tfidf term frequency inverse document frequency where you score some sentence like I'm super excited to welcome Ed Ian and Parker Duckworth to the podcast based on the kind of uniqueness of these terms in that query with how unique it is to the collection of documents that I have so then going from TF IDF to bm25 bm25 introduces this binary Independence model it's you don't count how many times the keyword is going to appear in the document just whether it appears or not you similarly normalize it for the length of the the document so it's a bit of a modification to tfidf and it's another way of scoring these documents based on the keywords that has been really successful so then we have these two uh search algorithms and so now with hybrid we're combining the the results from each of the lists so we're going to dive a little further into the rank fusion and then also say bm25 and the extension to bm25f but I want to come back to this um product engineering and Eddie and I want to ask about I thought with this project you did such a great job of leading the team this was one of the projects that I've been a part of since joining we V8 where there's been like it's like a task force almost like this is our project this is your responsibility this is your responsibility and then we've just kind of come together and it's almost finished and it's so exciting so can you tell me about like your initial thinking around the development of the hybrid search Project yeah that's that's great to hear by the way that's that's really nice yeah so so uh for our listeners to to understand a bit sort of how we structure that internally um we have the core team itself which basically builds vv8 which is kind of a lot of what we do but by far not the the only thing that we do and then uh Conor is part of the the research team as well so besides um like the podcast and and other sort of several activities there's also the the research part and what we consider research and research that the term research depending on what your background is this can have very different definitions or it can be have very different meanings um but we use research in the sense that we say we've identified an opportunity somewhere something that we will most likely want to add to alleviate but there is some kind of a question that we need to answer first and this question could be something as simple as what is the best ux to to integrate this into our apis like how would our users want to use it like do we want to give the user a lot of control or do we want to maybe abstract something so this this could be a question it could be a question of how are we going to build it so especially in in um so you mentioned uh rank fusion and then the scorebase fusion and these these terms this is basically something that that you know way better than I do and something where we benefit so much from from um yeah having these kind of kind of collaborations within the company um yeah so this sort of how do we build it what do we need what do we need to figure out how to to be able to build it could be an evaluation also something like does this idea make sense like it looks good on paper but what happens if we try it at scale let's try it with 10K objects a million objects maybe a billion objects thus it does it scale does it fit into into deviate in that sense and this is something where hybrid I think early on WE identified that there's an opportunity and um said like okay let's let's get started let's see what it is let's see um what do we need because hybrid sort of in a sense you need the both the building blocks for hybrid both The bm25 Surge and the the um the vector surge you need to have both Vector search obviously is kind of what bv8 is about so we can safely assume that we have Vector search covered bm25 is something that we gradually started building um it was actually TF IDF in in sort of sort of the the the simple building block for bm25 but I think from the indexing perspective it's actually the same or it's like one or two parameters need to additionally be indexed for bm25 that is something that we actually had in mind in the very very first prototype that we built so we we didn't have any apis for for TF IDF or bm25 but we had the inverted index early on I think over over two years ago we added the inverted index to to bb8 and it already had this this and Parker you may have come across that in the in the code whenever we put those buckets we had like buckets for with frequency and without frequency and the word frequency is so basically for all the text properties we We additionally uh to to indexing the word we also index the frequency and that was in preparation for that whole TF IDF pm25 step so we kind of knew that it might be something that we want to add at some point um but we also have to figure out like what is what is the real value of it and um if I'm a hundred percent honest something think that that I don't know at this moment is Will hybrid search play role five or ten years from now it could easily be the case and then I I don't think anyone can confidently end could be the case that semantic search just keeps improving so much that hybrid search basically is more of a stopgap solution at the moment to bridge a gap and in the Gap being exact keyword match in and out of domain search or it could be that while it still improves hybrid search is just always going to be better because it's the the combination of two things and this is something that that yeah I I don't know and I don't think anyone really knows but something that I find super exciting and yeah quote quote me on this five years from now and let's see let's see how it turned out there seems to have been quite a bit of Buzz about hybrid search in the community as well I think in our our slack Community I often see people requesting this feature or asking when it's going to be available or being excited when they hear that it's going to be available soon I think uh yeah it's always awesome to see people asking for things and then have it delivered that's so cool and I think um so in preparation for weeviate air Eric and I were coming up with a demo and I I think this example of how to catch an Alaskan pollock that query is a great example where you have the semantic meaning of catch you don't mean catch a baseball catch a cold you mean you know fishing and then Alaskan pollock which is a specific keyword and then that kind of fusion I I think what you're saying editing is very interesting about um you know will the vectors be able to contain that kind of keyword Centric focus in the future and I kind of think so also especially with like say the way that Colbert would re-rank with token representations but I think in addition to that uh this rank Fusion thing that we've been exploring will be very useful I can imagine combining it with the wear filter and near Tech search to have this kind of boost so you know saying recommendation or sir like you're in an e-commerce store again uh it's Black Friday and you go for rugs and it's like you know two thousand dollar for a rug three thousand dollar for a rug and so you also want to have where prices less than 300 but then you want to have like the fusion where you also show the extravagant for rugs so you could have that kind of rank Fusion so I think that rank Fusion is a core primitive of search pipelines that we've explored in this particular feature uh one other thing and then I really want to dive into ref devec with Parker and uh is this um how we've been benchmarking hybrid search and it comes back to the backups and I I think this is just so exciting for the development of levia and our features and our connection to the science is we've been uploading the beer benchmarks to eviate and we have the ndcg the recall scores and I think it's just an incredible exciting step for us um so I'm kind of curious editing if I could ask this kind of a question about like your thoughts on sort of the beer benchmarks and just sort of these kind of like academic information retrieval metrics and how they play with feature development yeah so the beer Benchmark is definitely more your area of expertise than the mine but I think this is this is exactly what makes this so great that we we now have something quantifiable as well as opposed to to just sort of it's it benchmarks are always reflective of some sort of scenario so you could set up in a benchmark in a way to produce some kind of a result so so let's say The Benchmark is primarily keyword based and probably a keyword-based algorithm is going to be better on it let's say it doesn't care so much about um like yeah specific unknown words but it matches a domain of a semantic search based model then you're probably screwing it towards that so so benchmarks are yeah or you always need like the asterisk for what what is the Benchmark meant to to show but nevertheless I mean that's not a reason not to to use benchmarks like it's it's very good to to be able to objectively say okay a is better than b whether that matches to a being better than b for a specific use case that is something that that users have to see for for themselves so that is why it's super important to me to have both approaches like the quantifiable approach but also the qualitative approach where he's like okay an actual use case and we we have our customer success team who deals with the the paid cases that we have uh for first semi we have the open source Community who sometimes share data or or give us some insight into what's working for them so the mix of both to me is super important that we don't just make these claims of chair picked examples but that we also don't do the opposite of saying like hey it's nothing is cherry-picked everything is scientific but then the user comes in saying like why doesn't it work for me yeah it's absolutely fascinating especially with uh I'm like kind of coming into the trending topic of the day uh chat gbt I don't mean to distract too much but this kind of ad hoc evaluation I did one query I like the result that means the system works compared yeah and that goes both ways right like you see the people saying like hey it's the best thing ever I only have positive results and you see people saying is the worst thing ever I only have negative examples yeah exactly and I think it's worth kind of knowing that these systems are a little different than the maybe traditional software cases where these Edge like machine learning performance is very like long tail to like hit or miss and I think the beer benchmarks a particular reason why I'm so excited about this particular work is uh the diversity captured in it it has you know papers about covid-19 it has Financial questions like uh are my personal taxes separate from my hobby income and then you have like nutrition questions about like are multivitamins a waste of money so you have this incredible diversity diversity and query length and I think we're also seeing the kind of intense this kind of explorationism This research is emerging as well where you'd say what is the intent of the search task and that kind of expiration um so yeah overall I just couldn't be more excited about the benchmarking I think it's such an exciting step for us so I want to play the topics I'm so excited to have uh Parker especially because he uh was so pivotal to the development of ref Parker could you start by kind of explaining what ref to VEC is and then I really to dive into sort of some of the questions that we've been seeing in our community chat like particularly clarifying um updating the references and how this kind of Cascades backwards uh thinking around like can we have custom aggregation functions but but maybe if we could set the stage can you tell us about what reftubec is yeah certainly so uh ref the back centroid is yeah a new module that we released uh recently and uh the idea of it is is that um an object which is set to be vectorized so to speak by rectifix Android um a vector isn't produced by this object itself but it's produced by like the aggregate of its references vectors so uh the ref to back module will take an object and then grab the vectors from all of its references all of its referenced objects and then we'll yeah compute a centroid with that um set of vectors to to to find something that's yeah similar to all of these things at once and and so the idea is that this is really useful when you want to represent something um as an aggregation of other things right for example um users uh based on their likes right what can we what can we uh show to a user that is something that aligns with what their their Express interests are and the rest of that centroid is something that's that's perfect for doing something like that yeah exactly I think um the the sort of the most obvious use case I think is the kind of bipartite graph recommendation case where you have users products uh the user liked a few products and now you represent the user with the uh with the vector from the products and then that's the query Vector to recommend new products with I think that really helps uh just get the idea quickly um yeah I think um I want to kind of stay on this topic of graphs and weviate a little more I have sort of my story of coming to weviate is um you know I when I had first talked to Bobby's it was Ted talked about how he was really interested in the semantic web in ontologies and then sort of shortly after we had that podcast I went to New York to meet Laura at the knowledge graph conference where their you know companies like tiger graph relational AI where they have these tuples and so I always kind of had this thinking that like we V8 is going to have this focus on the graphs sort of opening this up and maybe adding you could start them can you tell me about kind of the the motivation behind this cross-reference design because I think it's so powerful so under like I don't want to say underappreciated but I think it's maybe not hyped enough like this kind of design of having cross-references the way it lets you do multi-vector the way it lets you do multimodal I think it's such a powerful part of the data schema design and webiate yeah this this goes back a bit to the history of vb8 because there was a phase before we called ourselves or before we called bv8 a vector search engine um because we we we're sort of trying to figure out like what is it that that that alleviate can add or where it can add the kind of value and at that point um bb8 was no database on its own yet but bb8 was sort of thought as a layer on top of other databases and at that point we're actually planning on running the deviate on top of graph databases so we had an integration for what's called genus graph um a tool that I had not heard before and also kind of haven't heard of since but I think it's it's like it's a super Niche tool super good at what it does but also like a like a relatively small Niche and the idea of Janna's graph was that you build this itself on top of other databases so I think at that point I don't know if this is still true it was uh Cassandra and um an elasticsearch I think so sort of like store the data in in Cassandra and then uh query using using elasticsearch and and this enables you to yeah sort of build this like like super large scale graphs and then vv8 was basically the AI layer on top of that and originally the idea was was before we started yeah basically accepting vectors together with with objects to only vectorize the schema so one of the very first use cases was you have this these these knowledge graphs and they have different ontologies so so you would have a graph here any graph there and you kind of know that there's an overlap but because people didn't use the exact same words it was super difficult to to Really match like what is like in these two crafts like yeah they do intersect but you don't know how so the original idea was to use um yeah basically NLP technology early in LLP technology of the time to just figure out what the right schema is and then at some point Bob and I were on a call and this was super early on I think that the team was very very small and we were kind of figuring figuring out this video like what if we tried that same approach not on the schema but in the actual data and we were both like nah that's that's crazy like we can't we can't do that and and then we tried I was like whoa this this works kind of well and that was kind of the the step where where sort of this semantic graph ontology tool turned into a vector data Vector database so we kind of pivoted completely and and that was also the point when we started um don't want to say rewrite vivade because some parts of it like the graphql API for example still is modeled after after that original structure um but yeah it was kind of um sort of Shifting the focus a bit but at the same time our early users already had graphs that they represented with bb8 so they're like okay well we can't just abandon them we can just say like okay V8 now switched from sort of this semantic graph tool to a nosql document only search engine and now you can't represent your graphs anymore okay uh vv8 is probably not going to become a graph database because so if you in in architecture it's always it's always trade-offs I'm like what do you prioritize them said okay search is more important for us than than craft reversal so we kind of uh skewed the architecture towards search so the the agents W index and the inverted index and the way it distributes data across nodes and these kind of things so these are all set up for for search but we said we want to keep the cross references and the the cross references from an architectural perspective they're basically just links and and of course there's some couple of optimizations you can do when you resolve those those links um but yeah that's that's kind of the the history of why they're there and um and and now sort of it's it's an enabler for new use cases basically yeah that's amazing I I've always wondered like the vectorized class name thing now makes so much more sense to me with the context of that and that's so interesting um so I really want to dive into the technical details uh Parker could you tell us about like because I we're seeing this question about kind of people want to understand exactly how if they have a to B to C and they update C uh will it Cascade back like that this kind of question uh it seems to be something that people are curious about yeah absolutely So currently the um the only way to update an object's reference Vector uh is by updating that object itself the parent object which holds the references so let's say um object a references object B and C and object A's uh Vector reference Vector is the centroid of bnc's vectors uh if B or C are updated a is a reference Vector does not change right now we don't have any sort of back Channel mechanism that allows that information to to reach the object which references those vectors and primarily uh it was because this is our first iteration um this is something that could be very computationally heavy if for example um we have tons of these reference vectors around so uh currently the the only way to update an object's reference Vector is to update that object's set of references directly so that can be done either just by posting an entirely new object or I guess you could say put in a new object with the same ID and a new set of reference vectors or deleting some references from that object or updating that object's references one at a time but basically the only way to update an object's reference Vector is to mutate that object set of references on itself whereas updating one of its references directly is not going to affect that parent objects reference Vector yeah and that idea of the kind of yeah when you really chain out these graphs and there's kind of like the bipartite graph I originally designed described where you can have like multiple edges uh you maybe also have it going back and forth kind of if you imagine data like that but like multiple classes chained together I think the aggregate functions are going to be that's going to be something that we can really explore and as edian said in laying the future for what we can do them and maybe I even want to just work this in there because I'm happy to have gone so last night I went to mit's learning on graphs conference and it was firstly yeah it was super cool to be at MIT it's super super smart people and just walking around here like nice I'm at MIT but like um seeing the research and seeing the thinking around graph neural networks it can be so intense about this kind of thing of like what kind of problems can uh deep learning broadly solve sort of connecting to like the Turing machines and uh you know like what problems can be solved like NP completeness can graph neural networks take that on but I think there's a big middle ground but for like the just a graph convolutional Network being used somewhere that's useful and I think just this basic idea of chaining these things together aggregating maybe that could be the use of that and we could similarly have a python app for our module inference the same way we have text-to-back uh you know all these things we have that kind of container for the like pytorch geometric Library so kind of pivoting and I know that the graph neural network aggregation thing is a bit intense but can we talk about like um what would it take to build in like custom aggregation functions maybe starting with just like having biasing it so that the mean centroid is uh most heavily impacted by the most recently added cross-reference yeah currently uh are only uh are only module in the class of ref to vac is rough centroid this was built purposely to be able to be easily expanded um uh into more more um centroid type algorithms or more algorithms to to um yeah calculate this reference Vector however you want to calculate it so uh weaving its module system is by Design very modular and so if we were to want to introduce something like this um most of the boilerplate I guess you could say the groundwork has already been set so it's just a matter of coming up with the way you want to calculate these reference vectors and then um introducing a new module which piggybacks this existing ref the back framework that we build within the weaving module system to to use this new algorithm to calculate the reference Vector so I would say um for any reference or ref the back centroid modules in the future it's um not a whole lot of work to introduce a new one it's just a matter of like figuring out how you want to calculate these these reference vectors yeah I'm just just sorry just thinking out loud um about that that recency bias because I think I'm not 100 sure but basically references in vv8 have a specific order we we don't ever make use of the fact that they have an order but on disk they're they're saved in order so most likely we could use that fact we don't have time stamps for for a reference so right now we couldn't easily do say something like okay from one to two was a very short time difference then from two to three it was a large one but at least we know the order so if we just want to give the most weight to the most recent one it would probably be as simple as giving the most weight to the last element of the array yeah super interesting I think um kind of one other thing that excites me and yeah that I think that the building blocks of that are in place and that will be super impactful just basing on that a little longer you imagine like you want to have recommendation without sort of logging in and having that long Archive of user data you want to just be able to like scroll through Tick Tock or whatever and like quickly have recommendations I think that kind of thing lets you like control it with your by giving the signal of uh recency sort of one other thing that kind of excites me is this idea of clustering the um the embeddings I think that could be super powerful especially for diverse interests so like if you've liked these products and it's like uh Nike shoes Adidas shoes Jordan shoes I think instead of averaging it we could have this uh clustering and then the centroids could be used which brings this topic of how might we represent centroids like and I think the cross-reference thing again is we would use it again to do multi-vector representation and that kind of idea so super cool I think um yeah this overall this is 117 and thanks so much for the discussion on ref to VEC I'm so excited about reftubec I think this kind of graph structure how we can send embeddings through the graphs and aggregate them I think a lot of people are excited about it because I think it's exciting but but anyways thanks so much I think um yeah replication hybrid search and sort of the Italy 117 release all of it yeah yeah smooth a few few smaller improvements as well um we'll mention them in our release blog post but um a couple of uh performance improvements regarding startup time so uh both for for startup times at the time that the application takes to to restart so for example if there was a a node failure and this is something that with replication we have a lot more tolerance for but even with replication you still care about the time that the node is back because there may be a time window when when it's unavailable so there are a lot of improvements around the the um yeah startup time but also and and this was sort of similar uh a similar cost for this we've also improved uh batch latencies and and they're particularly sort of the the P99 or or the max latency so we had a pretty constant write speed already based on the LSM store having a constant write speed but then we had like these occasional Peaks and that those could lead to timeouts and then timeout would lead to a retry and then the retry would put more load on the cluster in all these this kind of chain of events and we have a lot of fixes around those as well that we implemented in one not 17 which is sort of one of these for for me my favorite category like not a very exciting feature but super exciting for those that actually operate VBA clusters awesome well thanks again both so much for the for coming on the podcast and everyone please check out weave 1.17 and thank you so much again for listening awesome thanks Connor"
    },
    {
        "Q7f2JeuMN7E": "hey everyone thank you so much for watching another weeviate release podcast I'm super excited about Ouija 1.18 the big theme of this is being speed memory savings some ux improvements like adding filters to hybrid search uh so I'm super excited to welcome we've a CTO and co-founder Eddie and dylocker to explain some of the new updates to Eva 1.18 hey Connor thanks for having me awesome so we have such an exciting list from bitmap indexing and update to hsw with adding product quantization and then the wand algorithm for bm25 and hybrid search so can we kick things off by diving into bitmap indexing and what's new about that yeah sure bitmap indexing is is one of those things that the first time we saw it was almost too good to be true um I think at some point I I tweeted out where we had like a thousand times a speed Improvement on filter query and that was not just some marketing stuff that was really we had a filter in a specific combination on 100 million uh data set where a filter could take uh I think something like five seconds and now that same filter would take five milliseconds so it's really a 1 000 times Improvement and the idea is basically we use for for filtering we use the inverted index and the inverted index is basically the the ground truth of what is allowed or disallowed and then we pass that we call internally we call it an allow list we pass that allow list to the hnsw index and say like hey these are the X number of objects that you are allowed to to use in your search basically um and the problem there was if a filter suddenly matches the entire database on 100 million objects then you would have or you would spend so much time building up that filter from the inverted index that all of a sudden like even if the vector search for for a filter that matches the entire database the vector search doesn't really care right it's like it's almost the same as an unfiltered one or it is the same as non-filtered one if it matches literally everything in the database but that mere fact of building up the filter that took so much time and then we thought okay what what's going on what what is uh happening with our current implementation and what could we improve and we came across roaring bitmaps which is basically a a perfect sort of fit because bitmaps or bit sets in general are made exactly for this like is something contained yes no and uh it works with with basically with increasing IDs which is exactly what we have because with the inverted index already so perfect fit only problem there was um that sort of if you store your your inverted index in a specific way and then you convert it into a roaring bitmap which is what some other databases do then you still have that cost of storing it in a in a different way basically you still have that retrieval cost and we said hey there's this one thing and and that's that's another tweet that I put out where at some point I said like we made that crazy decision to basically build everything from scratch in nvidate and and that is one of the points where it really really pays off right now because we just put those roaring bitmaps straight into the storage layer like in our LSM store itself and that means we we don't have to do any kind of um and he kind of retrieve then turn into a bitmap so so so for example um if you you could do it that way if you had um say a classical storage engine that would just Store The Roaring bitmap then you could still sort of retrieve the Roaring bitmap and you could do fast intersections like for an and or or operations once you have the bitmap that would still be fast but you'd always have that that cost up front but because we could build it straight into the LSM Store that cost is completely gone and that gives us latencies in the in the single digit milliseconds for that that Filter part so I think on the the 100 million was um our evaluation case because there before we ran into those five second latencies and then on that same case now a simple filter that matches everything is something like a millisecond or maybe two and then even if you have like some complex operations like like range queries or or merging different filters this and that or that like with multiple operands that's all super fast right now which is is incredibly cool um one more thing to say about this um of course this is this is a very different way of storing and handling data in VBA in 1.18 than it was in 1.17 so listeners might now think wow what do I have to do like is that is that going to break my my entire VBA instance the good news is no it's not so if you don't do anything if you just upgrade to 1.18 it will just fall back to what it's already what it's already used to but then you also won't have the speed benefits but we also have a migration option which you can basically run one time you just start a bb8 with an option say like hey migrate my my index is to Roaring bitmap indexes and then it will run through your objects once basically with the first time it starts up and vv8 will be in in read-only mode during that phase because of course when it rebuilds the entire index then you can't have rights coming in but you can still use it normally it's it's only read-only and once that is complete a readout only mode is gone and you have the benefits of super fast filters in VBA 1.18 with your data that you imported in in 1.17 . yeah that's extremely interesting and yeah that detail is super cool that you don't that you can just kind of update it and um yeah so maybe kind of coming back into the application and motivating a little further I remember like reading your article about pre-filled about pre-filtered uh Vector search and 50 milliseconds and this kind of thing and I always thought initially this idea of adding symbolic filters to the vector search was such an interesting idea learning more about how you use this allow list structure with the hsw like how technically that's implemented so maybe kind of talking a bit about the applications and the tests a little more um I really like these things like when we interviewed Sam bean from you.com he described this idea of like a vertically integrated search engine so say you want to search through just Reddit or just Wikipedia that's one case where you then would put this filter on the vector search and maybe if we could just talk through some more examples of filtered Vector search and what that looks like yeah that that's a great example um one that I always think of because I think where where users are most used to filters is e-commerce like if you order something from a web shop you would let's say you would filter the price range because you want your object to match a specific price range or you would filter it by a specific uh product attributes so if you're looking for a new uh VBA t-shirt it has to be navy blue and then you put navy blue on it and then you'll you'll get the the right one um not not for me I only wear black shirts I guess um no but but these are these are the kind of applications where users are so used to applying filters maybe without even being aware of it because it's just it's just natural and then yeah as you said I really like this idea of having a massive data set and sort of narrowing it down on the Fly you're saying well now I just want to filter by yeah only only search through through Reddit for example and um by having that that sort of big Vector space and having the ability to set those filters on the Fly you also don't necessarily need to know up front right because you could also say if say a vector search engine wouldn't support filtering you could just say okay I'm going to split it up up front I'm just going to here is my collection for Reddit here's my collection for uh I don't know the next thing um and then you search them one at a time but that only works if you can predict entirely what these kind of combinations are but if you can dynamically sort of combine these filters and that is also something that that that vv8 already supports but that will be way faster with bitmap indexing just because it's so cheap such a cheap operation to do like an intersection on a on two bitmaps basically so you can do that dynamically so it gives you sort of more uh flexibility later on like yes there are cases where you maybe want to optimize something by splitting something up up front like I'm not not saying that those don't exist like in those hyper optimized cases for when you use case but in the general just throw something at it let VBA deal with it and figure out what you want to do later without being sort of blocked by a previous decision that's that's nice and that's going to be much faster now than it was before yeah that example of uh chaining together the wear filters is so interesting like uh Navy like a t-shirt navy blue uh price between 20 and 40 dollars I'm not sure how much a sure it costs but but we're using my shirt but yeah so another question I got uh when I was at the New York Meetup we get Meetup was about role-based access control uh would this enable that interesting interesting uh question yeah so so yes and no so um there if you want to do or it depends a bit sort of where you do your role-based access course so maybe for for people who aren't aware role-based access control is basically a very fine-grained Access Control mechanism where different users have different roles and then one role grants you rights to specific things so basically by default you have no rights to do anything but then you could have right it could have specific roles and basically through those roles you inherit permissions to to access something whatever that something is that could be something in in the context of of the application could be something in the context of a vv8 basically a specific object so if you do set such a property on your vv8 object and then in your application you would make sure that you um that you set those filters to match the specific either the specific roles or the specific permissions you could definitely do that with with those filters the sort of downs are maybe not downside is just so if it's a it's an engineering decision and it has trade-offs and basically what you need to do then is make sure that the user can never find a path where they can basically skip that kind of filter because like if if for example there was a way that they could all of a sudden have an unfiltered search and they would they would search across um basically everyone's data which is of course what you want to prevent so um depending on your kind of security requirements it may also make sense to have the the role-based access control directly in the database and not sort of on top of it but that that that's basically that that you need to decide that from a use case to use case perspective perspective of how you would want to integrate this this by the way also a question that also comes up um every once in a while and I've been meaning to write a blog post about it but I haven't yet is multi-tenancy because I guess sort of the the role-based access control is very similar to to the multi-tenancy case where you would have um say your users are grouped by something and then how do you represent those user groups or their companies or whatever it is that Associates them to one thing how do you group that in bb8 and typically um what we do there is we do it on a on a class basis because that sort of gives you and and that's why I'm bringing this up as opposed to doing it in a sort of Monolithic large class and then set individual filters if you have it per class then you have that separation like the the way that a class is created in mediate it's very it's like everything is isolated basically it's it's a separate folder on disk it's separate files on disk and there's basically no way that that one tenant could influence another attendant but that said multi-tenancy is also not the exact same thing as a sort of fine-grained access control so but but these topics overlap so I thought that was that would be worth mentioning as well at that point yeah that's extremely interesting so I think that's a great coverage of uh bitmap indexing um you know beginning with the technical details and then you know talking about the applications the filtered search and I yeah just I think it's important for people to know that you can filter the vector search it's not just Vector search you can also add these symbolic properties add things to your data like you would with any kind of data management system and you can achieve faster Vector search by filtering with the properties and that's probably one of the most exciting things that we've in my opinion I love this kind of filter Vector search thing uh so so then stepping into the meat of the vector search the vector index so H and swpq what does product quantization add to the table yeah such a such a cool big topic and I'm so happy that we we finally have the uh the first release so I think we we teased something late last year already when when all of us were together in Italy and this was one of our our presentations that we had there um where the overarching goal is basically to reduce the operating cost which is mainly driven by by memory consumption so sort of goes hand in hand like if you want to look at it from Tech perspective we want to reduce memory usage if you want to look from the business perspective we want to reduce operating costs but really it's the same thing um that was driven by by sort of okay how can we get the the usage down basically without without sort of any making any sacrifices I mean what you could do is turn off the vector index like that would that would reduce your memory index but it would also make it kind of pointless to to use VBA um so so yeah how can we give the same kind of uh uh quality to the user while making it a bit cheaper to run and there we have basically this this two-step approach to it in hnswpq is that the first step the two step or the second step starting with the the sort of longer goal is a fully disk based solution so disk based solution basically means what's on disk doesn't have to be in memory so therefore it's it's cheaper and ssds are fast and if you optimize it for ssds there are certain ways where you can you can make benefits off those disks and still have a very similar um experience that said there's also sort of like you you can you can never run any kind of program without memory right like if every little thing was loaded from disk the minute or the second or the millisecond or nanosecond that it was used it would become very very slow so you can basically put parts to disk and you keep some parts in memory and put some parts to disk and the kind of idea in any disk based system so so Abdel for example published this comparison of vamana which is what what Microsoft's disk and uses which technically is quite similar to hnsw because it's also graph based single layer graph instead of multi-layered graph but it's still still very very similar so basically with all of these systems you need to keep something in memory and and the idea is you keep sort of something small and cheap in memory and have the large inexpensive stuff put to to disk and now to loop back sort of what is the the small and cheap stuff that you can keep in memory and what you find in all of these these ideas is compressed vectors and this is where product quantization comes in so we had matiz Do's on on the podcast uh who which is basically just div dive into into product quantization so he explained this way better than than I could and then I could in in like 30 seconds but the general idea is that that product quantization is a form of compression it's lost full compression so it's not lossless compressions Lost full compression and you can tune it um in in the simplest case you would have um a single float 32 Dimension which was which originally in the continuous Vector is four bytes would be turned into a single byte so we you'd have a four to one compression but then you can you can basically compress it further by having a multiple multiple Dimensions per segment so you could get to like eight to one sixty to one thirty two to one um and this comes at the cost of dropping recall so as with anything there's some sort of a sweet spot which is if you if you want to have the exact same kind of quality as you have with hnsw right now then The Sweet Spot is probably four to one compression um but if you say well I care less about recall I care about running billions of vectors cheaply then maybe your personal sweet spot is at eight to one or sixteen to one or thirty two to one and where abdell is currently working on a blog post um illustrating these these kind of kind of trade-offs anyway long story short um compression is a vital part of any disk based system and we we thought well we we we're currently developing this disk based system and it has a certain timeline but can we create value for the user sooner than that and then we thought like well hnswpq is basically half of this picnics like the 50 of this skin and it has the compression it still has the the uh the vector index and the only difference really is that the the vector index is still in memory and not on disk but the vectors if they are compressed four to one then fictional example let's say your current memory usage is 10 gigabytes of those 10 gigabytes two is the vector index and eight are the the vector embeddings then um and now I'm glad I chose round numbers that are easy to calculate then if you compress those eight gigabytes of vectors uh by four to one and then they would turn into two so basically your um you still have the two gigabytes for the vector index plus the two gigabytes for the compressed vectors so instead of 10 gigabytes overall you now have four gigabytes so you basically had a sixty percent reduction in memory usage and four to one is such a low compression that you're probably not even going to notice the the the um the drop in recall so it's like almost almost free almost for free you've got a 60 reduction in cost and then said you can drive it even further yeah that's incredible and ABDO has done such an incredible job with this blog post I'm not sure if this is uh the blog post is published when we're publishing this but I highly recommend people checking this out uh this blog post really helped me firstly understand the memory requirements of hsw uh the blog post describes the calculations of uh with hnsw you have this Max connections parameter and then you have to store this number of parameter uh connections and with like n16 and describing the details of what that creates an overhead as well as the compressed Vector as your calculation with the eight gigabytes two gigabytes and that reduction um yeah and then I really liked what you said about like if your goal is just to run a billion vectors compared to a million with the recall trade-offs uh basically you have this decision of whether how you want to group the vectors so like if this is a 32 dimensional Vector you you know segments of two right and so that's that's how whether you increase that you'll compress it more but at the cost of lower recall and it comes back to that I think a n benchmarks thing you did with the you know showing the recall of approximate nearest Neighbors yeah I think all that is just so interesting um something I'm really curious about is explaining further the um the K means and how that's used to Cluster the vectors and kind of what the overhead and the thinking is behind that yeah so for for the compression algorithm basically to reduce the amount of data you need to sort of build groups you you just uh said that example of taking two dimensions and putting them into into like groups of two so then the questions becomes what identifies a group like how do you pick your groups because you could pick them very very bad basically where the distance between two points in a single group would be very large and then if you put all of your your groups served on top of each other in a graph all your groups would suddenly be identical and then you'd have the problem that that well what's the point of having this many groups if they're all the same so basically you need to to pick your groups for and without going too deep into what uh how product quantization works under the hood but basically you need to distribute your data in groups that have no overlap and where something that's within a group is actually similar because if you if you use that for approximation um then yeah if if it's dissimilar then basically you you that's why you recall drops and k-means is a very simple clustering algorithm to that basically does exactly that so if you imagine all your your vectors like a 2d graph plot it somewhere there are naturally going to be clusters and then k-means is just an algorithm that identifies those clusters and it's it's by no means perfect but um it has like a an exit condition basically where it stops running if it can't improve further um and then you have ways to to um sort of run it over and over again to to find like the the the best possible clusters and that is essentially what's used in PQ these kind of kind of it's like the I would say the initialization in PQ is is this K means uh or these K means clusters because these clusters then represent the individual segments in in um in product quantization and uh then something that that I think that will also be uh mentioned in uh abdel's blog post or has been mentioned depending on whether it's been published already um you don't need you don't necessarily need all your vectors to produce good PQ produce a good PQ code book or good uh PQ clusters so in the example that he runs in his blog post I think he uses 20 of the data and um then it runs under the assumption that the remaining 80 of the data is distributed in a similar way as the initial 20 and like this this assumption becomes true depending on the size that data set and of course if you if you think of um yeah if you think of your your data set in real life topics if your first 20 happen to be about um navy blue vv8 t-shirts and then your remaining 80 percenter all of a sudden about a car parts or something then that distribution that that assumption doesn't doesn't hold true but typically that's not the case like typically you have like a slice of your data that's somewhat distributed and then if you add a like a light blue t-shirt then it makes sense that that you would find the existing group for for the light blue t-shirt and then if you add a new uh I don't know headlight then the headlight the LED headlight would be close to the the whatever incandescent headlight that's already in the not sure if they're still used anymore but uh yeah it kind of runs under the assumption that um these clusters still make sense and typically they do which is nice yeah that that notion of like distribution shift and how it applies to Vector index structures are super interesting I've read this paper Ood disk a n that yeah touches on this problem and I definitely think that'll be something that uh to keep an eye on I I found that find the k-means thing to be so interesting uh like for example we were comparing k-means with umap on our Bert topic podcast and in the case of Bert topic you're trying to Cluster Vector spaces to understand the similarity between clusters so uh so k-means has a bias on trying to evenly distribute the space with the centroids which makes a ton of sense for product quantization because you're trying to compress the vectors with the centroid IDs so it works great for that whereas umap is better for like if you want to have some odd shape where Martin uses this example where it's like a circle with a dot in the middle and K means would just try to like cut that up rather than um you know clustering the circle around the dot so it's very interesting to think of the trade-offs between the clustering algorithms is very cool to see the k-means implementation in go Lang and leviate and seeing the clustering coming as a part of the vector uh analysis stuff as well so so awesome so I think that's so exciting the opportunities with reducing the memory requirements of vector indexes and yeah all that making it more accessible to create billion scale maybe if we like dream a bit about this like how much how much can these kind of savings help people with you know scaling up their clusters or the things they imagine doing but limited with the cost and thus needing optimizations like this yeah yeah I I think the the biggest uh sort of enabler or this is going to be a big enabler for cases that don't necessarily have a lot of traffic but have large data sets so if you think of the other data let's say someone's building a new type of search engine and that would hit millions of requests per second they would probably probably not mind that the setup is relatively expensive because just to be able to serve millions of requests per second you need large infrastructure anyway so it doesn't matter if the memory requirement is a bit higher because you have I don't know tens or hundreds of CPUs anyway so that kind of goes hand in hand but for those cases that are maybe and I think we also find them a lot in the sort of more analytical space and maybe on the research side where you have a massive data set but your requirement is not necessarily to get like real-time information if the query takes a bit longer you do it async that's fine your your goal is basically to get that information and then you can't justify having a cluster that costs like multiple thousands of dollars a month then you need a cheaper way and and in that case you're also probably very happy to to trade off performance because well you don't have that many concurrent queries and that is I think these cases are going to be enabled because you can easily just say like okay here 50 60 70 cost reduction and that's only the first step right like with um if we get the the go for for the full blown disc based solution then the reduction will be even further but already um I think we we can enable these these kind of cases where vv8 maybe right now would be sort of a nice solution but just isn't viable and then it becomes more viable for for these yeah that's super exciting uh super cool so if we're pivoting topics a bit uh we have two more uh like kind of update things then coming back into a speed Improvement another algorithm to talk about um could you explain the cursor API and what that adds to aviate yeah so we have I think in in this release and the other one we haven't talked about yet but in this release we had the two highest or most requested feature requests from our community and uh the second one that we're going to talk about later um is now the the highest the most upvoted one but before that was the case this one the cursor API was the the most upvoted one um and the the cursor API is if you think about it it's a super super simple construct but it's so uh useful so before the cursor API existed if a user used vv8 as their primary database um then at some point they they might have a requirement could be for for auditing purposes could be for something completely different but basically the requirement is how do I go or the question that they would ask is how do I get all of that data out of vv8 again and then in the past we would say well that's kind of like maybe it isn't designed to get your your data out maybe it is designed to to get your data in and then search your data and you can like if you find the right search queries you can find everything that's that's in that uh database but there was basically no no way to get it out and then um people would try to use like the pagination feature as a workaround but the pagination is is built in a way where basically each each higher page becomes more expensive than the last so you could do that to a certain degree but then typically after 10 000 elements you would run into some sort of an error message where V8 would tell you um you've reached your I think it's called query maximum results and you could increase that but there's also warning that that comes with higher costs and that you shouldn't necessarily do that and now if you want to get a million or 10 million or 100 million objects out of basically there there just was no way and then you could build a workaround where you would every 10 000 object you would assign a a field to a unique value so you could set like a a page property on your object like a fictional page and then every 10 000 you increase the page and then you could set a filter give me page seven or something and then you could get those 10 000 that would match the the fictional page seven but that's of course super complex so all of that throw it out of the window don't have to do it anymore now we have the cursor API the cursor API is a super super simple construct you basically start somewhere with an ID and if you don't know where to start then you just start with nothing basically with an mtid and you say give me a thousand objects from the database and then from that list a thousand objects they are in a specific guaranteed order and the last one you can take the last ID and then you can just take that ID for the next page so to speak to bb8 and say give me the results after this ID and then you get the next ten thousand and the cool thing is that this this query has a constant cost so it doesn't matter if you've already retrieved 500 million objects and now you want the the I don't know how how many if page basically to get the number next object it always starts a new sort of retrieval process at that point that you specify and now you can in constant time basically get every object out of your database which makes it we've added the backups feature which allows you to get every object out if you want to get it back into Eva basically but now you can also if you want to pipe it into a different system like you could you could use that to stream your data to something let's say you would have a specific or let's say you want to train a model Based on data that's in bb8 then how would you get all the data out of it you get well you can do that now with the cursor API and you could either do it in one go and basically just export it and put it maybe in a parquet file and use that or you could yeah do it on the Fly and sort of go through the cursor and do something with it yeah that case of training the model is what inspired me to click on the thumbs up on the on the roadmap thing and uh yeah it's very interesting like getting the data out like imagine like just for the case of you maybe just want to have your data somewhere else again just because it's your data and you want to have it as secure as possible and as effective as many times in as many ways as you can think of I guess but um this kind of idea of getting the data out for the machine learning models I think that's so interesting I I also think you could use we V8 to retrieve the next batch of training data kind of similar to Mosaic ml released this streaming data set thing and I think we could be used in a similar way to get batches of data for training so so yeah all of that I think is extremely interesting um so if we also come in now uh pivoting topics again um so adding the filters to hybrid search uh maybe first we could start with just like what it is and then I'm very curious what the technical challenges behind this were yeah so this this is the other one that was most requested so in in 1.17 we um released um or we had bm25 support before already but I think it was considered experimental and we removed that flag and then we said like here it's General availability in in 1.17 um but it did not support setting filters yet so it's exactly the the same kind of use case as you as we talked about before on unfiltered Vector search so same we can use the same example of the navy blue t-shirt or combine it with price tags and everything um and you couldn't do that on a bm25 search and in turn because a hybrid search is both a bm25 and a vector surge that also meant you couldn't do it on a hybrid search so in one at 17 we have this awesome new feature um of hybrid search which um we we see that it can improve the results running it over over a specific data set benchmarks to enable it you see that you can get a higher ndct score and higher recall and everything which is super awesome but now you're missing that kind of other functionality that Vivid already has for for Vector search so it was only a matter of time and um this was accelerated by I think our community of voting that ticket like crazy and it it really became the the most upvoted ticket um very very quickly and um yeah that's that's in Viva right now so you can now nothing changes from a search or if you let me put it this way if you've used a filter in vb8 before you already know how to use the new feature like it's literally the same wear filter and the only thing that's different now compared to 1.17 is that that error message goes away that tells you uh filtered search is not supported yet um and then now you can you can do it uh from an implementation perspective um filter filtered bm25 search is actually not that difficult because we already have the inverted index which we use for for Vector search which by the way also means that that these features that happen in parallel like if you set a filter that's uh that's using the bitmap index that we talked about before then all of a sudden these will also now be much much faster so there the good thing is that these two parts are a bit decoupled so that wasn't necessarily A a big change to get them working into vv8 however we've also completely Rewritten the way that we do bm25 scoring because in 1.17 the initial release was a um sort of a let's call it a primitive approach where we'd literally score every term that was matched um but now we've introduced the wand algorithm which is um yeah another way to to improve the retrieval a bit and then that tied in nicely with the filters because we just did that that all in once super cool and um yeah awesome so yeah that's very interesting how the like I was very curious like how the allow list kind of concept generalized to the bm25 scoring with the inverted index and the other connection to the bitmap indexing that's also interesting um so yeah so maybe stepping now into the wand algorithm um I I had to give Dimitri Khan credit for he's the first one who kind of taught me about this kind of idea where he's like oh you could probably have some way of uh sorting the terms so when in bm25 you have a query like how to catch an Alaskan pollock I like this car and you like um you know you start scoring how to catch with the matching and you can probably you know have some efficient way of scoring the documents like these have matched so many words so far that it's like highly unlikely that the match would be high enough so I don't know if that's the exact algorithm behind Wanda could you explain like uh what what the idea is with one scoring yeah yeah that's exactly the idea so you have in a bm25 index or in a in a let's say in a in full text search and this is something that traditional search engines have been been uh or have provided for for quite some time already now you can also get that in in viviate in full text search you have specific keywords so so in very simple terms TF IDF or bm25 basically rewards terms that are rare So in the how to catch Alaskan pollock uh the the um how to is probably super super common and doesn't really add any any value uh to the the um the search query but Alaskan already sort of narrows it down quite a bit so ideally in your search and so so maybe to to step one step back scoring everything that's in that term has a certain cost right so if we because we use the inverted index that means the inverted index will tell us every single document that contains a specific term so for for Alaskan that's very few documents compared to overall unless your entire database is about Alaska and I guess you know that that wouldn't um be true anymore but but then you you'd have another query in that term that would be or if your entire database is about phishing then yeah same thing so so basically there's always going to be frequent and rare and ideally you want to give more weight so to speak but I'm not talking about weight in the sense of of um changing the calculation because the calculation like the score is going to be the same but more weight in the sense of I want to tackle this first because it adds more more value and then maybe I can skip something else so that's why it's also called Dynamic pruning algorithms because you you want to prune the stuff basically that that you don't need so if in a classical inverted list or inverted index a kind of scenario the the database doesn't know upfront which terms which right like it doesn't have a sort of semantic understanding and it does with hybrid search because of the whole Vector search part um but the the full text part doesn't um so so now you have these like how which matches so many documents then you have Alaska I'm just going to narrow the query down for for example say it's just going to be how Alaska now doesn't make sense anymore of it makes it easier to explain um that then um yeah basically you have these mini matches for for um for how in these few matches for Alaska and you know how much each term can potentially contribute to it based on uh the the bm25 scoring so basically there's a maximum and this is from this tfidf calculation it's also part of bm25 there's a maximum contribution that each term uh can make so now if you've let's say you've not had one but you've already started scoring 20 documents maybe um and your your top K so basically the user defined limit is let's say it's 20. so you already have 20. that means you now have 20 scores already so now you know basically that let's say the bm25 score of the the worst of those 20 objects that you have is 17 made up number now you know that if an object if a new object cannot reach score 17 you can discard it and how would you find out if a new object can can reach 17 is basically you look look at the maximum contribution of each term so let's say the word how and this this gets nicer if you have a longer example let's just say I'm going to stick with this how we'll ask him let's say the term how has a maximum contribution of three and the term Alaskan has a maximum contribution of I don't know 15. then if those two are combined that's 18 that's good but if they're not combined if you only have how you can no longer reach your threshold so basically what that tells you and this is everything the one algorithm does it tells you skip every object for how that doesn't also match Alaskan and and this means that if you have the these like sort of relatively dense list which is the one with how which is dense because there's so many matches and then you have the sparse list for Alaskan which is rare at some point those lists are going to overlap and basically it allows you to to sort of move that pointer ahead to the next point where they're overlap where you have the the minimum possible score to reach and then um same same goes on basically let's say you managed to skip 5 000 matches for how because only then you would find the next ID that matches both how in Alaska and then after that there's another 5000 which only match how can do the same thing again in this way you can reduce the amount of scoring drastically like in the most extreme case you could reduce it to the number of scores for the word Alaskan and almost sort of ignore the word how but of course if the if if the difference is not that extreme then you still you don't want to completely ignore it you just want to sort of yeah not score it if you know that it's mathematically impossible to reach a higher score and then you want to want to skip it and and that is what makes that algorithm super cool because in the context of vector search we deal with approximate algorithms a lot but this is not an approximate one this is really a just a smarter way of scoring basically in the the score in the end is going to be exactly the same yeah that is so interesting to hear about this sparse scoring algorithm and maybe uh talk a little more about as far as vectors but just quickly like I've plugged in this wand test into the beer test with natural questions and for me evaluating natural questions is 2.6 million documents and this time is how long it takes to evaluate the ndcg of 3500 queries and the difference before and after wand is like five and a half hours to like 14 minutes and this lets you yeah I missed so I was like wow this is pretty exciting just anytime you see that kind of thing it's similar back to when you had the Thousand speed up with filters yeah those things are always like the Eureka thing to see but yeah I think it is very it's so interesting like um I did a little research into one and I saw the paper from 2003 that uh proposed this and I think it's so interesting to see it uh still being used and I also saw this other idea of splade sparse vectors or used deep learning models to uh to create sparse vectors so the idea being uh you have how to catch an Alaskan pollock and you put the language modeling head on each of the tokens and then you get a distribution over the vocabulary that comes out of the language model so I think Pollock would probably be the best one because maybe like in addition to Pollock the language model might have thought it was like salmon that would make some sense right so it would um so you'd have this sparse distribution and it would be the same kind of scoring building blocks as far as I understand for uh how you would index those that particular kind of sparse vector and then you know retrieve it efficiently but yeah the whole thing hearing about the thresholding algorithm it's all so fascinating um so super cool so yeah I think uh coming out of the some of the speed things and now something that is into the database thing uh so what's new about replication yeah yeah so I think today um like uh depending on on who the audience is either you have to wait a very long time to to uh to get to the parts that are interested in or they're they're going to learn something completely new because we we have so many like this is such a diverse kind of release like we have we provide value in so many different areas like the the bitmap indexing I guess was sort of also more on the database side but maybe easier easier to to grasp and uh yeah the the um PQ is sort of completely in the in the nitty-gritty uh um Vector indexing thing kind of side but yeah coming back to sort of the the good old uh distributed system kind of um uh um yeah replication topic so we released replication in 1.17 and and something that we made very clear in the beginning is basically this is the feature set of what replication can do and this is if you if this is sort of good enough for you then use it this is what we're going to do in the next couple of releases and if you have a certain point where you say okay I need this particular feature then you can you can sort of start using it that specific point it's just going to get better and better over again so sort of the the idea is to release this in in chunks or in increments um where it constantly gets better and you don't have to sort of wait over and over again because 1.17 probably covered 80 of cases but then you have those remaining uh 20 basically that take take much longer but also that there may be a requirement for it for some cases depending on what you want to do with with replication replication to me is is such a an interesting one because the the motivation of why someone needs replication in their database can go anywhere from they have a specific failure scenario that they need to protect and where they need to make sure that if a happens the database doesn't go down and then a database can still read or can still write into these kind of things all the way to they need to sign off some sort of a corporate sheet where there there's some sort of a compliance policy that they cannot use a system that has a single point of failure so everything needs to be replicated and we're really seeing we're seeing all of those we're seeing like people who who need replication for the actual value of replication we're seeing people who just need replication so they can take some sort of box and say like okay vvn has has replication um and yeah depending on on um where you sit on that that line I think different different points and different features add more more value um that said that was all these sort of the long introduction of how we release replication in in smaller chunks but what's new in replication in this release is we now now have tunable consistency for every single input or I think almost all endpoints and and this is um so tunable consistencies is modeled after Cassandra which in our opinion like basically we looked at okay what's out there in the space and what behaves in the kind of way that we want VBA to behave and then Cassandra with its hyper scale kind of capabilities with a very good good role model for for this basically so tunable consistency is something that's modeled after Cassandra where basically you as the user can make that kind of trade-off how consistent does this read or write have to be and the the most extreme case would be all where you say like you have your database replicated three times and then you make a a write with consistency level all that means that request is only going to be successful if every single one of those three nodes acknowledge your your right and if one doesn't then the request fails but then you could also have the Other Extreme which would be consistency level one where the idea is that only a single node has to acknowledge right that doesn't mean that only that node owns the data like the data is still going to be replicated to the other nodes but let's say node 2 failed and now it can only be replicated to node well let's say you're hitting Node 1 and it can be replicated to Note 3 that and node 2 is currently down this would still be allowed with with a consistency level of one so basically in this case your your request would still succeed and that can be desirable because this gives you a kind of scenario where you can still write even if a note is down and why would a node be down well could be because it actually like Hardware failure or something is broken but it could also be something that's that's planned where it's down for a very short time for example because you're doing a rolling update so you could say like I want to upgrade vb8 from 1.17 to 1.18 um then each node would be in this this rolling restart kind of fashion would be down for a very short time and if in that short time period something happens now you have a full control do you win strong consistency then you you you probably right with consistency all which means this request will fail and you have to retry once it's back up again or you could do it with one where basically it will be replicated later or you could do it with a quorum which were the idea behind a quorum is basically that that it's it's a fancy term for a majority basically so a majority of the notes has to be present and then if you use the majority both during writing and reading then you always have to guarantee that that it has to match because if a majority of the nodes have received an update and have confirmed receiving an update then when you read with the majority there's mathematically no way to get the the old stale data so these are the kind of um trade-offs that you can do and and really configure this this for your for your needs and then there's the the second new part which is read repairs which to me is like the first time I used it I was blown away by how simple it is and how effective it is so if we're sticking with this kind of situation where a specific node was down during an update and you accepted it because you said okay my the consistency level is say just Quorum or maybe even one then how do you deal with it when the node gets back like because at some point that that node is going to be out of date right and if then you you read with just consistency level one and there's very good motivation to read with just one because that means your throughput can go up like if you if basically every data is always replicated and you read with just one then now you have not one node that can serve all the data but now we have three nodes and you could also add a fourth and a fifth and so on so there's there's very good motivation to read with with consistency level one but now what happens if that node is is out of date like is it if you happen to hit the note that didn't receive the update you would be serving stale data and you can prevent this by setting a a higher consistency level of course but also this is where the eventual consistency of of um a VBA basically you know that that whole design comes in if you don't want to read immediately with the high consistency level because it has a higher cost and lower throughput you can rely on the fact that eventually this will be consistent and one way to repair the data and this is new in 1.18 is the read repair in the read repair basically says if you query the object with a um let's say with a quorum consistency level and VV it realizes that let's say three nodes and two are queried and one of them is out of date viviate will just repair it and VV it knows exactly based on on timestamps and based on other kind of metrics of what happened DV8 knows who of those two nodes is basically is wrong who who missed the update and then if you query it with that read repair by the time that your query is returned the data is already repaired so you won't even notice like you can query exactly the note that had the stale data but as part of the query and that's why it's called a read repair basically as part of reading the data VB had already repaired it in the background you you'll receive the right request and that was the moment when I tried this out for the first time like I tried to to specifically get my my database in an inconsistent state by actively killing a node then making an update making sure that the node missed it we started it um try to query it and and forgot to to set the the one consistency and then default it I think to to either Quorum or all and immediately it was already repaired by the Moon that I I got the response and I was like wow this is exactly how it should be like this is this is so cool because um yeah if you really want to fine tune you've got all the ways but also if you just want your data not to be out of sync then let's just read it just just let it repair itself yeah it's amazing and I really liked how when we first started talking about um replication you had like the um the Black Friday e-commerce example and understanding these kind of uh trade-offs with read write and when you need certain levels of consistency yeah that read repair thing is so interesting I think this would be also a great transition of you've recently given this amazing lecture on our journey to build a vector database and go and this is published on YouTube if people are interested in checking this out um so a question I have with uh replication these kind of things is um you can touch on what features of the golang programming language help build things like this like distributed database systems yeah um that's nice because my my uh talk was basically about what what features of go um are hindering us from building the best possible database no that's not true because it also it also has solution for all of those challenges um features in in go that really work well is I think the main two things that that um come to my mind right now is one is dealing with concurrency because especially if you have these kind of kind of background operations you may have to spawn like in in go you don't spawn a new threat but you spawn a go routine a go routine is basically like a lightweight threat but then you can have because it's so lightweight you can have way more than than actual physical Hardware threats and then the go runtime will just manage it for you and it gives you lots of tools to basically make sure because the moment that you have concurrent processes there's always a risk of data races and the whole tool set and go both during development as well as for for how the language is structured really makes it easy to write threat safe or or sort of safely concurrent code um so that's that's one of the parts that that um really really helped while there um the second is the strong standard Library so this is something that depending on on so I way back before I I started writing go um I used node.js because that was sort of the thing at the time and and I also did some front end work and then some point moved into back-end work and then it was yeah it was node.js and in node.js you have the exact same opposite the exact opposite sorry um so so basically anything that you do you have to import a package there was this I think it was called the left pad Fiasco at some point where um a day a package that would pad a string on the left side which is like one line in any language even in JavaScript that's just one line that was an external package and then this particular package had a security vulnerable or I think it wasn't a security vulnerability in this case I think it was just deleted I think it was just the author decided to delete this this package uh but the problem was that thousands upon thousands of packages or or a node uh programs and packages that were used everywhere in production dependent on that one package and all of a sudden all the bills started failing um so coming from node like way way back in the day like I was an early adopter of go so that I don't even remember when that when that was but it was really like like way back coming from node to go seeing how strong the standard library and go is and not needing these kind of third-party dependencies and and these these kind of third-party packages that was such a such a both an eye-opener that you can do it just with the standard library but also such an enabler because there's all of a sudden you don't have to learn framework after framework after framework anymore you just know that the the standard library of go and it is it is somewhat sort of limited but over time you you yeah you'll feel like you know all of it that's probably not true it's probably way bigger than what people typically use but um it gives you a nice yeah sort of the the com commonly use things you just know how to use them and don't have to look stuff up and just by either by just typing it or relying maybe on your your code editor IDE to do some autocompleting you can build basically an entire database and that's that's not even a fictional example because that's basically what we we do in in V8 and to to loop back in the beginning when we talked about bitmaps we said that we took that crazy decision to um to build everything from scratch and not have many external dependencies and that works super super well with go because I think that's also the philosophy and go that the language is so powerful on its own that you don't need these third-party tools but just with the language provides is there and very practical example in replication because we're we're dealing with the distributed systems where nodes have to talk to each other there's so much Network traffic going on and then there's multiple ways of how to do that um but basically being able to do all of that just with tools that the standard Library provides is is super powerful and really he keeps the iteration time down so those would be my my two favorite points about go um for a feature such as replication yeah that's that's so interesting it's so interesting like going into the golang and yeah I mean the whole end to end as we start off by talking about the bitmap the P hsw like vector search deep learning and then coming into the features of golang It's amazing And um maybe one other detail on your talk that I was curious about is um see as you mentioned you you're talking about the hindrances of going in them and one thing was the um the stack versus the Heat memory escaping to the Heap and and I know we talked about the garbage collector uh maybe talk a little more about like the state of I know the go mem limit and all these kind of things and like uh how that's been because I understand that it's getting better and yeah yeah let me try to answer that from the perspective of a vv8 user amazing because I think the vb8 user has two interests well or basically in in V8 but in turn then also in in go uh one is the application shouldn't just randomly run out of memory like that's that's basically the worst case you you sized everything correctly you plan for everything and then all of a sudden vbh just goes bam out of memory kill and the other is you want high performance and and these these things are closely related because um performance in go often is a topic of of memory management which may not be super obvious because if you think of performance you think like the bottleneck is your CPU right like if your CPU does this many things per cycle uh then it's it's fast and if you can reduce the amount of things that it has to do per cycle or I don't know use a smarter way of doing it like in the talk I talked about simd for example where you could do eight operations for the cost of same operation that is what you immediately think about but in a garbage collected language like go often it's it's not even it's not CPU but or it is technically still CPU bound but because of something that you do with memory and that is where where memory allocations come in and um the idea between or the split between the stack and the Heap the stack is basically represents your function Stacks so as you move through functions you move through that stack and you have variables with a very very short life cycle they they are as you start calling the function basically the variables are created or are avoiding the word allocated right now but they're they're assigned basically and then when the function is over those variables can just go away so they have a very fixed lifetime whereas the Heap is everything and that's why go uses the term escaping which I kind of kind of like because it escapes that clear function stack and now and this could be for example because it's shared between different function calls or because the lifestyle cycle is meant to be longer than these individual functions called so basically something escapes and this escaping makes it way more costly so both the process of escaping because now the the go runtime Master like okay this is this needs to be tracked basically and this needs to we need to find some space somewhere on the Heap um but also then it needs to be deallocated again because it's it's it's like now now that it's escape the function stack it's no longer in this kind of predictable life cycle of when can we free it and if you couldn't or if you would ever lose that kind of Association of how it could be free then you'd essentially have a memory leak you'd have memory that's assigned somewhere that you don't need anymore that's still around and and then your your memory starts ballooning and you don't know why and that's that's um yeah there are some things you definitely want to avoid and then to avoid this basically go use the garbage collector it's a very good garbage collector but nevertheless something that's allocated on the Heap is always slower than something that's that's allocated on the stack so this is the the kind of perform performance a background in this kind of setting but then I also said you don't want your application to just randomly run out of memory and this was the the biggest change in go 1.19 I'm almost getting confused with the versions of like bb8s at 1.18 right now goes ago is actually at 1.20 right now at some point we have to start releasing a releasing V8 faster than goes so we can hire her numbers which I think that the bb-8 release Cycles are way faster so that's only a matter of time um but yeah in go 1.19 uh the go runtime introduced this go mem limit thing that we have a blog post about um also and go mem limit does something that's that's actually is one of those things that are super simple in in hindsight or once you know and it's super simple go mem limit just says this is the limit of memory that you have available in the garbage collector will try to respect that limit and now the question is like Why didn't it before that like why wouldn't the garbage collector do that before and before the garbage collector had um a tuning parameter that was relative to the current Heap so basically in the default was a hundred percent which would mean that if you have a machine with 64 gigabyte of memories and you currently have 33 gigabytes of memory allocated the next hundred percent increase would be to 66 gigabytes bam out of memory so so now you're at like 51 usage and just because of the way that the garbage collector would try to allocate or try to um The Collector doesn't allocate memory but it would just delay running again so basically now would say like okay I'm gonna run again once the memory is at 66 gigabytes but your machine only has 64. so that that doesn't work and all that go mem limit does is basically give the garbage collector that kind of hint of where the the limit is and go call this a soft limit because it's really it's just a hint to tune the garbage collector um to to run faster and now the garbage collector basically knows okay I'm at 33 gigabyte my limit is 64. so I'm not going to wait till 66 but maybe I'm going to run at 60 already and all of a sudden like you you made use of that memory that that memory is there to yeah to serve whatever purpose you want whether it's a purposeful Heap allocation so for example in bb-8 uh the vectors or with agents wpq also the compressed vectors they stay on the Heap because that's what we want right we want them to stay around um but something that we do for example a filter that's used as part of a part of a single call stack that doesn't stick around so we don't want that that on the heat but either way no matter whether it's intended Heap or unintended Heap now with go mem limit vv8 can can or or any Go app basically can respect that and make sure that it doesn't quote unquote accidentally run out of memory again because that's really what it was like it it was it just set the wrong target basically and that's that's prevented with that and I think at some point we also added a a feature into vb8 specifically that marks your objects as read-only if if that if you get close to that threshold so I think it's it's getting more and more like as vv8 improves and as as go improves it's getting more and more difficult to actively manage to run out of memory which is awesome for a user because that essentially means it's it's more stable and and harder to kill and if you still manage to kill it then you have replication and then all is good because that that one note death doesn't matter because it's replicated so yeah it's so amazing it's so amazing like the depths of the languages that build databases the features of databases and then the whole search technology more broadly and uh so awesome Eddie and thank you so much for another weeviate release podcast it's very inspiring for me working on Wi-Fi to hear the depths of your knowledge with all these things and I'm sure it inspires people who are using levator already or are interested in checking out alleviate and I think we've 1.18 is packed with so many awesome features with this bitmap indexing to have these filters just as these like thousand times faster is obviously like just so incredible the wand the way that speeds up bm25 and sparse search I've seen that myself it's incredible and hswpq all these things replication and that's just so exciting so Eddie thank you so much thank you for for having me and also big thank you of course to the entire team that that actually built this I am just here basically summarizing everything that that we've done as a team in this this print but of course we have these individual experts on on all of these topics and and then that that allows us as you just said to have a a release with so many different topics that provide value in this is this is really cool and I'm super happy um with our team and super proud of our team so thank you everyone both both inside bb8 but also of course outside or Community for for all the input and for telling us um what the kind of features you need what kind of features you benefit the most from and and yeah any kind of feedback like use vv8 and tell us whether you like it also tell us if you don't like it so we can improve it but please also tell that tell us if you do like it because that's that's awesome to hear um so it's so motivating both for for me and I think everyone on the team to hear the success stories with bb-8 and this kind of there was a tweet the other day where uh sort of Father and Son uh pairing did some experiment with with V8 and just reading that and reading the actual stories this is so cool to to yeah change people's lives one at a time with reviate yeah that was an awesome tweet be yeah the whole the moment of like the father-son hacking with Ouija amazing awesome thanks everyone for watching thank you"
    },
    {
        "Du6IphCcCec": "hey everyone thank you so much for watching another wevia release podcast releasing we've made 1.19 we have all sorts of exciting new features but before diving into the into any of the features edian has on what I think is the best iteration of the Wii V8 shirt uh so Eddie thank you so much for joining the 1.19 podcast thanks for having me I'm calling this the 1.19 shirt I think it's not the the 19th iteration yet but it fits perfectly with all the cool stuff that we have in the new release yeah awesome I remember we joked about having like a t-shirt for all the blog posts the blog cards as philana makes and yeah yeah I really like the book of it uh so super cool so let's kick it off with the grpc API and the story around what led to that yeah uh grpc API was sort of an initially unplanned feature but we wanted to add a VBA to a n benchmarks and in a n benchmarks we've we've talked in in the past we've talked about the sort of Library versus database split and in a n benchmarks you have all these libraries and um but now also a couple of databases so we thought like okay yeah we have nothing to handle it let's let's get VIVA in there as well and uh what you can see in those those uh benchmarks is that especially for their so so for those of you who don't have the context in The Benchmark essentially you have a graph that on one axis shows you the the recall and then the other latency and with any a n algorithm especially with hsw but with any it's always a trade-off between recall and latency or throughput as sort of a as the inverse latency basically so throughput in the A and benchmarks is actually estimated because it's not really a throughput benchmark it really just a single threaded individual queries and then if a query takes let's say a hundred milliseconds then the throughput would be considered 10 per second so just one second divided by by the individual latency the the actual ladies these are way lower than 100 milliseconds but that's that's kind of the The Benchmark setup and what you can see in those sort of low recall settings of course the latencies go go down quite a bit but that also means that everything that you have as constant overhead so let's say a vector surge takes for example half a millisecond but you have a constant overhead of half a millisecond and all of a sudden in these low recall situations you now have 50 overhead or depending on how you see it 100 overhead and that's not great in benchmarks so um we thought well what can we do here and and can we do something that doesn't just optimize for benchmarks because that's kind of well I don't want to say it's pointless because in the end benchmarks are a great signal for users and they help them sort of decide but also to some degree if you over optimize for benchmarks you always have to ask yourself like what benefit does the actual end user have and we looked at the grpc API in um adding grpc API sort of in the background for some of the clients so what is super important for us was to make sure that there are no breaking changes or that users would have to sort of start changing their their usage patterns or would have to learn something new no we've sort of slowly started adding this this grpc API which some endpoints not all yet but some can run with this and the python client for example can automatically discover if grpc is enabled it's also an optional package on the on the client side so if you don't have trpc installed then you don't have to install it if you don't like it but if it's there it's it's used in the background and that allows you to to yeah basically have more efficient queries where just the the overhead and of course that overhead before I think it was was around 500 to 600 microseconds so it's a bit more than half a millisecond that was there in every request but the the shorter the request duration is the more noticeable is the overhead You could argue that a user would never notice in real life and that you only only argue it in that you would only uh notice it in benchmarks but nevertheless overhead is there and overhead is gone so making something more efficient is never a bad idea I think even if if you could argue that that most users might not see the the real life impact yeah um so definitely want to dive a little over a couple more questions about how these a n benchmarks and the recall latency but um maybe just super quickly so grpc graphql HTTP uh could you just quickly give me the difference yeah uh so so interestingly grpc actually isn't a separate protocol it runs over HTTP 2. so from a sort of these are all sort of one layer higher than the TCP stack they all run over TCP and then grpc basically runs uh via HTTP uh so the the the way and this is a very interesting because I think some people uh just assume that grpc is faster because it's a faster protocol but it's actually not a faster protocol because it's the same protocol as HTTP and by the way graphql this is also typically sent over over HTTP um it's simply that grpc uses the protobuf protocol which is very very close to how uh the the data types are represented in their respective languages so uh the the go memory model is very similar basically to to the protobuf structure and then also the memory model in Python so if you have this sort of network request goal which is vv8 wb8 server uh via a grpc to python they're simply way less sort of decoding encoding restructuring and not restructuring but so re-encoding that you need to do whereas uh both graphql and and uh rest that we typically use for for HTTP um use Json and Json basically is a is a sort of space inefficient protocol because it's just everything text and it needs to be parsed it needs to be sort of the same way that that if if you as a human right Json you have the curly braces and you have the quotes and then you have the colon then you know okay this is the key value mapping in the same way the server and the client for that matter have to parse that and that that simply costs time and it's it's not a massive overhead but in a benchmark scenario where you don't have a lot of time like even that can can um show up what was important for us was to to like not not add new overhead for the user like cognitive overhead we're reducing computational overhead but don't want to add a usage overhead um so you probably won't notice you will still be able to use graphql especially for for sort of just learning the vb8 API just discovering it playing around with it the graphql console this is an amazing tool but at the same time if you run your use case in production and you run it through the language interface that is or clients anyway you might not need graphql anymore so it's kind of a more more options yeah super cool so stepping into the A and then benchmarks a little bit I remember you'd published that amazing a m benchmarks collection and we also had a podcast on that I think it's like number seven earlier podcast and I think we're on like 47 47 or something but um wow I'm sorry I remember the um so like the hyper parameters of you know the effect of ef ef construction Max connections um so are all those fixed and then each of these you know a n providers all have to adhere to the same hyper parameters and the same machine as that sort of a like a fixed hyper parameter of the hsw kind of uh no actually so the machine is fixed yes and not even just the machine there there's actually a so I I said that the in benchmarks was single threaded before it's actually not entirely true they're limited to one CPU which basically is not the same as single threaded you can run multiple threats with that CPU but there's no benefit because you can't ever exceed more than than one CPU so that is one one restriction uh one restriction is the build time so I think if it times out after I think I think two hours is the limit or so so you can't run anything that takes more than than two hours but other than that it's completely free so you can set your own parameters I think all the hnsw based implementations do have the same parameter set so it's typically it's basically a grid search over over all parameters so you have if constructions from very low to very high you have EF from very low to very high and same for for Max connections and and since the hnsw algorithm sort of works the same I guess in a sense you're just comparing the the implementations and that also means that the the parameters typically perform the same so so maybe in in absolute terms like one solution would be slower than another but still for both Solutions like uh setting eight connections would probably have the same effect on both Solutions as opposed to setting like 64 connections um and then in a and benchmarks it's not limited to hsw based algorithms so for for those so so for example a PG Vector is in there the new uh postgres plugin which I think is IVF based or something or iufpq base so it has completely different parameters but that's the nice thing about a n algorithms you can compare them them still and and sort of yeah you're kind of kind of free to use all your parameters to get the best out of all parameters under the Restriction that you still need to be able to build it within two hours on a single CPU so so what scale is the test is it like the sift uh the sift one million vectors or like uh yeah we have sift one one m in there I think we have I forget the name but I think it's a deep one B which is not a billion that that would be that would be a bit large but I think it's 10 million or so so they're they're definitely smaller and larger but I think they're all within within yeah into one one to ten million do you think maybe that's something that the benchmarks uh like should aspire to is or you know just like the continued scale anyway yeah so the whole topic is so interesting um so so maybe pivoting topics a bit we also have in our uh release the uh degenerative cohere module and so I'm really excited to talk about this because this is actually the first module I've ever worked on at webiate and I mean it was pretty easy to the the the infrastructure that's been in place to uh extend these modules with new models or new external model providers so you know open AI Co here when the anthropic Cloud Model comes in the hug and face Transformers gbt for all the llamas like all these large language model providers can be integrated so easily into eviate and so yes I wanted to maybe ask you anything about um your latest thinking on the large language model stuff and that kind of um the the trends and all these new models that are coming out yeah cool so first of all I'm really happy to hear that this was easy to do because this is this is kind of the point of the module system like there are so many sort of complex tasks within the database both on the the victory mixing side but also on the just sort of old school database indexing site so so having that sort of almost sandbox scope of the module where it's very easy to integrate something new that's that's that's always awesome and and um we're always awesome to hear that it actually works like this as as it is intended um which serves a nice nice segue into everything that's happening right now because um we're really in this this sort of crazy mode of something new popping up and and breaking GitHub Star Records within a couple of days or something it has more than everything out there and I love that we're able to to integrate them I I was about to say react but I think react is not the right reward I think we're able to integrate them very quickly as they pop up because it's not react would imply that we somehow like have to chase them but it's not the case it's just enabling users they did There's Something New comes up a new be it a new model a new integration something that provides value in in this AI space and if it provides value then there's a good chance that it can also provide value to deviate users and I think being able to to quickly adapt there and quickly integrate those is is super valuable for our users and similarly um having vv8 basically as the the stable API for them I think is also super cool because if you if you manually let's say you're using generative open AI so the completion endpoints from openai and now you want to maybe test that against the cohere endpoint and then yeah I just want to see like okay what what performs better for my specific workload if you were to integrate with those two providers separately you would probably have to adjust some code because you wrote something that was meant to contact the the open AI API and now the the cohere API I haven't looked at them but I'd be surprised if they were identical it probably is some difference but if you use vv8 with all of that the VBA API is the same all you have to change is the model or change some string somewhere in the model that's essentially just configuration and that goes for for way more of these things like if you this is just one example you could say you could argue like yeah okay changing changing one of those requests is easy but now if you combine that maybe with with the generative search which typically is the the retrieval step and then the the generative part what if you want to change something on the on the retrieval side you could have text back open AI text to back cohere text effect Transformers contextenary uh uh text hugging phase for any hugging phase model probably new ones that that have been released during during the recording of This podcast so um you can combine all of these all of these five or however many it was you can combine bind them in a hybrid search with bm25 so so now we have this like large Matrix of different options and you can do this within rebate by simply changing configurations I think that's that's a nice addition additional value of having that collection of of different model providers within mediate yeah I think that um like they're kind of like model inference orchestration of leviate is so fascinating to me like whether you want to manage this on the database side or on the client side and I think by doing it in the database you just have kind of one thing with all this infrastructure and and yeah the whole thing is so exciting the head of the one paper I really liked that came out recently is hugging GPT where it's kind of about the you know the large language model is like the task decomposer that routes it to the models with descriptions of the models and I mean the the future of like we originally talked about this pipe API which is kind of like a dag of computation flow like retrieve retrieve rank read but now we're seeing such a dynamic uh kind of chaining of the things you can do with search interfaces and it's so interesting so the next topic is another innovation in search that I think is extremely exciting so much potential with this idea which is Group by arbitrary property and yeah can you explain how maybe with the document passage example I think would be the best like yeah yeah this this feature this was basically a a request from one of our commercial users but when they requested it we we kind of almost expect this already because it's just something that that is an Ever sort of growing problem that we now have a solution for so the idea is if you have your documents split into some kind of sorry not to document your your entire content split into some kind of hierarchy um typically you're you're either forced to give up the hierarchy or you have to do some kind of work around so so what I mean is for example let's say you have documents and you have passages and a document is sort of a grouping of a number of passages if this is a very small grouping you could for example do the ref tvec so you could have them in this as two bv-8 classes you can do the ref Tech and then reftube gets the mean embedding of those passages the problem with this is as the number of passages per document grows it gets harder and harder to represent so much information because essentially all gets combined into a single vector and there's only so much information that a single Vector can can hold so if you try to combine a thousand passengers and and these all have maybe diverse topics so some that says maybe not document passages maybe book and chapter or book and passage then it's very very hard to keep that meaning so what users typically do is they would want to search uh buy the the sort of smallest unit which in this case would be the the passage but then the problem is well how do I get back to my association off of the document like now I'm searching for passages uh maybe the top 10 passages are all of the same document maybe in the top 10 passages it's 10 different documents and what our users were missing is basically a simple way to to sort of keep that kind of Association and and make make that part of the ranking basically and with the group by feature um and it's and a group by by any arbitrary property I think it's it's most interesting for this particular example to group by the the reference prop that that would be the the association from passage to document it now allows you to have basically the first hit comes in and let's say it's off document one um then you can you can group all the passages that match that same document in one group and then you can either sort of group this by what is the the highest similarity per group or you could take the mean similarity per per group or something there's like more more options but basically if you want to display like if you want to search through passages but to your users want to display um uh documents you can do this now because by finding the the the one passage that fits the best you still get the the latest passages basically the the most related passages in that same document that you can represent to your users so it gives you like a nice way to to display that as like almost like a like a tree kind of way um or various other ways I think gives you more options to to show that to your users yeah that's amazing I mean um firstly quick shout out I was just at the um the haystack search and relevancy con a conference that I open source connections I know they would love this topic um yeah so like with the ref devec yeah you try to aggregate all the passages and averaging is maybe too much of a compression I used to think that maybe a graph neural network could aggregate those vectors and or maybe cluster some ideas like this it also kind of reminds me of like multi-vector representation where you you know you have a document and then passage that kind of thing maybe like title passage and so on author but yeah that's um it like I I like the analogy of the podcast like taking these podcasts and then um you know putting the transcriptions in and you know you have all the chunks and you have the similarity the chunks that you aggregate it's such an interesting innovation in Search and I like to call this like top level indexes and yeah I'm so excited to see how this uh evolves so so now transitioning into uh some new data oh yeah unless you want to stay on that no no just as I I love your example about the podcast and I just have to think because we opened with the topic of benchmarks and we also have a dedicated episode on on benchmarks I think that would be a great example because now it gives you the ability like do you want maybe that snippet from today's podcast which is very relevant to to benchmarks but the podcast is not about benchmarks or do you want more of the mean similarity where you have dedicated episode about benchmarks where maybe no snippet fits as good as today's snippet but overall you have the the podcast that fits the topic better and I think this is something you can very nicely represent with with uh the new grouping feature yeah amazing that's a perfect example yeah okay is the one passage Masters it enormously but then the whole collection yeah awesome stuff so um so okay so coming into the um the database stuff and I think this is so interesting how um so now you have a tunable option between whether you want to use the bitmap index which I assume to be like a super fast inverted index because of how the underlying data structure is implemented and then you also have the um the bm25 score index uh so so what goes into kind of this tunable indexing yeah so in the past we kind of made an assumption that if it's text you would always want to do a full text search and that that just I mean that may be true in some cases but it wasn't it wasn't granular enough because exactly as you say we've introduced in in the previous release we've introduced those bitmap filters and everything so so we had it for everything except text props for for this particular reason and um everything else is super fast now but text still kind of like was sort of still the old implementation because text was built in a way to be indexed for for bm25 which is or without going into into too much uh detail but it's very different because you need a lot more information so for bm25 every hit and every Association needs to be scored so you need to know uh the the frequencies and all these kind of additional additional things that you don't need to know for for Pure filtering but now if your use case is to Simply filter and nothing else on text on something that happened happens to be text um yeah right now you get options you can either index it only for filters then it's fast it's the bm25 index you can only index it for search then you can have then it's basically the same behavior that you have before so filters if it's not specifically filter indexed it can still fall back to um to the the old index to the searchable index but if it has both then it just takes the the best at each time then it's fully in your control um it's fully Backward Compatible so by by default we build both indexes but you can take the turn them off and then simply tune in so if you said Okay I I know I'm only going to need so so for example if the field is like a a user group that you need for permissions checking or something you're never going to run a bm25 search over that you know it'll only be there for for filtering so just turn the searchable index off keep the filterable index and then you get fast filters using bitmaps under the hood for string properties as well yeah it also makes me think about like you could probably represent categorical uh variables that way like if you have yeah yeah typical um so kind of yeah just just because it's a string doesn't necessarily mean it's it's full text that was kind of the assumption that we made before but it simply doesn't hold true yeah super cool and and you can use those with the wear filters yeah amazing stuff um so then there's um so now there's more tokenization options as well um yes you may be taking me through the thinking around the tokenization yeah yeah so um mention string and text before and I think this has been a big source of confusion for for some users because some users just assume that because it's string it would use a specific kind of tokenization some users didn't even know that there was a split between string and text so they used string for something that should have been text or use text for something that should have been string and as I'm saying this well what should have been a string what should have been a text like that's just not clear that's that's very hard to explain and that's why we've we've simplified this and again all in a non-breaking way so right now there's only going to be text string basically disappears nothing breaks for you you can still in your apis or in your your schemas you can still set string and it'll be automatically converted to to text and to keep that kind of difference that we have before there are now more tokenization options so you can tokenize if you want buy the whole field which is something by the way that a lot of people assume that that string would do but it would only do that if you set the tokenization to field so right now that's simply an option you can tokenize by word which basically means vv8 will split on the spaces but we'll keep the individual things you can can or or will sort of keep the alphanumeric contents of the the words that will remove special characters you can also split just by white space which is sort of again splitting at the word boundary but not removing those special characters so for example if it's like a a product description that has a special sign in it or something and you want to you want to keep that because it's relevant then you can so again more control and simpler API at the same time so keeping on the um the apis have been um so I've started diving into typescript and learning a little bit more about that I've seen more of the consistency level and um so the newest thing tunable consistency for Vector search and the get requests and all that um yeah the whole so replication is something that you can so I hope I'm getting this correctly and Eddie and will correct me in a second if I'm wrong but replication is like how you're replicating your data across nodes in a cluster so so how does this work with each tune each query you can tune this yeah so the the there's two sides basically for for tuning one is how do you write your data and how do you reach your data and they they somehow depend on one another because for example if you make sure that during writing every note already has the same copy it doesn't matter what you do add reading because if you just read it from one note or if you read it from from multiple nodes you already made sure that they're in sync during during writing however that has a large cost because now for every ride all the notes have to basically agree on it all the notes have to be alive so so um with tunable consistency the idea is to to make some trade-offs basically you could say I'm only uh writing with me with a majority of notes and I'm also only reading with the majority of notes and then you know okay we still got some consistency guarantees basically because um because if if both writes and reads happened with with a majority of notes then uh it should still be consistent or you could say it's actually not super critical that my data is always up to date maybe you want to accept that it's sometimes a bit bit um yeah eventually consistent so it could temporarily be out of date and hasn't been repaired yet and that may be fine because maybe something that you're updating is is yeah it's not a bank account details or something or or Bank transactions but rather product search where it's not a big issue if something is slightly outdated or maybe you can catch it in the application side if it's outdated and um yeah for for a graphql searches basically that was the the last point that we had where we didn't have tunable consistency yet and for those three queries you can now set it there as well amazing uh so everyone we are a little time constrained so we're gonna we're cutting it a little short but um as always the incredible devrel team and there's this release blog post with all these releases if you want to consult more details Eddie and I are available in slack and more than happy to answer any of your questions so grpc generative cohere Group by arbitrary property bitmap indexing more fine-grained control over that with the tech surge deprecating string with more tokenization than the tunable consistency and some patch releases So yeah thank you so much everyone"
    },
    {
        "xk28RMhRy1U": "thank you so much for watching the wevia podcast I'm super excited to welcome we V8 CTO and co-founder Eddie and dilocker for the weeviate 1.20 release podcast this is another packed release Abate with all sorts of cool things multi-tenancy PQ rescoring Auto cut re-rankers a new hybrid uh rank Fusion algorithm and some Cloud monitoring metrics every time we do these release podcasts it's always so much fun I always learn so much so Eddie and firstly thank you so much for joining the podcast thanks so much for having me same for me I love talking about these things I love the the great questions that you always prepare and I'm very very excited as well to talk about this one yeah amazing yeah I love just like the breadth of it going through all the different topics in weaviate and so starting off with I think just a super exciting topic diving into the database thing multi-tenancy can you tell us uh maybe just to begin in the highest level abstraction like the overview of what multi-tenancy is yeah yeah what it is and sort of why why you need it even in the first place so multi-tenancy uh for us and I always feel like I need to sort of because when people hear multi-tenancy they think of cloud operations sort of share resources and Cloud operations and of course we have a cloud service so multi-tenancy for us is not about how we run the VB account service yes you can use multi-tenancy on the cloud service but it's actually about multi-tenancy for you the user so let's say you have an application your application has separate users and they have somehow data that needs to be separated from one way or another so for example let's say you build an app and your app allows you to index documents that you have on your hard drive so maybe just sort of install something on your let's say something like Dropbox or so you want to search to all your your documents you only want to be able to search through them yourself you definitely don't want other users who happen to be using Dropbox to be able to search through your documents so that's kind of the the idea of scoping that to to individual tenants so you as a Dropbox user in this case would be a tenant or it could be that multiple users maybe instead of Dropbox it would be a workspace on a notion or an atlassian Confluence or something so in this in this setup basically a group of users could be attended so it doesn't necessarily have to be an individual user but it needs to be some kind of isolation unit basically and this is this is so far this is not even like a technical requirement that's essentially just an application Level how do you curate what do you have access to requirement uh but then where this gets super interesting for Vector search is that it kind of mixes with the technical requirements and it's almost like like it's sort of it perfectly aligns because in in Vector search we have the problem that we need to somehow figure out how to sort of limit the vector space and we've talked about H and SW the sort of the indexing graph in the past and now think of this whole graph that contains maybe a billion vectors but these billion vectors are spread out over a million tenants now you would have to sort of assuming there was no multi-tenancy you would have to essentially cut that graph into a very small chunk that only contains about a thousand objects each though for a million tenants have a total of billion objects that's only a thousand each uh chances are this graph becomes either it becomes disconnected or you have to travel a lot through that graph without sort of hitting notes basically that you're not allowed to hit or you're not supposed to hit so this single graph and filter kind of approach is at best the workaround like you can we have this in mediate we have this flat surge cutoff basically where if the the filter becomes too specific you actually do a flat search so this would be one way to work around it but then um you sort of lose the whole benefits of of uh the fast paid or the high throughput and low latency kind of search that you expect from hnsw so there is a need to to basically also do this kind of separation from a technical level and this is where we said okay enough workarounds another work around that that users have semi-successfully used in the past was to separate this on a class level because in deviate a class is already a an isolation unit so you could sort of uh say per tenant you create one class and they would all have all these classes would have an identical schema because they're it's all the same use case basically you just copy it for individual users so your schema keeps on growing and growing and growing and like each schema update made the whole thing slower and slower and this worked okayish for maybe two to three maybe five thousand tenants and one workaround and then really we're talking about work round after work round uh another one was then you could turn off graphql because part of the the part of rebuilding the schema part of what took so much time was rebuilding graphql so you could turn off graphql only use grpc that would make it scale a bit farther but we were really in the territory of like this is this is not a long-term solution this is this is happening from one workaround to another so we said we really want a dedicated multi-tenancy solution where the apis support tenants where uh the the architecture under the hood supports a lot of tenants and most importantly where this also somehow scales linearly and we can talk a bit more about about scaling yeah it's so interesting the um I remember when I first heard the question about doing this kind of thing I heard uh you know I was at the Meetup in New York City and someone said you guys support our back role-based access control and you know the time I'm thinking also that you could just have that filter through your class where you if you have like you know a document class and you have content you have user and you know user edian user Connor to only look at like Conor's emails instead of Connor seeing edian's emails and so hearing about the limitation of that as you know if you connect to hsw graph the filter it might not be connected still so you need to modify hsw itself um so as you were giving that explanation it really helped something click for me is the the difference between just kind of naively using multiple classes in weave as we understand it you know like I have an Eddie in class I have a Conor class I have a John class but so can you tell me a little more about the design of multi-tenancy and how you have native multi-tenancy and I think really to uh separate these two things hopefully it's not a selfish question from my understanding but this the difference between just creating a bunch of classes and you know maybe at 1.19 compared to the native multi-tenancy in 1.20 absolutely this is definitely not a selfish question I think our viewers and listeners will will absolutely appreciate that as well so um the this class-based workaround kind of work because one thing that a class already does is it creates something like some separate space somewhere uh basically in the class and internally in V8 this is a chart and within one class you could have anywhere between so so this is in the traditional mode without multi-tenancy you could have anywhere between one and any number of shards and A Shard is essentially you can think of everything that's in the databases contained in that one chart and whenever there are two shards and you want to maybe query across two shards then under the hood this is split into two queries each Shard does their part on their own and then somehow aggregated again um so this is why this workaround kind of work because by creating 10 classes you also under the hood created 10 charts um it was kind of doing a lot of overhead for essentially you just wanted to end up with 10 charts but you could kind of do it with with creating 10 classes the multi-tenancy feature the native multi-tenancy feature in the simplest terms you can think of it it's a single class but within that class we create one chart per tenant so shards are now no longer this static thing but they're completely Dynamic like you can add them on the Fly you can delete them on the Fly and um the the kind of cluster association with a chart still holds true so I need to need to explain a bit more for this probably so in a vv8 cluster let's say you have a cluster of three notes let's make it simple like multi-10 so you can go hundreds of notes but let's keep it simple three notes and let's say that in our class example uh you would have each class would just have one shard um A Shard is something that that can't be split further basically like if you want to split it you need you need more charts so in that old setup this chart with the specific configuration would live on one of those exactly three notes so one way of Distributing this around the the setup of it would be if you have 30 tenons or sorry in the old set of 30 classes I mean you would have 30 tens but you would model them with 30 classes each node could hold 10 of those basically and then that's that that would sort of be evenly distributed um but you'd have very little little control now with multi-tenancy we keep that idea of having one chart um but the shards have become much more lightweight so you have essentially we've run a couple of load tests and um we could in one example this depends a bit on on sort of what you actually what kind of properties you have in your class and these kind of things um and essentially the bottleneck is just the file descriptor limit that that Linux systems have and one test we could reach 70 000 charts per note and per node is now where this this sort of very interesting part comes in because you can just scale this by adding more and more notes to your cluster so um you would have a single class that class potentially spans the entire cluster but a single tenant still is isolated to one node so this this and and to make this a bit more complex we could then also add replication because then you can make sure that this node doesn't become a single point of failure but for now let's just ignore ignore replication so you could have you could start with a three node cluster let's say your tenons are really huge huge and you would only fit 10 per per note then you could fill up to 30 tenants on that cluster now if you want to onboard more users you just add a new note and per note you would again have roughly 10 tenants capacity and in reality it's more like 10 000 but for for our example that that um yeah makes it easier to to reason about and then vv8 under the hood make sure that you hit the right note so of course as a user you don't know where that stuff is scheduled but will be able to say like with every and this is the the only real API change for multi-tenancy is that now you have to specify your tenant key so you don't have to use a filter basically um in your let's say you do a a get near text search then you just have an additional property that's the tenant and then you should specify that and vv8 uses that under the hood to figure out sort of where in that three or five or five hundred node cluster where does the tenant actually live so then um so so I'm curious now about kind of maybe the design decisions behind the tenant key and kind of the the reduction of the sharding the sorry like taking down the size of each Shard maybe we could step more into the technical details behind what it means to have a Shard be lighter weight and dynamic yeah yeah yeah so um the the idea of splitting by Shard or by class or by index type for multi-tenancy that is not new I know for example that um for for if you need to be gdpr compliant and traditional search engines such as elasticsearch you would also try to use that same pattern where you would create an index I think it's called an elasticsearch per 10 and then to have that strict isolation so that is kind of where also this is essentially our class-based workaround um but this always yeah sort of comes with with limits so what we said is we need to make the chart more lightweight and lightweight making it more lightweight is essentially sort of a an umbrella term for all the kind of things that kept us from um from having sort of a lot of charts on the on the uh on OneNote one thing for example is asynchronous processes so since A Shard is its own contained unit in the past we would have lots of async processes so so for example for uh the agents W index for maintenance that would be an async process for every property that's stored in an LSM store one async process would be to switch uh from ment table to segment so when a mem table is flushed that's essentially the memory storage is then to disk then you have compactions in the background so you have all these kind of sort of background processes that tend to be relatively lightweight but let's say you have 10 per of them per Shard and now you have 10 000 charts because you have 10 000 tenants now all of a sudden you would have a hundred thousand of those backup processes and in our very first test before adapting anything we could see that essentially all CPU time was now spent on just idle background processes doing nothing and then to make this even worse some of those doing nothing kind of processes uh for example the ones that would uh check for whether a LSM segments need to be compacted they did that using a discrete so a very innocent sort of simple discrete of hey what is the state on disk right now but now this happens a hundred thousand times in parallel and now you hit your disk with all these unnecessary reads just to find out that you don't have to do anything so very simple changes such as sort of yeah making sure can we cache some of that information can we check less frequency less frequently if there hasn't been a change in that much time uh can we yeah so if all these these kind of kind of can we combine uh internally I think this thing is called the cycle manager and there's one discussion that we have like instead of having 10 per chart could we maybe have just one or could we have less than one because we take that outside of the chart so all these kind of optimizations um that enable us to run more charts under hood I'm essentially making them them more lightweight um another thing is also the memory footprint so in our very first test um we had a surprisingly large memory footprint for an idle chart don't remember what it was but I think it was something around five megabytes or so where in a single class single chart setup this would just never like you wouldn't even notice that that like you have one class with one chart and now you have five megabyte of memory usage not a lot but now again times a hundred thousand or times ten thousand all of a sudden you have this like 50 gigabyte of idle memory for or not idle memory 50 gigabyte of use memory for what is essentially idle classes so that that was another optimization again sort of a very simple optimization which is to to make this more more lightweight just look at what kind of memory how are we allocating memory how are we doing this dynamically are we sort of are we a bit too optimistic about where the the chart is going to grow to and just sort of setting more reasonable defaults making sure it can still grow so there's essentially no no negative user impact everything can still grow but just the defaults are more reasonable and more aligned for for having many of them and that's kind of what we mean by by making them more lightweight amazing that's such a clear explanation of it and um yeah it's it's really interesting hearing about these background processes um you know hearing about the Compact and merge I don't know too much about the LSM myself but that explanation I can understand how there would be you know background processes that check on the database and that kind of thing so um yes you mentioned the um you know seeing the five megabytes of overhead per class and you know kind of the insights that you gain by testing it I think that transitions nicely into this next question that I'm uh how have you been testing multi-tenancy and I guess it's kind of I think like some like what are the lessons from it like it sounds like with the megabyte thing you you know even though you have such a deep understanding of weeviate internals you still learn from your tests yeah that makes I'm curious like how this feedback process of testing it how exactly it's tested and then what has been illuminated from the tests oh yeah absolutely this this feedback cycle goes sort of Beyond just the the test it's also it's user feedback like I think the the whole journey of getting into multi-tenancy started out with user feedback saying like hey I'm trying to apply these workarounds but now I'm seeing large memory footprint or now I'm seeing seeing stuff slowing down or emptying my disk being hit even though I'm not querying and these kind of things and and that was I think in multi titanically that was the first thing that we that even made us aware of hey there is a need for something new there's a need for for a revolutionary change basically not just not just extending the the workarounds and then um I think they're in testing you could say that there were two major faces so the first one was initially when we started out this was in proposal phase and we just wanted to say like is this a viable idea we have the proposal out on because we do this out in the open of course we have that out on on GitHub and just it was sort of a mix of this is why we think it's technically feasible this is what it would provide and uh just ask a couple of of users for feedback and if that would solve their cases and that was overwhelmingly positive so that was great so then we just did a very simple load test essentially the the old setup which because we knew that the shark is going to be a Shard yes we're going to make it more lightweight but essentially we can already create a chart so I think the first test we use the classwork around but then we also increase the number of shards per class because our end goal was having as many shards as possible and then we we um I don't remember the exact numbers but let's say it was something like 50 shards per class and then we just kept on adding class after class after class and at some point we would say maybe hit 10 000 and we would say like oh all of a sudden the next query is now failing I'm going to investigate like why why are the queries failing what is what is going on and that that would be part CPU profiles for example seeing what do CPUs spend time on that is when that whole background cycle sort of thing became apparent memory profiles just to see like where is the memory actually used right now um simple queries or sending queries where we said like okay this query in isolation should be fast but now in this large setup it's slow and then sort of investigating working backward from that where where it is that that was the initial phase where we saw okay what we have right now is not a good sort of not not the final solution but we're well aware of what these problems are and how we can fix them sort of with the final solution so that was essentially proof of concept that we're on the right path and that was that was sort of it it failed successfully like it failed in the places where we expected it to fail um but also it proved that if we fix these kind of hurdles if we get rid of those hurdles it would essentially work and that was before we rode any kind of line of production code so that was basically back in you could say in the 1.19 release cycle we kind of prepared for the 1.20 really cycle where we've built multi-tenancy uh then came sort of a classical implementation phase of course Implement implementing that has lots of tests and everything and we would sort of constantly try to evaluate but also sometimes you just need to wait for it to be sort of like for to reach a certain level of majority that you can do sort of these these um Black Box end-to-end tests where sort of in unit tests and integration tests and and even to some degrees end to end tests you always have some kind of knowledge of the internals but with these kind of end to end really end-to-end black box um uh sort of API level tests you just don't know anything about the system you only get to use the same functionality that your user would use and then you just try this just try to replicate how would a user use it and and see where it goes and there was a point I think about two weeks or so before before the release uh when red run from our team I asked him sort of like hey what what can we do right now to to sort of help you give you confidence in the release because he took a lead basically on the whole Cloud orchestration side of multi-tenancy and he said please break it for me try to break it and then we got together and we tried to break it and only we found a couple of there was no no uh no fundamental issue but we did find a couple of things and um it was great that we found them because I think some of them like some of them were a bit on the edge case side that would have probably taken a few weeks for users to run into them some of them were a bit more obvious we really could find them out right away and then um we built this more or less elaborate I would say load test setup where he just kept on importing kept on querying measuring all the the metrics like what so so the the linear scaling was extremely important for us we wanted to make sure that if we have say a I'm trying to use the actual numbers that we used but I think we started with a three node cluster and we aim for 10 000 uh tenants per node so we'd start with a three node cluster with a total of 30 000 notes and then we said hey if we turn this into a nine node cluster and instead of 30 000 we would do 90 000 attendance then we still have that same ratio of ten thousand per node so everything should scale the same is that the case and then we tried and we could see and this was this was like one of those moments where it's like yes we've reached linear scaling where we could see that the the nine node cluster essentially is three times the three node cluster and that is that is very very comforting because then you know okay now most likely the 12-minute cluster is also going to be four times as large as the three note cluster and and all these kind of things and that gave us the the confidence that hey the these claims that we make about if you want to extend it just add more notes so this is really true and there's no additional overhead for say um for for import time or something because the the orchestration needs to happen in the cluster is minimal uh the node that owns the chart basically does all the work so by adding more nodes to your cluster you're not just are you adding a more more space in a sense like more disk space and more memory but you're also adding more compute power which can take off the load so um a very simple way of scaling import throughput essentially just using a larger cluster now is the the second testing phase where um sorry that gave us the release confidence and this is something of course like you would do testing and automated and in manual and explorative and all these kind of stress testing chaos testing you do this before any release but I think for for this release this was the most amount of testing we've ever done so this is the I would say the most confident we've ever been about a feature and this is mainly because we knew we have stakeholders that were really waiting for this and they're really saying like hey this is sort of not having multi-tenancy or not having a multi-tenancy solution that scales to millions of tenants is what's keeping us from from reaching the next level with V8 or maybe for others keeping us from from using vv8 in the first place so we really wanted to make sure that while we knew that there was a limit with the previous solution like now we want to confidently say these limits are gone you can use it for the kind of scale you want and now essentially the the only limit that there are two limits one is the number of file descriptors so that varies a bit and that is is local to one note so um the safe estimate is around 50 000 tenants per node most likely you're going to run out of um out of uh other resources before that um so that's the the one limit um so so upwards of fifty thousand per node and the other is just resources so resources is something that um that that it's just Vector search in general has nothing to do with multi-tenancy and that is also something that's easy to increase you just add more notes so these are the the two theoretical limits for number of tenants and both are very easy to overcome because both linearly scale with the number of notes in a cluster yeah it's so interesting hearing about the uh like try to break it test I think that's been one of the like one of my favorite stories following along with weaviate has been like you know the sphere tests and you know blog posts about that and you know trying to get a billion nodes into eviate and that whole like load testing thing has always been so interesting like I remember it's kind of like a theme of all of AI like you know like with language models it's like how big of a language model can you train it's like oh we train a 50 billion it's like hot so impressive and this is kind of like our analog of that is like how many vectors can we put into a into one index and I think also kind of seeing it you know across indexes and you know one kind of system across the nodes is so interesting um I have a quick kind of clarifying question selfishly for me hopefully there is a listener with it too so when you're sharding a class so I understand that each class you know it's the um you know it's the document class and I have a million documents in it and I have one vector index so when I Shard this you mentioned like searching each chart separately and then aggregating the results somehow can you maybe take me a little through further how you Shard a class so here we really need to um separate multi-tenancy from single tenant cases because in in multi-tenancy we use the these shards as isolation units because you only want to search through one like in in multi-tenancy like your query is for a specific tenant so we'd say there are 100 charts each corresponding to one Tenon we would pick exactly one of those hundred we would have that chart serve the request return it to the user nothing nothing changed so this is kind of the the um like a small portion of the data on the note is queried in isolation in the entire query is completely sort of self-contained in that chart if you Shard a so so for that case it doesn't matter how many charts you have like even if you have a million charts because you're always hitting exactly one there's never any overhead for for number of shards like whether you have whether you query one out of ten or one out of 100 or one of out of 100 000 it's always the same you're always querying one uh but in a single tenant case the motivation for sharding is different so in a single tenant case you don't have the tenant key so the results that you expect are the entire Vector space so now your motivation for sharding is kind of the uh the the other way around like you don't want to have as many charts as possible you want to have as few shorts as possible so for this the question is basically why do you want to so if you if it's better to have fewer shards why do you want to have shards at all and this is uh where where sort of um the the scale of a single index comes comes into place with respect to the hardware that is scheduled on so that was a very very complex way of essentially saying like you can only fit so much on one machine and if you need more than that one machine you need to Shard it across two or three or four so that is the motivation in let's say we have an index of a billion objects and each machine could only fit 250 million then you would chart that across four machines and now if the query comes in all four machines would say hey okay I'll give you the top 10 results out of my 250 million so you would now and end up essentially with four lists of uh each 10 top objects now you have to aggregate that list again that's super easy to do because you have let's say the distance metric so essentially you just Resort that list of 40 and cut off the the bottom 30 again so you remember you have the the top 10 remaining that is easy to do but from a sort of scaling out perspective you need four nodes to serve your query whereas in the multi-tenancy case because only a single node hits sort of owns the the data for your one tenant also only that one tenant needs to answer it which means all the other nodes and all the other CPUs on that node are idle to serve other tenants basically so when you when you have the hsw graph is there anything to how you partition so imagine like on layer zero I have like these are clustered and so we're going to go Shard together is there anything uh the the chart is at a higher level so the the hnsw index would be fully contained within them and it wouldn't even know that there are other charts like the the shards you can think of this like class Shard and then in the chart you would have Vector index uh regular index Etc so the the agent's double Unix doesn't even know that that said this is something I think that that would be interesting for for future research to see like could we Shard instead of sharding by sort of an application Level attribute so right now we use a hash on the ID you could also potentially Shard by Vector proximity so that would be the case where sorry if a bit similar to how IVF based uh indexes worker you have these kind of buckets so you could say if I know that my data set is going to have 10 charts could I try to sort of pre-select one of those 10 shards based on proximity and make sure that the vector sort of ends up in in The Shard that's closest to it problem is what we know from from IVF based indexes is with a multi-dimensional distance it's not as easy like in a sort of it's not a binary decision it belongs into bucket one or maybe along so in bucket ten so in IVF what you need to do is you still need to query multiple buckets like the let's say out of 100 buckets you still need to query the the top 25 closes so that's a like interesting for for for research um but currently that's that's not something so currently the the chart is at a higher level in a in a um single tenant sharded setup and the vector index within that chart wouldn't even know that other shards exist yeah it's really fascinating thinking about the you know like the vector indexes and then how you distribute Vector indexes across Cloud computers across the world and this is all really exciting I think kind of we're transitioning into this like you know future improvements Future Works uh topic broadly I I hopefully we didn't go too into sharding particularly but like back into the multi-tenancy like do you see you know what are some things that are top of mind now that rolling it out yeah there's one aspect that I haven't even mentioned at all yet that I'm super excited about um what what we wanted to achieve with 1.20 was we wanted to have a stable API we wanted to have uh sort of the main functionality which is scale so anyone who was blocked by not being able to to onboard uh or or to to get started with VBA because of lack of multi-tenancy features we wanted to say like you can get started with 1.20 but this is not where multi-tenancy ends basically this is basically just the beginning and one one big sort of potential for improvement that we have is cost reduction because something that if you have a multi-tenancy case most likely not all of your tenants are active at the same time if they are all your tenants would need to sort of Be Active and this is I'm using this fuzzy term of active versus versus inactive because that's that sort of yeah just just as an abstraction we can go into what that that means as well um but if we say that we have active tenants who are potentially expensive like let's say they need a lot of memory they need a lot of compute and we would add inactive tenants which are not expensive because they don't need memory or or they would need the memory at the moment that they become active but in their inactive State they would use fewer resources now what that allows us to do is potentially size vv8 cluster for the number of active tenants as opposed to for the number of total tenants so if you have let's say a hundred thousand tenants and you would say that you're in for a bill for a hundred thousand tenants is one thousand dollars if you know that only ten percent of your tenants are active at the same time and you have a way to deactivate those other ninety percent you could reduce your infer cost from a thousand dollars to a hundred dollars and this is where where it gets really interesting because that's the the cases grow like in Vector search right now almost all cases are always sized for data set like it's always the first questions like how large is your data I said what's your dimensionality how many objects do you have um maybe can you use compression what are your query requirements Etc but kind of the cost is linear to the number of objects but with multi-tenancy in the ability and this is sort of what's what's coming next the the ability to deactivate and then there's there's more things sort of in the pipeline um Beyond just deactivating them but just with deactivating them we would not need any memory for for those tenants anymore so if you can say that even if it's just 50 or so they're inactive you would essentially have twice the number of tens or could host twice the number of tenants on the same hardware and this is where I see a ton of potential for for even more cost saving because realistically there's always this like long tail distribution where you have a few tenons that have a lot of queries and a lot of tenants that maybe query once per minute or once per hour even or maybe just once per day and you can potentially serve them for much much cheaper hardware and that makes it way easier to to reduce operating cost with multi-tenancy hmm yeah that is it's really interesting hearing the hearing the uh yeah like the prioritization of certain resources and so on and I I just kind of was having this Epiphany about how this could help like if you know if I'm building a machine learning app with weavi and I'm collecting user data I'm thinking about you know I just kind of unlocked my brain how much this multi-tenancy enables like you know just as a random example our last movie a podcast was with Alexa gordich from ordis and you know if I'm collecting the user data of everyone interacting with his uh YouTube chat bot I I have this new multi-tensity way of collecting that data and feeding it into my machine Learning System and yeah just incredibly exciting as I am understanding it better from hearing you speaking about it yeah also makes it easier to get the data out for a specific tenant because essentially like a like a list all query is now specific to attendance so you're now listing all the data for for that tenant so yeah I guess like when I was first um thinking about this topic of multi-tenancy I hadn't understood the just users with the same kind of class well I hope I'm making this like I I thought about it like say I want to have all these different classes with different properties like for whatever reason Conor has a age property but Eddie and I don't know you don't have you just have like a HomeTown like you know what I'm trying to say it's like different properties or like different uh I'm ageless just a good example yeah like I I guess I was thinking like you know especially with the vectorization configuration I think maybe you know like different classes would vectorize a different property and stuff like that but yeah I think just you know having you know millions of users for the same kind of class that application makes a ton of sense and yeah it's just really really exciting stuff uh awesome so I think that's kind of a Roundup of multi-tenancy edian's written this incredible blog post in addition with the Alice Corp and Bob and car yeah like the examples and uh so you know release post uh release notes all this cool stuff so awesome so pivoting to our next topic um product quantization I think this was first released in 118 or 119 uh you know a super clever Vector compression technique I think the history of we V8 you first started exploring a vector compression with binary passage retrieval where you're like hashing you know positive negative of each Vector Dimension but the problem with that uh product binary passage retrieval I think is you really need to train with it it's not really something that you can do with vectors that just come out of the box from whatever model but this product quantization thing where you uh cluster the values and then represent them with the centroid IDs you can apply that to any kind of vector and so it's really Universal way to compress vectors and just overall really exciting way to kind of reduce the memory required in these Vector indexes um it could maybe start broadly on like where is your head currently at with product quantization yeah yeah as you said we we initially released this uh I think two or so releases ago one 1.18 and we put this we we slapped this experimental label on it and that was not because we didn't trust our implementation that was because we said we're hey we're Gathering feedback we want to see how do users use that what can we potentially do to improve it and and we were well aware that this could come maybe with with some API changes that in the sort of regular semantic versioning API guarantees we couldn't uphold so they said like hey this is experimental this is probably going to change maybe not but this is kind of the the evaluation phase at the same time we don't want to keep it from users here you are here please please do use it be aware that yeah either it would change or maybe you don't use it in production yet unless XYZ and that also in turn meant that at some point that the experimental phase needs to end and it needs to become generally available and that is now with 1.20 and that's the the one big change just in general we incorporated all that feedback um of sort of maybe it wouldn't work as expected in some cases maybe there are ways to to do things better like that is that is one part of of uh where we're at with PQ right now but then the other part is also we've improved it we've added what we call re-scoring and this is a very simple change that has a massive impact so in general PQ is a lossful compression so you reduce the number of let's say standard example you have um a 768 dimensional vector with PQ you would represent four dimensions or this configurable then let's say for example you represent four dimensions with a single byte so now what you have is previously for those four dimensions you use float32s so that was 16 bytes and now you not just do you reduce this or from float 32 into just a single byte but also you pack multiple Dimensions into a single byte so in this simple example you can reduce the memory footprint for 786 Dimensions um by a factor of 16. so that's that's extremely sort of yeah that's an extreme Improvement but at the same time if you if you do that you're recall would drop considerably likely you would you would yeah your memory requirement is is a fraction of what it was before but also your result quality is bad now so what we said is well but we're still using PQ in combination with this H and SW index and in hnsw essentially you have a a top K Heap so you have like a temporary list of what you think is the current results and while traversing the graph you sort of keep um improving upon that list and the final result is essentially the result that we pass to a user so that's a very simplified explanation of of how search results are gathered within hmsw and that means that if we use that that compressed PQ distance to make a decision of whether we wanted something basically whether we want to add it to the list or whether we want to drop something because the list has a fixed size so whenever we add something something that's a worse result needs to drop off and if we do that based on the compressed distance oftentimes that decision is wrong and then this this sort of small compression error would accumulate more and more and more and in the end you would have a lot less recall but if instead if occasionally we actually load the real Vector from disk and use the real Vector to make that that decision so this is the rescoring part we actually take the the original Vector from disk so now you're trading a bit of performance you have a discrete now where previously you wouldn't have a discrete but in exchange you now get way more accurate results and I think we have a blog post coming up in a week or so or maybe two where we have some some graphs in there and it's super exciting because in some cases the uh recall QPS trade-off sort of the these these curves were top into the right is typically best it's actually not any worse than it is without compressed so it's like you you don't lose anything that's not true for every case like in some cases there is still a bit of a trade-off where you can essentially say like am I optimizing more for efficient storage or am I optimizing more for for efficient querying but in in many cases it's actually not a draft anymore you just have to turn it on and this is this is in turn a bit sort of depends a bit on how much memory you have available in general because just of the way that operating systems work like a disk hit only makes it to disk if that that portion of the disk hasn't been cached in memory before if there is memory available the operating systems and it's the the page cache will start caching portions of the disk so what is technically a disk read might actually not even be a discrete if you still have some memory available so you get these like buffers zones in in which the performance is still great even though the memory um usage has dropped a lot and this is the second feature for PQ that we said like okay once we go for General availability we want to want to have that in and now we're at a point where previously it was like use PQ sparingly in some cases because it reduces your Recall now it's more like why would you use PQ please everyone go out and use it it's awesome yeah I think when I first heard PQ rescoring I thought of it in the re-ranking way that we'll talk about next where it's like you know you get the top 100 results with the compressed distances and then you just kind of bring the full Precision to just re-rank I didn't realize how uh deep into the hsw traversal how say you know I I am still like Curious so you're like exploring the neighbors of the center node and you're going to kind of like Resort the candidate I think maybe um yeah I don't know I think it's maybe outside of the scope of the podcast but like just yeah understanding that hsw has this like candidate list and dynamic nearest neighbors and you like rescore it to have a better search like deep into the traversal rather than just at the very end I think very important yeah yeah that's like the Elegance of it so I have kind of two questions I want to ask you about PQ the first is maybe a clarification of the in-memory on disk part of PQ so I'm curious if this is the correct understanding so you know we we load say 200 vectors into Eva and now we trigger PQ we fit the k-mean centroids so from then on do we a new Vector comes into weviate do we just send the full uh full Precision Vector to disk and then the compressed Vector that goes to memory is exactly exactly yeah so we always retain the full Vector basically on on disk like you store what the user provides which is the the the real Vector so to speak but we also compress it and then only keep the compressed one in memory additionally we also need to store the compressed one in memory so if the the server restarts you don't want to recompress everything so we we store both actually but exactly as you say initially we keep the the original on disk and keep the compressed in memory fascinating I remember one of the my takeaways from when we met with Matthias douzay from meta who's one of the pioneers of product quantization was this um like online k-means kind of problem and thinking about like you know how many vectors do you need to load in to compute the centroids and then how does this scale with incremental updates um like I guess the question would be like what are you kind of learning about that problem uh yeah you you need a certain so so it's very hard to pick an exact number because you need your data or that subset of your data needs to somehow be representative of the entire data set so in in the ideal case let's say you would have five clusters in your data naturally then those five clusters and then you want to train on ten percent then those five clusters need to be represented in your original um a ten percent if they're not for whatever reason like if let's say the fifth cluster didn't even exist and you only have four clusters for training then you're training on something where the fifth cluster doesn't exist yet so that doesn't mean the fifth cluster can't be assigned anywhere but it means the the sort of trained model isn't as good as it would be um uh if if that had print part of it the good news is that there's a certain size where it gets more and more likely that the the distribution of the data set sort of doesn't change of course that's no guarantee like it could completely change um but often what you see so for example on on um these a n Benchmark data sets that are typically between 1 and 10 million I think we typically didn't see any Improvement uh when using more than a hundred thousand vectors for for um for training so that that's still a very manageable amount I think it takes 10 seconds or so to train with with a hundred thousand vectors um yeah sort of a good a good trade-off yeah fascinating Yeah a hundred thousand just like getting a sense of that is something that I think is a really interesting research question when you're you know going into the details and yeah I think this whole like cluster analysis is definitely a lot to it I remember you know earlier we did a podcast on Bert topic and we were discussing like what would cluster analysis and weba look like I think it's starting to see I don't want to give away I know that we have something planned but like um you know we're seeing Partners Integrations other technology companies building further into this like embedding space visualization topic modeling cluster analysis and you know definitely people listening to be on the lookout because there's going to be some really cool stuff around that coming soon on embedding drift detection and all this cool stuff um yeah awesome so I think that's a great coverage of the PQ rescoring I think probably for me the biggest takeaway is um you know understanding How Deeply that goes into the hsw traversal and what you said I think is a huge point about originally you were thinking I'm trading off memory for accuracy but now that you know you have this restore ring there's no trade-offs so just use it that's kind of a funny takeaway yeah yeah for for some cases I mean there there is it's engineering so there's always trade-offs and it's kind of like not seeing our users data it's hard to make predictions but we're trying to model real life data usage with with as many different data sets as possible and I'm really surprised to see sort of how good it is in in some cases like yes in some cases there is still a bit of a trade-off but in others it's just not oh yeah yeah I think um having all these diverse data sets is such an interesting thing of machine learning generally like the very is still kind of no free lunch in a way like uh yeah awesome so I think kind of we have a trio of new features I'd like to kind of cluster into one kind of category which is kind of like we V8 being feature Rich for search you know like vector search obviously like search relevance is a huge topic and I kind of just to give a quick tldr of each of these things uh the first of which is re-ranking where so as I mentioned with my original understanding of PQ re-scoring re-ranking is this idea where you take your top say k 100 results from Vector search and then you're going to take each of those candidates in the query and pass each of those as input to a high capacity scoring model so where do these scoring models come from sentence Transformers has six of them that are open source and so we've built the docker image where you know similar to text to back Transformers you you have this like local image if you wanted to spin it up in your laptop host it yourself and all that good stuff we have the you know the cross encoders that have been trained from sentence Transformers are up there and then also really interestingly we have a re-ranking endpoint from cohere and obviously our friendship with Mills rhymers you've seen Eddie and those rhymers together um you know this is a an API for a re-ranking model and I think that's quite fascinating I don't think there's another another company doing that and it's pretty interesting to see that API and you know it comes into like weeviate's module system how we have these um this orchestration of API requests I think I think it's quite fascinating maybe actually adding we could take a pause and I I really wanted to ask you this question about how um I think an interesting thing about we v8's module system is how golang handles these uh like concurrent HTTP requests and maybe talk a bit about like um what goes behind that how golang is optimized for scaling this kind of you know orchestration to model inference because I think it's such a fascinating part if we get in general go is a language as you say sort of easy simple concurrency is one of the building blocks of why you would use go so go has channels and it has a simple synchronization Primitives such as mutexes and and these kind of things that you need for easy concurrency it has a race detector which is also a super important feature to to figure out like are you doing your concurrency safely are there any kind of risks so in general sort of go is a good I would say it it makes it relatively easy and then what that allows us to do is if a user just gives us sort of a a bulk data thingy like like a batch of data objects essentially and our job is to say vectorize them or or also re-rank them I'm going to pass them to to the model if we would do that sequentially that would also mean that whatever the latency is for one of those requests will be bounded by that but the kind of cool thing about these serverless applications is that they tend to scale way better than that so if you hit them concurrently that allows you to to sort of not have to wait for one thing to be finished but just do multiple of them at a time that said again there is a bit of a trade-off because um often these models have rate limits so I think something that that often pops up in our support forum is for example with open AI there the the default rate limit is I don't know what it is but something like I think maybe 60 per second or so that that has a 600 per minute so if you want to vectorize more than that you could potentially hit the the rate limit so also concurrency is not a free lunch here but it it helps to at least sort of max out whatever the the model provider can can use and this is the same for for if you self-host your your model um if you don't concurrently do something and it doesn't necessarily mean that it has to happen with with concurrent HTTP requests that could also mean that you bolt this up into a single request and then maybe the concurrency happens on the model side but in in general you want to sort of not have idle capacity around like like if there are either gpus or CPUs that are used for model inference you want to make sure that they're they're maxed down and that's one of the motivations for using concurrency in in that case and then sort of taking that back to to golang it's a language that makes concurrent programming rather easily I know that that some of our team members sometimes hate go for it because it's it's not perfect in any way but no no language is um but it's it's yeah a very good sort of fundamental concurrency kind of model yeah fascinating I think I also like the um you know like the Gen like our generate module with openai palm the code here I the way that particularly the way that it paralyzes doing the single result call I find a ton of use in that where you want to put a prompt of each of your search results and have that all be parallelized it's so much faster than looping through there yeah you know I think it's pretty cool so okay cool so on this topic again of search results so we have re-ranking now where you can get better search results and you know kind of building it right into weviate is why we took this tangent into the more technical thing you don't have to have you know manage some other service of your own to do re-ranking it's built natively into alleviate and you know taking that search result slices and either way so the next cool thing is auto cut so autocut has this high level motivation of you know how many search results are relevant if you sir you know if you search for I I've stolen Bob's uh landmarks in France thing I use this all the time now so if you search landmarks in front and you have the Eiffel Tower in Paris and you have some other things and you know say you only have like eight thing eight landmarks in France and your search result cutting off that nine and only showing eight results this is incredibly interesting for the language model retrieve blog went to generation thing as well because the language model can be quite thrown off by these irrelevant results so like I did see something about Spain it might write about that because it's in it says based on the search results and it's like well it's there so you know so I think the interesting thing about autocut also is understanding uh how that's done so it's like the um you know it's like calculating the delta in the slope from the point so you take each of the distances and then you have to interpolate a line from there and then you measure the steepest change in the slope so what we've got users will see is a hyper parameter on which of the steepest slopes because you know you have like two to three as each reverse of the thing so it's like how extremely do you want to be cutting it and then the last thing is you know one of our favorite topics is this kind of uh rank Fusion algorithm weeviate has built-in BM 25 as well as Vector search so what that means is you end up with two lists and so you know the first time we did this we were just doing you know just combining it based on the ranks so you know if you're like first ranked and BM 25 and like fourth rank in Vector search that ends up you know just purely based on the ranks and now we have like a relative score Fusion so yeah I think all these things um just all this new uh benefits in search I think the question I'd like to ask again is just kind of where your head's at on yeah like the search stuff I think you know maybe Berlin buzzwords there was some discussions on the search side of things I've been to the haystack conference personally and seen that kind of like you know world around search relevance yeah to to me this is a someone who's who's very use case driven this is for me extremely important that we think of these these use cases and search is a big one it's not the only one there's recommendations there's classifications there's called the of course the whole a generative AI part but generative search then also as a part in there um and for me it's I I like being able to give our users something that they can use end to end where they can say like hey I want to spin up bb-8 I want to have the data management part so you want to have the whole database I want to have the vector search obviously because that's the whole reason of of using bv8 as opposed to say traditional search engine uh the Hybrid part that's such a big one just the the bm25 like not having to spin up something else that that does the pm25 for you but being able to to do that um and then all these new additions and and re-ranking and auto cut these are just sort of the most recent ones but I think there will be way more down the line and a essentially if it makes your search experience better then it's something that we could potentially extend deviate within because of the module system we're very flexible and even adding sort of things that that essentially the module system allows us to add things that not everyone needs without loading VBA because if you don't need the module just don't turn it on so um this this kind of yeah everything that makes search easier to use better and I guess in a way also more accessible because there's like information res information retrieval is like this whole research topic out there and there's there's specialized conferences you just mentioned Berlin Bus words uh which are super fun for us but for someone who just wants better search maybe they're not maybe they don't want to get into the nitty-gritty details maybe they just want to have better search and there we're we're always trying to find that balance of making it easy for someone who's new to the space without sort of taking away the flexibility that someone who's more experienced needs and I think these these new re-ranking modules and then AutoCAD these are great examples because if you have let's say you have your own complex ranking pipeline that's way more complex than something that you could fit into evade just don't use the re-ranking module but for someone else who's already using bb8 maybe doesn't have a complex pipeline says like hey there's something that I can just turn on with a single flag and it makes my search experience better by all means do it that's that's what we're here for yeah yeah yeah I I think there are so many interesting tricks from the information retrieval uh Community we can get out of to get better search I think um I think with re-ranking to maybe stay on it a little more I think maybe some of the search zealots would be a little bit saying you know I still can't do symbolic re-ranking you know like if you're searching for products you might want to have the price and the relevancy these as features that you would put into re-ranking uh so you've seen we've had a podcast with meta rank with Roman and Siva where you know we're thinking about this thing as well it'll interface very similar to this re-ranking with coherent sentence Transformers we'll just send the property to the API as well and and it'll look super similar there's also a pretty interesting discussion around like multi-valued vector search like I have a title Vector as well as a document Vector so you know we're looking into all these things as well as like the the large language model re-ranking thing is quite interesting where you prompt it with kind of the symbolic rule so like please boost it if it has the if it's recent and then you and then you give it the result like Json dictionaries and that does work pretty interestingly I think it's also really exciting for Integrations with like llama index and Lang chain about and Samantha Colonel and Pat make a list of these things but I'll just leave it there about how you can you know just get better search results by just taking these things on and yeah I think also maybe just before uh graduating to our last topic I also just want to thank Dirk for this Dirk did an incredible job with you know the rank Fusion and the auto cut I remember like the other guy was trying to figure it out myself and it was such a headache trying to figure out how to do the um uh get the go the go numb uh thing to do the slope interfellation and I had showed it to Dirk and he was just like the next day like oh yeah here's the notebook to test it I was like that's so fast I've turned it around so really impressive stuff and uh yeah it's awesome so kind of our last topic is the um the cloud monitoring metrics with the Prometheus and I'm curious you know I imagine as you do we mentioned like the trying to break your testing with the multi-tenancy I'm curious like where your head's at with these you know how you you know basically observe this massive thing running in the cloud right yeah it almost feels like one of those boring features compared to all the exciting features that we just talked about but I think this is this is what's so important to to pay attention to the boring stuff because even something like multi-tenancy I mean we're super excited about it but in the end you could argue like from a from a AI perspective this is a very boring feature like it's not a New Concept but these are the kind of features that you need to run reliably in production for me anytime we we add something like that and we we sort of expand on on the observability stack like this is a simple change um essentially it allows you to to track your success query rates like previously you could only monitor I think latency but now you also get just a ratio of what queries have succeeded versus what queries have failed and and the failure reason so was this a user initiated failure so in HTTP status codes this would be like a 400 uh plus a status code or is this did something go wrong in vb8 so a 500 or so so internal server error and these kind of things and the ones from from out there um so so adding the the monitoring for this to get better observability basically on these kind of things it's very boring but it's very essential and I I love seeing bva grow up and and have our users ask for these boring things because in the end boring is production and production makes our customers money and that makes us happy so always always a place in my heart for the boring features yeah I think well yeah I think um like just for me learning about like datadog and learning about these companies that have built around that kind of thing is it's really fast yeah I mean it's it's to me it sounds like kind of like Insurance on your uh thing that you know like your data is up in the cloud it's a super abstract concept of like a little bit of it is in Virginia a little bit of it is in Germany so yeah yeah oh that is yeah data Doc is a massive company so that just goes to show like how much how much need there is or how much how much room there is for for these kind of observability topics exactly as you say it's like insurance it's early warning it's just sort of looking into that the the more complex that your system grows you can only see so much from the outside and sort of getting these insights and just seeing what's going on can help sort of in in hindsight for for debugging cases like what was going on but ideally before something happens like early warning systems like simplest thing uh monitoring for for memory usage or something like you see something goes up nicely in a linear line and you know that there is a certain limit with your capacity if it says 30 maybe that's fine 50 maybe that's fine if it's approaching eighty percent maybe you want to change something about your infrastructure so yeah these kind of operational things that you need um it's it's for me it's it's just such a positive sign of like hey people people do need them which means they're doing serious stuff with bb8 so I'm very happy to always ask our team to prioritize some some observability features yeah well eddian awesome thank you so much for another release podcast multi-tenancy PQ with rescoring re-ranking autocut new hybrid rank fusion and then new updates to the Prometheus Cloud monitoring all so exciting um and also you know you can join the community slack if you need help with any of these things everyone's always around and checking that all the time and uh yeah thanks so much for listening thanks for having me had a blast thank you"
    }
]