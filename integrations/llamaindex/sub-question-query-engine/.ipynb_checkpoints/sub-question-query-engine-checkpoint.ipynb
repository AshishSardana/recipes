{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classes': [{'class': 'BlogPost',\n",
       "   'description': 'Blog post from the Weaviate website.',\n",
       "   'invertedIndexConfig': {'bm25': {'b': 0.75, 'k1': 1.2},\n",
       "    'cleanupIntervalSeconds': 60,\n",
       "    'stopwords': {'additions': None, 'preset': 'en', 'removals': None}},\n",
       "   'moduleConfig': {'generative-openai': {'model': 'gpt-3.5-turbo'},\n",
       "    'text2vec-openai': {'model': 'ada',\n",
       "     'modelVersion': '002',\n",
       "     'type': 'text',\n",
       "     'vectorizeClassName': True}},\n",
       "   'properties': [{'dataType': ['text'],\n",
       "     'description': 'Content from the blog post',\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'content',\n",
       "     'tokenization': 'word'},\n",
       "    {'dataType': ['text'],\n",
       "     'description': \"This property was generated by Weaviate's auto-schema feature on Thu Jul  6 15:56:15 2023\",\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'node_info',\n",
       "     'tokenization': 'word'},\n",
       "    {'dataType': ['text'],\n",
       "     'description': \"This property was generated by Weaviate's auto-schema feature on Thu Jul  6 15:56:15 2023\",\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'relationships',\n",
       "     'tokenization': 'word'},\n",
       "    {'dataType': ['text'],\n",
       "     'description': \"This property was generated by Weaviate's auto-schema feature on Thu Jul  6 15:56:15 2023\",\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'document_id',\n",
       "     'tokenization': 'word'},\n",
       "    {'dataType': ['text'],\n",
       "     'description': \"This property was generated by Weaviate's auto-schema feature on Thu Jul  6 15:56:15 2023\",\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'doc_id',\n",
       "     'tokenization': 'word'},\n",
       "    {'dataType': ['text'],\n",
       "     'description': \"This property was generated by Weaviate's auto-schema feature on Thu Jul  6 15:56:15 2023\",\n",
       "     'indexFilterable': True,\n",
       "     'indexSearchable': True,\n",
       "     'moduleConfig': {'text2vec-openai': {'skip': False,\n",
       "       'vectorizePropertyName': False}},\n",
       "     'name': 'ref_doc_id',\n",
       "     'tokenization': 'word'}],\n",
       "   'replicationConfig': {'factor': 1},\n",
       "   'shardingConfig': {'virtualPerPhysical': 128,\n",
       "    'desiredCount': 1,\n",
       "    'actualCount': 1,\n",
       "    'desiredVirtualCount': 128,\n",
       "    'actualVirtualCount': 128,\n",
       "    'key': '_id',\n",
       "    'strategy': 'hash',\n",
       "    'function': 'murmur3'},\n",
       "   'vectorIndexConfig': {'skip': False,\n",
       "    'cleanupIntervalSeconds': 300,\n",
       "    'maxConnections': 64,\n",
       "    'efConstruction': 128,\n",
       "    'ef': -1,\n",
       "    'dynamicEfMin': 100,\n",
       "    'dynamicEfMax': 500,\n",
       "    'dynamicEfFactor': 8,\n",
       "    'vectorCacheMaxObjects': 1000000000000,\n",
       "    'flatSearchCutoff': 40000,\n",
       "    'distance': 'cosine',\n",
       "    'pq': {'enabled': False,\n",
       "     'bitCompression': False,\n",
       "     'segments': 0,\n",
       "     'centroids': 256,\n",
       "     'encoder': {'type': 'kmeans', 'distribution': 'log-normal'}}},\n",
       "   'vectorIndexType': 'hnsw',\n",
       "   'vectorizer': 'text2vec-openai'}]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\n",
    "  url=\"https://llamaindex-sub-question-demo-drx6wazb.weaviate.network\",  # URL to Weaviate instance\n",
    ")\n",
    "\n",
    "client.schema.get()  # Get the schema to test connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema was created.\n"
     ]
    }
   ],
   "source": [
    "schema = {\n",
    "   \"classes\": [\n",
    "       {\n",
    "           \"class\": \"BlogPost\",\n",
    "           \"description\": \"Blog post from the Weaviate website.\",\n",
    "           \"vectorizer\": \"text2vec-openai\",\n",
    "           \"moduleConfig\": {\n",
    "               \"generative-openai\": { \n",
    "                    \"model\": \"gpt-3.5-turbo\"\n",
    "                }\n",
    "           },\n",
    "           \"properties\": [\n",
    "               {\n",
    "                  \"name\": \"Content\",\n",
    "                  \"dataType\": [\"text\"],\n",
    "                  \"description\": \"Content from the blog post\",\n",
    "               }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "client.schema.delete_all()\n",
    "\n",
    "client.schema.create(schema)\n",
    "\n",
    "print(\"Schema was created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/pkg_resources/_vendor/jaraco/text/__init__.py:593: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.11/site-packages/llama_index/readers/llamahub_modules/web/simple_web/requirements.txt' mode='r' encoding='UTF-8'>\n",
      "  for item in lines:\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from llama_index import download_loader, SimpleWebPageReader\n",
    "\n",
    "SimpleWebPageReader = download_loader(\"SimpleWebPageReader\")\n",
    "\n",
    "loader = SimpleWebPageReader()\n",
    "blog = loader.load_data(urls=['https://weaviate.io/blog/llamaindex-and-weaviate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the Documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "parser = SimpleNodeParser()\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(blog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index import VectorStoreIndex, StorageContext\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "import os\n",
    "\n",
    "os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# construct vector store\n",
    "vector_store = WeaviateVectorStore(weaviate_client = client, index_name=\"BlogPost\", text_key=\"content\")\n",
    "\n",
    "# setting up the storage for the embeddings\n",
    "storage_context = StorageContext.from_defaults(vector_store = vector_store)\n",
    "\n",
    "# set up the index\n",
    "index = VectorStoreIndex(nodes, storage_context = storage_context)\n",
    "\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Sub Question Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.query_engine import SubQuestionQueryEngine \n",
    "\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine = query_engine, \n",
    "        metadata = ToolMetadata(name='BlogPost', description='Blog post about the integration of LlamaIndex and Weaviate')\n",
    "    )\n",
    "]\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(query_engine_tools=query_engine_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 2 sub questions.\n",
      "\u001b[36;1m\u001b[1;3m[BlogPost] Q: What is the integration of LlamaIndex and Weaviate\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[BlogPost] A: \n",
      "The integration of LlamaIndex and Weaviate is a combination of Weaviate as the vector database that acts as the external storage provider and LlamaIndex as a data framework for building LLM applications. LlamaIndex provides a comprehensive toolkit for ingestion, management, and querying of external data so that it can be used with an LLM app. It offers connectors to 100+ data sources, ranging from different file formats to APIs to web scrapers. It also supports indexing unstructured, semi-structured, and structured data. Finally, it offers the ability to query the data with a wide variety of data structures and storage integration options.\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[BlogPost] Q: How does LlamaIndex help data indexing in Weaviate\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m[BlogPost] A: \n",
      "LlamaIndex helps data indexing in Weaviate by providing a comprehensive toolkit for ingestion, management, and querying of external data. It supports indexing unstructured, semi-structured, and structured data, and offers connectors to 100+ data sources, ranging from different file formats (.pdf, .docx, .pptx) to APIs (Notion, Slack, Discord, etc.) to web scrapers (Beautiful Soup, Readability, etc.). These data connectors are primarily hosted on LlamaHub, making it easy for users to integrate data from their existing files and applications.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = await query_engine.aquery('How does LlamaIndex help data indexing in Weaviate?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LlamaIndex helps data indexing in Weaviate by providing a comprehensive toolkit for ingestion, management, and querying of external data. It offers connectors to 100+ data sources, ranging from different file formats (.pdf, .docx, .pptx) to APIs (Notion, Slack, Discord, etc.) to web scrapers (Beautiful Soup, Readability, etc.). These data connectors are primarily hosted on LlamaHub, making it easy for users to integrate data from their existing files and applications. It also supports indexing unstructured, semi-structured, and structured data. Finally, it offers the ability to query the data with a wide variety of data structures and storage integration options.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
