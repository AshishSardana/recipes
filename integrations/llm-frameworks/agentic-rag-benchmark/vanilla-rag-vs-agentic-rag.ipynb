{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RAG versus Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will compare Vanilla RAG with Agentic RAG on the task of answering questions about Weaviate.\n",
    "\n",
    "Both systems are connected to a Weaviate Database instance containing chunks of Weaviate's blog posts. These blog posts can help answer questions such as: \"How does BM25 work?\", \"What was released in Weaviate 1.27?\", or \"What is Retrieval-Augmented Generation?\", to give a few examples.\n",
    "\n",
    "We use an LLM-as-Judge to determine which answer to a question is better, the Vanilla RAG answer or the Agentic RAG answer. Both systems use the GPT-4o Large Language Model.\n",
    "\n",
    "We find that **Agentic RAG wins 74% of the questions**, with **33 wins** where **Vanilla RAG only achieves 12 wins**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Data into Weaviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/weaviate/warnings.py:133: DeprecationWarning: Dep005: You are using weaviate-client version 2.5.1.dev2434+g7589c2f. The latest version is 4.9.3.\n",
      "            Consider upgrading to the latest version. See https://weaviate.io/developers/weaviate/client-libraries/python for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import weaviate.classes.config as wvcc\n",
    "\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "\n",
    "# Create Schema\n",
    "if weaviate_client.collections.exists(\"WeaviateBlogChunk\"):\n",
    "    weaviate_client.collections.delete(\"WeaviateBlogChunk\") \n",
    "\n",
    "collection = weaviate_client.collections.create(\n",
    "    name=\"WeaviateBlogChunk\",\n",
    "    vectorizer_config=wvcc.Configure.Vectorizer.text2vec_transformers(),\n",
    "    properties=[\n",
    "            wvcc.Property(name=\"content\", data_type=wvcc.DataType.TEXT),\n",
    "            wvcc.Property(name=\"author\", data_type=wvcc.DataType.TEXT),\n",
    "      ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Break a list into chunks of the specified size.\"\"\"\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentences using regular expressions.\"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def read_and_chunk_index_files(main_folder_path):\n",
    "    \"\"\"Read index.md files from subfolders, split into sentences, and chunk every 5 sentences.\"\"\"\n",
    "    blog_chunks = []\n",
    "    for folder_name in os.listdir(main_folder_path):\n",
    "        subfolder_path = os.path.join(main_folder_path, folder_name)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            index_file_path = os.path.join(subfolder_path, 'index.mdx')\n",
    "            if os.path.isfile(index_file_path):\n",
    "                with open(index_file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    sentences = split_into_sentences(content)\n",
    "                    sentence_chunks = chunk_list(sentences, 5)\n",
    "                    sentence_chunks = [' '.join(chunk) for chunk in sentence_chunks]\n",
    "                    blog_chunks.extend(sentence_chunks)\n",
    "    return blog_chunks\n",
    "\n",
    "# Example usage\n",
    "main_folder_path = './blog'\n",
    "blog_chunks = read_and_chunk_index_files(main_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1874"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(blog_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: 'Accelerating Vector Search up to +40% with Intel’s latest Xeon CPU - Emerald Rapids'\n",
      "slug: intel\n",
      "authors: [zain, asdine, john]\n",
      "date: 2024-03-26\n",
      "image: ./img/hero.png\n",
      "tags: ['engineering', 'research']\n",
      "description: 'Boosting Weaviate using SIMD-AVX512, Loop Unrolling and Compiler Optimizations'\n",
      "---\n",
      "\n",
      "![HERO image](./img/hero.png)\n",
      "\n",
      "**Overview of Key Sections:**\n",
      "- [**Vector Distance Calculations**](#vector-distance-calculations) Different vector distance metrics popularly used in Weaviate. - [**Implementations of Distance Calculations in Weaviate**](#vector-distance-implementations) Improvements under the hood for implementation of Dot product and L2 distance metrics. - [**Intel’s 5th Gen Intel Xeon Processor, Emerald Rapids**](#enter-intel-emerald-rapids)  More on Intel's new 5th Gen Xeon processor. - [**Benchmarking Performance**](#lets-talk-numbers) Performance numbers on microbenchmarks along with simulated real-world usage scenarios. What’s the most important calculation a vector database needs to do over and over again?\n"
     ]
    }
   ],
   "source": [
    "print(blog_chunks[0]) # 1 500 Token Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What simple operation does it spend the majority of its time performing? If you guessed **vector distance calculations** … BINGO! 🎉\n",
      "\n",
      "While vector databases use many techniques and algorithms to improve performance (including locality graphs, quantization, hash based approaches), at the end of the day, efficient distance calculations between high-dimensional vectors is a requirement for a good vector database. In fact, when profiling Weaviate indexed using HNSW, we find that 40%-60% of the CPU time is spent doing vector distance calculations. So when someone tells us that they can make this quintessential process *much faster* they have our full attention! If you want to learn how to leverage algorithmic and hardware optimizations to make vector search 40% faster keep reading!\n",
      "\n",
      "In this post we’ll do a technical deep dive into different implementations for vector distance calculations and optimizations enabled by Intel’s new 5th Gen Xeon Processor - Emerald Rapids, parallelization techniques using SIMD with the Intel AVX-256 and Intel AVX-512 instruction sets, loop unrolling and compiler optimizations by transpiling C to Go assembly. We explain how we attained a **~40% QPS speed up at 90% Recall** in Weaviate running on Intel’s new Xeon Processor, Emerald Rapids.\n"
     ]
    }
   ],
   "source": [
    "print(blog_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/weaviate/warnings.py:329: ResourceWarning: Con004: The connection to Weaviate was not closed properly. This can lead to memory leaks.\n",
      "            Please make sure to close the connection using `client.close()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from weaviate.util import get_valid_uuid\n",
    "from uuid import uuid4\n",
    "\n",
    "blogs = weaviate_client.collections.get(\"WeaviateBlogChunk\")\n",
    "\n",
    "for idx, blog_chunk in enumerate(blog_chunks):\n",
    "    upload = blogs.data.insert(\n",
    "        properties={\n",
    "            \"content\": blog_chunk\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build Vanilla RAG and Agentic RAG Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools model used for OpenAI Function Calling API\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional, Literal\n",
    "\n",
    "class ParameterProperty(BaseModel):\n",
    "    type: str\n",
    "    description: str\n",
    "    enum: Optional[list[str]] = None\n",
    "\n",
    "\n",
    "class Parameters(BaseModel):\n",
    "    type: Literal[\"object\"]\n",
    "    properties: dict[str, ParameterProperty]\n",
    "    required: Optional[list[str]]\n",
    "\n",
    "\n",
    "class Function(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    parameters: Parameters\n",
    "\n",
    "\n",
    "class Tool(BaseModel):\n",
    "    type: Literal[\"function\"]\n",
    "    function: Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then implement an LMService to capture:\n",
    "\n",
    "- Generation from a Prompt\n",
    "- Generation from a Prompt with a Structured Output Model\n",
    "- Function Calling Loop Generation (our Agentic RAG System)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class LMService():\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str,\n",
    "            model_provider: str,\n",
    "            api_key: str\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.model_provider = model_provider\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        match self.model_provider:\n",
    "            case \"openai\":\n",
    "                from openai import OpenAI\n",
    "                self.client = OpenAI(api_key=self.api_key)\n",
    "            case _:\n",
    "                raise ValueError(f\"Unsupported model_provider: {model_provider}\")\n",
    "\n",
    "    def generate(\n",
    "            self,\n",
    "            prompt: str\n",
    "    ) -> str:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are a helpful assistant. Use the supplied tools to assist the user.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    def generate_with_output_model(\n",
    "            self,\n",
    "            prompt: str,\n",
    "            output_model: BaseModel\n",
    "    ):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are a helpful assistant. Use the supplied tools to assist the user.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        response = self.client.beta.chat.completions.parse(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            response_format=output_model\n",
    "        )\n",
    "        parsed_response = response.choices[0].message.parsed\n",
    "        parsed_response = parsed_response.json()\n",
    "        return parsed_response\n",
    "        \n",
    "\n",
    "    def generate_with_function_calling_loop(\n",
    "            self,\n",
    "            prompt: str,\n",
    "            tools: list[Tool],\n",
    "            tools_mapping: dict\n",
    "    ) -> str:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant. Use the supplied tools to assist the user.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "        calls, call_budget = 0, 20\n",
    "    \n",
    "        # Initial call to get first response\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=messages,\n",
    "            tools=tools\n",
    "        ).choices[0]\n",
    "\n",
    "        while calls < call_budget:\n",
    "            message = response.message\n",
    "            \n",
    "            if not message.tool_calls:\n",
    "                return message.content\n",
    "            \n",
    "            # Add assistant message with tool calls\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": message.content if message.content else None,\n",
    "                \"tool_calls\": [\n",
    "                    {\n",
    "                        \"id\": tool_call.id,\n",
    "                        \"type\": \"function\", \n",
    "                        \"function\": {\n",
    "                            \"name\": tool_call.function.name,\n",
    "                            \"arguments\": tool_call.function.arguments\n",
    "                        }\n",
    "                    } for tool_call in message.tool_calls\n",
    "                ]\n",
    "            })\n",
    "            \n",
    "            # Handle parallel function calls\n",
    "            for tool_call in message.tool_calls:\n",
    "                function_to_call = tools_mapping[tool_call.function.name]\n",
    "                tool_arguments = json.loads(tool_call.function.arguments)\n",
    "                function_response = function_to_call(**tool_arguments)\n",
    "                \n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": function_response,\n",
    "                    \"tool_call_id\": tool_call.id\n",
    "                })\n",
    "            \n",
    "            # Get next response\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                tools=tools\n",
    "            ).choices[0]\n",
    "            \n",
    "            calls += 1\n",
    "        \n",
    "        return \"Exceeded maximum number of function calls\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "lm_service = LMService(\n",
    "    model_name=\"gpt-4o\",\n",
    "    model_provider=\"openai\",\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "print(lm_service.generate(\"say hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"greeting\":\"Hello!\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:1138: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# The Structured Output makes it so the Language Model can only output either `hello how are you` or `Hello!`\n",
    "# We use this for our LLM-as-Judge to determine which System produces the better answer to a technical question about Weaviate.\n",
    "\n",
    "class StructuredHello(BaseModel):\n",
    "    greeting: Literal[\"hello how are you\", \"Hello!\"]\n",
    "\n",
    "print(lm_service.generate_with_output_model(\n",
    "    prompt=\"say hello\",\n",
    "    output_model=StructuredHello)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Vanilla RAG` Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this code is written to demonstrate the functionality in a jupyter runtime\n",
    "# For prodcution deployments you would want to decouple the client connection.\n",
    "# To graduate to Lakehouse-style querying in Weaviate, you remove the collection name internal to the search function.\n",
    "\n",
    "def search_blogs(search_query: str):\n",
    "    search_collection = weaviate_client.collections.get(\"WeaviateBlogChunk\")\n",
    "    results = search_collection.query.hybrid(\n",
    "        query=search_query,\n",
    "        limit=5\n",
    "    )\n",
    "    stringified_response = \"\"\n",
    "    for idx, o in enumerate(results.objects):\n",
    "        stringified_response += f\"Search Result: {idx+1}:\\n\"\n",
    "        for prop in o.properties:\n",
    "            stringified_response += f\"{prop}:{o.properties[prop]}\"\n",
    "        stringified_response += \"\\n\"\n",
    "    \n",
    "    return stringified_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search Result: 1:\n",
      "content:At the time of writing this article in early 2021, the first vector index type that's supported is HNSW. By choosing this particular type, one of the limitations is already overcome: HNSW supports querying while inserting. This is a good basis for mutability, but it's not all. Existing HNSW libraries fall short of full CRUD-support. Updating is not possible at all and deleting is only mimicked by marking an object as deleted without cleaning it up.author:None\n",
      "Search Result: 2:\n",
      "content:Furthermore, the most popular library hnswlib only supports snapshotting, but not individual writes to disk. To get to where Weaviate is today, a custom HNSW implementation was needed. It follows the same principles [as outlined in this paper](https://arxiv.org/abs/1603.09320) but extends it with more features. Each write is added to a [write-ahead log](https://martinfowler.com/articles/patterns-of-distributed-systems/wal.html). Additionally, since inserts into HNSW are not mutable by default, Weaviate internally assigns an immutable document ID that allows for updates.author:None\n",
      "Search Result: 3:\n",
      "content:14**: *The chart shows Recall (vertical axis) Vs Indexing time (in minutes, on the horizontal axis). For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n",
      "\n",
      "### Memory compression results\n",
      "\n",
      "To explore how the memory usage changes with the HNSW+PQ feature, we compare the two versions: uncompressed HNSW and HNSW plus compression using the KMeans encoder. We only compare KMeans using the same amount of segments as dimensions. All other settings could achieve a little bit of a higher compression rate but since we do not compress the graph, it is not significant for these datasets. Keep in mind that the whole graph built by HNSW is still hosted in memory.author:None\n",
      "Search Result: 4:\n",
      "content:But once the database got to around 25 million objects, adding new objects would be significantly slower. Then from 50–100m, the import process would slow down to a walking pace. #### Solution\n",
      "To address this problem, we changed how the HNSW index grows. We implemented a relative growth pattern, where the HNSW index size increases by either 25% or 25'000 objects (whichever is bigger). ![HNSW index growth chart](./img/hnsw-index-growth.jpg)\n",
      "\n",
      "#### Test\n",
      "After introducing the relative growth patterns, we've run a few tests.author:None\n",
      "Search Result: 5:\n",
      "content:### Vector Index Configuration\n",
      "Since our application didn’t require high recall, we opted in to save costs by enabling PQ. There are a few [PQ parameters](/developers/weaviate/config-refs/schema/vector-index#pq-configuration-parameters) that you can configure like `trainingLimit`, `segments`, and `distribution`. Additionally, you can configure the [HNSW parameters](/developers/weaviate/config-refs/schema/vector-index#hnsw-index-parameters) like the `distance`, `ef`, `efConstruction`, and more. It is important to note that some parameters cannot be changed once the collection is created. When configuring HNSW, you should consider a few questions:\n",
      "1.author:None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(search_blogs(\"Hnsw\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_rag(\n",
    "        search_query: str,\n",
    "        lm_service: LMService\n",
    "    ) -> str:\n",
    "    search_query = search_query\n",
    "    context = search_blogs(search_query)\n",
    "    prompt = f\"\"\"Assess the context and answer the question.\n",
    "\n",
    "    [[ question ]]\n",
    "    {search_query}\n",
    "\n",
    "    [[ context ]]\n",
    "    {context}\n",
    "\n",
    "    [[ answer ]]\"\"\"\n",
    "    answer = lm_service.generate(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HNSW stands for Hierarchical Navigable Small World, which is a type of vector index used for approximate nearest neighbor search. It is known for its efficient querying while allowing concurrent insertions, making it a good choice for mutable datasets. However, HNSW typically lacks full CRUD (Create, Read, Update, Delete) support; for example, updating is generally not supported, and deletions are only simulated by marking objects as deleted. The underlying principle of HNSW is based on building a graph where each node is connected to its nearest neighbors, which helps in navigating the space efficiently to find approximate nearest neighbors.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_rag(\n",
    "    search_query=\"What is HNSW?\",\n",
    "    lm_service=lm_service\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Agentic RAG` Quick Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [Tool(\n",
    "    type=\"function\",\n",
    "    function=Function(\n",
    "        name=\"search_blogs\",\n",
    "        description=\"Search a Vector Database containing blog posts information about Weaviate.\",\n",
    "        parameters=Parameters(\n",
    "            type=\"object\",\n",
    "            properties={\n",
    "                \"search_query\": ParameterProperty(\n",
    "                    type=\"string\",\n",
    "                    description=\"The natural language query to search for in the database\"\n",
    "                )\n",
    "            },\n",
    "            required=[\"search_query\"]\n",
    "        )\n",
    "    )\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_mapping = {\n",
    "    \"search_blogs\": search_blogs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HNSW stands for \"Hierarchical Navigable Small World\" graph, which is an algorithm used for approximate nearest neighbor search in high-dimensional spaces. It is widely used in vector search engines and databases due to its efficient performance in handling large datasets.\\n\\nHNSW operates by building a layered graph structure where each layer represents a different level of abstraction, allowing for rapid navigation through the data to find the closest points. The higher layers contain fewer nodes and provide broader connections between different regions, while the lower layers hold more detailed information and connectivity.\\n\\nThe key features of HNSW include:\\n\\n1. **Hierarchical Structure:** Nodes (data points) are arranged in a hierarchy, enabling fast traversal during search queries.\\n\\n2. **Navigable Small World Properties:** The graph is constructed in a way that mimics the small-world phenomenon, where most nodes can be reached from every other by a small number of steps. This ensures fast search capabilities.\\n\\n3. **Efficient Index Construction and Querying:** HNSW provides a good balance between search speed and accuracy, making it suitable for real-time applications.\\n\\n4. **Scalability:** The algorithm is designed to handle large-scale datasets efficiently.\\n\\nHNSW is widely used in vector search implementations like those provided by vector databases, such as Weaviate, that require efficient high-dimensional similarity searches.'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_service.generate_with_function_calling_loop(\n",
    "    prompt=\"What is HNSW?\",\n",
    "    tools=tools,\n",
    "    tools_mapping=tools_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluate Agentic RAG vs. Vanilla RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# Maybe want to add `retrieved_contexts` for eval sake\n",
    "class Winner(BaseModel):\n",
    "    winner: Literal[\"vanilla rag\", \"agentic rag\"]\n",
    "\n",
    "class RAGEvalModel(BaseModel):\n",
    "    query: str\n",
    "    response: str\n",
    "    win: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"weaviate/WeaviateBlogRAG-0-0-0\")[\"train\"] # Please leave a heart if you find this dataset useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Note, the current implementation of hybrid search in Weaviate uses BM25/BM25F and vector search. If you’re interested to learn about how dense vector indexes are built and optimized in Weaviate, check out this [article](/blog/why-is-vector-search-so-fast). ### BM25\\nBM25 builds on the keyword scoring method [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Term-Frequency Inverse-Document Frequency) by taking the [Binary Independence Model](https://en.wikipedia.org/wiki/Binary_Independence_Model) from the IDF calculation and adding a normalization penalty that weighs a document’s length relative to the average length of all the documents in the database. The image below presents the scoring calculation of BM25:\\n![BM25 calculation](./img/BM25-calculation.png) <div align=\"center\"><i>Source: Wikipedia page on Okapi BM25</i></div>\\n\\nThe score of the document, query pair is determined by weighing the uniqueness of each keyword in the query relative to the collection of texts. BM25 contains additional static parameters, k1 and b that may help calibrate performance to particular datasets.',\n",
       " 'gold_answer': \"The Binary Independence Model in the BM25 algorithm used by Weaviate's hybrid search plays a crucial role in the calculation of the Inverse Document Frequency (IDF). It is used to weigh the uniqueness of each keyword in the query relative to the collection of texts by adding a normalization penalty that weighs a document’s length relative to the average length of all the documents in the database. This helps in determining the score of the document, query pair, thereby aiding in the retrieval of relevant documents.\",\n",
       " 'query': \"What is the role of the Binary Independence Model in the BM25 algorithm used by Weaviate's hybrid search?\"}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'Note, the current implementation of hybrid search in Weaviate uses BM25/BM25F and vector search. If you’re interested to learn about how dense vector indexes are built and optimized in Weaviate, check out this [article](/blog/why-is-vector-search-so-fast). ### BM25\\nBM25 builds on the keyword scoring method [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (Term-Frequency Inverse-Document Frequency) by taking the [Binary Independence Model](https://en.wikipedia.org/wiki/Binary_Independence_Model) from the IDF calculation and adding a normalization penalty that weighs a document’s length relative to the average length of all the documents in the database. The image below presents the scoring calculation of BM25:\\n![BM25 calculation](./img/BM25-calculation.png) <div align=\"center\"><i>Source: Wikipedia page on Okapi BM25</i></div>\\n\\nThe score of the document, query pair is determined by weighing the uniqueness of each keyword in the query relative to the collection of texts. BM25 contains additional static parameters, k1 and b that may help calibrate performance to particular datasets.', 'gold_answer': \"The Binary Independence Model in the BM25 algorithm used by Weaviate's hybrid search plays a crucial role in the calculation of the Inverse Document Frequency (IDF). It is used to weigh the uniqueness of each keyword in the query relative to the collection of texts by adding a normalization penalty that weighs a document’s length relative to the average length of all the documents in the database. This helps in determining the score of the document, query pair, thereby aiding in the retrieval of relevant documents.\", 'query': \"What is the role of the Binary Independence Model in the BM25 algorithm used by Weaviate's hybrid search?\"}\n",
      "\u001b[96mQuery: What is the role of the Binary Independence Model in the BM25 algorithm used by Weaviate's hybrid search?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The Binary Independence Model (BIM) plays a foundational role in the BM25 algorithm used by Weaviate's hybrid search. The BIM is used in the IDF (Inverse Document Frequency) component of the BM25 scoring mechanism. Essentially, it provides a way to calculate the significance of a term within a collection of documents by assuming that the presence or absence of a term in a document is an independent binary event. BM25 extends this model by introducing a normalization factor that accounts for document length compared to the average document length in the collection, improving the effectiveness of the keyword scoring used in search.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The Binary Independence Model (BIM) plays a crucial role in the BM25 algorithm, which is used in Weaviate's hybrid search. BM25 builds on the keyword scoring method known as TF-IDF (Term Frequency-Inverse Document Frequency). Specifically, it uses principles derived from the Binary Independence Model in its Inverse Document Frequency (IDF) calculation. BM25 further extends this by adding a normalization penalty that adjusts a document's length relative to the average length of all documents in the database. This helps in weighing the uniqueness of each keyword in a query relative to the collection of texts, effectively improving search results by taking the independence of terms and document lengths into account.\n",
      "\n",
      "The BM25 formula includes static parameters such as \"k1\" and \"b\" that can be tuned to better suit specific datasets. This approach, combined with other techniques, makes it a powerful component of the hybrid search functionality in Weaviate, which balances keyword and vector-based search methods.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/main.py:1138: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 0 wins\n",
      "Agentic RAG: 1 wins\n",
      "After 1 rounds\n",
      "\n",
      "{'source': 'Updatability: The index data is immutable, and thus no real-time updates are possible. 2. Scalability: Most vector libraries cannot be queried while importing your data, which can be a scalability concern for applications that require importing millions or even billions of objects. Thus, vector libraries are a great solution for applications with a limited static snapshot of data. However, if your application requires real-time scalable semantic search at the production level, you should consider using a vector database.', 'gold_answer': 'Vector libraries might not be suitable for applications that require real-time updates and scalable semantic search because they have immutable index data, preventing real-time updates. They also cannot be queried while importing data, posing a scalability concern for applications that need to import large amounts of data. Furthermore, they lack full CRUD support, which is necessary for real-time scalable semantic search.', 'query': 'Why might vector libraries not be suitable for applications that require real-time updates and scalable semantic search?'}\n",
      "\u001b[96mQuery: Why might vector libraries not be suitable for applications that require real-time updates and scalable semantic search?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Vector libraries might not be suitable for applications that require real-time updates and scalable semantic search due to several limitations:\n",
      "\n",
      "1. **Lack of Real-Time Updates:** Most vector libraries cannot manage data efficiently in scenarios where data is frequently updated or changed. They typically store vector embeddings in in-memory indexes, which are not designed for dynamic updates. This means they cannot handle real-time data updates efficiently, making them less suitable for applications that require frequent and ongoing data modifications.\n",
      "\n",
      "2. **Scalability Issues:** Vector libraries generally do not support queries while importing or updating data. This limitation presents a scalability challenge, especially for applications that need to handle millions or even billions of objects. Such applications require systems that can manage and query extensive datasets in real-time, an area where vector databases excel.\n",
      "\n",
      "3. **Limited CRUD Operations:** Unlike vector databases, vector libraries often lack full CRUD (create, read, update, delete) capabilities. This restricts their use in applications where data must be continuously updated and manipulated, and where CRUD operations are essential for maintaining data integrity and performance.\n",
      "\n",
      "Therefore, for applications that demand real-time updates, enhanced scalability, and flexible data management, vector databases are a more suitable choice over vector libraries.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Vector libraries are not always suitable for applications that require real-time updates and scalable semantic search due to several limitations:\n",
      "\n",
      "1. **Inability to Query During Data Import**: Most vector libraries cannot be queried while data is being imported. This limitation can pose a scalability concern for applications requiring the handling of millions or billions of objects. This makes vector libraries more suitable for applications that operate with a limited, static dataset.\n",
      "\n",
      "2. **Limited Data Updating Capabilities**: Vector libraries often lack full CRUD (create, read, update, delete) support. They are typically designed to handle static snapshots of data, making them challenging to use in applications where data changes frequently or requires real-time updates.\n",
      "\n",
      "3. **In-memory Storage**: Vector libraries store vector embeddings in in-memory indexes to perform similarity searches. This approach can become unsustainable for scaling applications that require managing large datasets persistently and efficiently.\n",
      "\n",
      "4. **Algorithmic Limitations**: While vector libraries use various Approximate Nearest Neighbor (ANN) algorithms to index vectors (such as clustering-based, tree-based, or compression-based indexes), constructing these indexes can be time-consuming, impacting their suitability for dynamic applications.\n",
      "\n",
      "For applications that need real-time updates and large-scale search operations, vector databases are a more suitable alternative. They are designed to overcome these limitations by providing full CRUD capabilities and better handling of dynamic data operations in production environments.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 0 wins\n",
      "Agentic RAG: 2 wins\n",
      "After 2 rounds\n",
      "\n",
      "{'source': \"I recommend checking out the GitHub repository to test this out yourself!\\n\\n## Additional Resources\\n• [LangChain Guide](https://www.commandbar.com/blog/langchain-projects) by Paul from CommandBar. import StayConnected from '/_includes/stay-connected.mdx'\\n\\n<StayConnected />\", 'gold_answer': 'The document recommends the \"LangChain Guide\" by Paul from CommandBar for learning about LangChain projects.', 'query': 'What guide does the document recommend for learning about LangChain projects?'}\n",
      "\u001b[96mQuery: What guide does the document recommend for learning about LangChain projects?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The document recommends the [LangChain Guide](https://www.commandbar.com/blog/langchain-projects) by Paul from CommandBar for learning about LangChain projects.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The document recommends the [LangChain Guide](https://www.commandbar.com/blog/langchain-projects) by Paul from CommandBar for learning about LangChain projects.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"vanilla rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 1 wins\n",
      "Agentic RAG: 2 wins\n",
      "After 3 rounds\n",
      "\n",
      "{'source': 'If we compress the vectors then the memory requirements goes down to the 1572 MB to 2129 MB range. After compression, recall drops to values ranging from 0.7337 to 0.9545. Latency rises up to the 7521 to 37402 microsends range. A summary is shown in Table 3 below. |                       |              | Recall100@100 | Latency ($\\\\mu s$)         | Memory required (MB)         |\\n|-----------------------|--------------|---------------|---------------------------|------------------------------|\\n| Sift1M Low params     | Uncompressed | 0.91561       | 293                       | 1277                         |\\n|                       | Compressed   | 0.91361       | 401               (x1.36) | 610                 (47.76%) |\\n| Sift1M High params    | Uncompressed | 0.99974       | 1772                      | 1674                         |\\n|                       | Compressed   | 0.99658       | 1937             (x1.09)  | 1478               (88.29%)  |\\n| DeepImage Low params  | Uncompressed | 0.8644        | 827                       | 9420                         |\\n|                       | Compressed   | 0.85666       | 1039             (x1.25)  | 4730               (50.21%)  |\\n| DeepImage High params | Uncompressed | 0.99757       | 2601                      | 15226                        |\\n|                       | Compressed   | 0.97023       | 2708             (x1.04)  | 12367             (81.22%)   |\\n| Gist Low params       | Uncompressed | 0.74461       | 2133                      | 4218                         |\\n|                       | Compressed   | 0.73376       | 7521             (x3.52)  | 1572              (37.26%)   |\\n| Gist High params      | Uncompressed | 0.99628       | 15539                     | 5103                         |\\n|                       | Compressed   | 0.95455       | 37402           (x2.40)   | 2129               (41.72%)  |\\n\\n**Tab.', 'gold_answer': 'The percentage reduction in memory requirements for the Gist dataset with low parameters after compression is approximately 62.74%.', 'query': 'What is the percentage reduction in memory requirements for the Gist dataset with low parameters after compression?'}\n",
      "\u001b[96mQuery: What is the percentage reduction in memory requirements for the Gist dataset with low parameters after compression?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "To calculate the percentage reduction in memory requirements for the Gist dataset after compression, we need the initial uncompressed memory requirement and the compressed memory requirement:\n",
      "\n",
      "- Uncompressed memory requirement for the Gist dataset: 4218 MB to 5103 MB\n",
      "- Compressed memory requirement in the range mentioned is 4730 MB to 12367 MB, but for the Gist dataset specifically, another section mentions a compression rate better suited to Gist, though exact compressed figures aren't clearly specified from that input derived directly suggesting Gist compressed works around mention of uncompressed 3840 MB (contextual assumption in practicality of details).\n",
      "\n",
      "Using any specific comparative compression guide suggestion context like potential result surrounds compressed values:\n",
      "- Assuming here: uncompressed example value can be approx 3840 MB as practical reference header.\n",
      "\n",
      "If the practical guidance context suppose notable compression of which we might assume improved compression closer aligned to practical experiences including mention earlier on 4:1 guidance.\n",
      "- If above mutual inference: Assuming a practical reference compression sometimes as better suited:\n",
      "\n",
      "Now make practical narrative assumes calculation may direct also fraction concept:\n",
      "- Let's denote a general practical compression improvement level, following a very ideal reference context.\n",
      "- Assume initially equipped aligned goal could refer for practical comparative highlight.\n",
      "\n",
      "Real starting comparative distinct frame possibly use for memory reduction rate may theoretically observed like:\n",
      "- Assume e.g., sites wider references notion for perhaps achieving high quality cause variations typically exist when evoking theoretical ideal suggest best based real initiatives, assume on practical alignment.\n",
      "\n",
      "As this generalized hypothetical ideal guidance follows:\n",
      "- Assume reach derived a broad conceptual frame seeing if practical calculation notion like situational generalized hypothetical across stated around within prevalent highlight accurative mapping.\n",
      "- This result based typical thriving so say implying frequently aligning 75% compression rate advisory.\n",
      "\n",
      "Practical considering hypothetical fraction follows like assuming both coherent agreed selection assume generalized ultimate contexts, practical suggestions:\n",
      "   \n",
      "Take example memory reduction:\n",
      "- Practical supposed narrative reduces uncompressed value per operational lower side 3840 MB, general inferred reduction seen 75% whole derived may refer structure.\n",
      "\n",
      "Use hypothetical reduction rate concept like:\n",
      "- 75% as instructed aligned.\n",
      "  \n",
      "Hence this result viewed entity:\n",
      "- Approximately suggests conceivable potentially referenceably guided content may establishing logic reason to consistent assessment.\n",
      "- Convert prior hypothetical compress generalized into approximate narrative direction derive potential output reference.\n",
      "\n",
      "**Hypothetical reduction formula view showed like: percentage reduction = calculated specific comparison**:\n",
      "- 75% applicable structurally scenario seen way practical applicable framework notion.\n",
      "\n",
      "Note: Exactly highlighting ultimate practical relevance frequently conceptual proactive themes aided upon particular dataset, real depth calculations based evolving comparable impression acquired contextually further yield crystalized data methodological pattern insights qualitative context where statement experience invested adherence quality enclosure completeness yields interpretive returns into value.\n",
      "\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The memory requirements for the Gist dataset are significantly reduced after compression with low parameters, requiring only approximately 15% of the memory needed under uncompressed settings. This reduction is part of achieving a compression ratio of about 6.6x.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 1 wins\n",
      "Agentic RAG: 3 wins\n",
      "After 4 rounds\n",
      "\n",
      "{'source': '### Testing queries\\n\\nSemantic (`nearText`) searches may be one of the most common (if not *the* most common) searches our users perform. So let\\'s see how we might test semantic searches. A semantic search requires vectorizing the query, so a test will validate the integration with the vectorizer (`text2vec-openai` in this case). We\\'ll run a query for \"chemistry\" and check that the top result is about \"sodium\". :::info Will the top result always be the same?', 'gold_answer': 'To test semantic search functionality and result consistency using `text2vec-openai`, you would run a query and check the top result. For example, if you run a query for \"chemistry\", you might check if the top result is about \"sodium\". However, due to the nature of semantic search and language understanding, the top result may not always be the same. This could be due to various factors such as the complexity of the query, the database\\'s content, and the specific algorithms used in the `text2vec-openai` module. Therefore, testing should involve running multiple queries and checking the relevance and consistency of the results.', 'query': 'How do you test semantic search functionality and result consistency using `text2vec-openai`?'}\n",
      "\u001b[96mQuery: How do you test semantic search functionality and result consistency using `text2vec-openai`?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "To test semantic search functionality and result consistency using `text2vec-openai`, you would typically follow these steps:\n",
      "\n",
      "1. **Vectorizing Queries**: Begin by vectorizing the search query using the `text2vec-openai` library, which transforms the text query into numerical vectors. This is crucial for semantic search as it relies on vector representations to find conceptually similar matches.\n",
      "\n",
      "2. **Integration Validation**: Ensure that the integration with the vectorizer (`text2vec-openai`) is working correctly. This involves checking that the queries are being accurately transformed into vectors and that the search mechanism utilizes these vectors for retrieving results.\n",
      "\n",
      "3. **Semantic Search Execution**: Perform a semantic search using the vectorized query. For instance, if you run a query for \"chemistry,\" you should receive search results based on semantic relevance.\n",
      "\n",
      "4. **Result Verification**: Compare the retrieved results for consistency and relevance. Specifically, you could check whether the top result is about a related topic like \"sodium\" as indicated in the context. This consistency can indicate that the semantic search is functioning correctly.\n",
      "\n",
      "5. **Handling Variability**: Determine whether the top result is consistently the same across multiple search attempts. It is noted in the context that the top result may not always be the same due to potential variability in how vectors are used and how semantic similarities are calculated.\n",
      "\n",
      "By methodically vectorizing, executing searches, and verifying the consistency of results, you can effectively test the semantic search functionality and its integration using `text2vec-openai`.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "To test the semantic search functionality and result consistency using `text2vec-openai`, you can follow this approach based on insights from the search results:\n",
      "\n",
      "1. **Integration with Vectorizer**: \n",
      "   - You'll need to ensure correct integration with the vectorizer, in this case, `text2vec-openai`. This involves checking that your queries are properly converted into numerical vectors using this tool.\n",
      "\n",
      "2. **Running Semantic Search Tests**:\n",
      "   - Perform semantic searches using example queries. For instance, you might run a query for \"chemistry\" and verify that the top result matches your expectations (e.g., finding content closely related like \"sodium\").\n",
      "\n",
      "3. **Validation of Results**:\n",
      "   - Confirm that the most relevant results consistently appear at the top. This can be a bit tricky because semantic search may yield some variations in results depending on the dataset size and updates.\n",
      "\n",
      "4. **Consistency in Results**:\n",
      "   - While testing, note that the top results may vary slightly due to the nature of vector representations capturing subtle differences in meaning or context. However, significant disparities should be examined for potential issues in model queries or vectorizing inconsistencies.\n",
      "\n",
      "By focusing on these aspects, you can verify both the functionality and the consistency of semantic searches using the `text2vec-openai` model.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"vanilla rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 2 wins\n",
      "Agentic RAG: 3 wins\n",
      "After 5 rounds\n",
      "\n",
      "{'source': 'For sure it might not be for everybody and every use case. But if you are using Weaviate at scale, in production, we believe enabling it will add significant value and encourage you to consider its use.', 'gold_answer': \"The document recommends that users running Weaviate at scale in production should enable its use due to the significant value it can add. They should consider the flexibility it offers during ingestion time, such as extracting PDFs or applying stemming to their BM25 and hybrid search. Users should also take advantage of Weaviate's ability to handle large-scale performance, run complex queries with strict latency requirements, and its advanced monitoring and replication capabilities. The document also suggests using Weaviate's unique search features to drive performance and efficiency.\", 'query': 'What does the document recommend for users running Weaviate at scale in production?'}\n",
      "\u001b[96mQuery: What does the document recommend for users running Weaviate at scale in production?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The document recommends that for users running Weaviate at scale in production, Weaviate should be run on Kubernetes. You can use Helm, and there is an officially supported Weaviate Helm Chart available.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The document recommends several strategies for running Weaviate at scale in production:\n",
      "\n",
      "1. **Persistence**: Weaviate ensures every write operation is persisted, so if there's an interruption like a machine crash, operations can resume without data loss. This reliability feature is essential for maintaining progress during large-scale operations.\n",
      "\n",
      "2. **Modularity**: Weaviate is built on modularity, particularly with its ANN vector index models, allowing flexibility and scalability in its use.\n",
      "\n",
      "3. **Multi-Tenancy**: With a shift in large-scale usage, Weaviate supports native multi-tenancy, allowing multiple tenants to operate on a single setup. This feature is crucial for large-scale and enterprise users requiring massive scale operations.\n",
      "\n",
      "4. **Containerization**: Although Weaviate can technically run without Docker, it is officially supported and recommended to run within containerized environments for reliability and scalability.\n",
      "\n",
      "5. **Monitoring and Observability**: In production, Weaviate supports monitoring through a cloud-native approach. This includes:\n",
      "   - Publishing Prometheus metrics.\n",
      "   - Utilizing Kubernetes liveness and readiness checks.\n",
      "   - Configuration via environment variables.\n",
      "   \n",
      "6. **Cluster Setup**: For production use, especially for semantic search solutions, setting up a Kubernetes cluster is advisable for better scalability and management.\n",
      "\n",
      "These recommendations indicate a focus on reliability, scalability, and efficient management for large-scale Weaviate deployments in production environments.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 2 wins\n",
      "Agentic RAG: 4 wins\n",
      "After 6 rounds\n",
      "\n",
      "{'source': 'Since around [80% of all data is unstructured](https://www.forbes.com/sites/forbestechcouncil/2019/01/29/the-80-blind-spot-are-you-ignoring-unstructured-organizational-data/), it is hard to actually search and retrieve insights from most of the data. The Weaviate vector database unlocks the potential of unstructured data. Searching by fuzzy terms and classification of rich data like free text becomes possible with Weaviate. It uses AI-driven indexing and search technologies to enable real-time text processing. With machine learning methods, Weaviate automatically classifies texts.', 'gold_answer': 'Around 80% of all data is unstructured. Weaviate helps in processing unstructured data by using AI-driven indexing and search technologies for real-time text processing. It also uses machine learning methods to automatically classify texts, making it possible to search and retrieve insights from unstructured data.', 'query': 'What percentage of data is unstructured, and how does Weaviate help in processing it?'}\n",
      "\u001b[96mQuery: What percentage of data is unstructured, and how does Weaviate help in processing it?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Approximately 80% of today's data is unstructured. Weaviate, an open-source vector database, aids in processing this unstructured data by enabling semantic search and automatic classification. It creates a network of knowledge by storing data along with its derived context, such as linguistic information and relationships to other concepts. This enables the classification and real-time processing of unstructured data like free text, making it possible to search and retrieve insights effectively.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Approximately 80% of today's data is unstructured. Unstructured data includes information such as images, text (including documents, social media posts, or emails), audio files, sensor data, and videos. This type of data is challenging to store and organize in a way that allows for easy retrieval and analysis.\n",
      "\n",
      "Weaviate helps in processing unstructured data by using AI-driven indexing and search technologies. It enables real-time text processing, allowing for searches by fuzzy terms and classification of complex data like free text. Weaviate uses machine learning methods to automatically classify texts, providing a way to unlock the potential of unstructured data. \n",
      "\n",
      "By representing data in vector embeddings, Weaviate facilitates the understanding, indexing, and retrieval of unstructured data, transforming it into actionable insights. Through this process, organizations can leverage unstructured data for better decision-making and improved operations.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 2 wins\n",
      "Agentic RAG: 5 wins\n",
      "After 7 rounds\n",
      "\n",
      "{'source': 'Particularly from my conversation with Nils Reimers, I have become very interested in the continual learning nature of this. For example, when we released the `ref2vec` module and discussed it on the podcast, the `all-miniLM-L6-v2` model has never seen ref2vec before in its training set. Additionally, a model fine-tuned up to podcast #30 will have never seen ref2vec either!\\n\\n    I am also very interested in the fine-tuning of cross-encoder models, which you can learn more about [here](/blog/cross-encoders-as-reranker). 3. Custom Benchmarking\\n\\n\\tI have also been working on the BEIR benchmarking in Weaviate (nearly finished!).', 'gold_answer': 'The module that was discussed on the podcast that the `all-miniLM-L6-v2` model had not been trained on is `ref2vec`.', 'query': 'What module was discussed on the podcast that the `all-miniLM-L6-v2` model had not been trained on?'}\n",
      "\u001b[96mQuery: What module was discussed on the podcast that the `all-miniLM-L6-v2` model had not been trained on?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The module discussed on the podcast that the `all-miniLM-L6-v2` model had not been trained on is the `ref2vec` module.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The search results did not provide specific information about a podcast or a module that the `all-miniLM-L6-v2` model had not been trained on. Would you like me to search further or assist with something else?\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"vanilla rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 5 wins\n",
      "After 8 rounds\n",
      "\n",
      "{'source': 'In other words, the User vector is being updated in real-time here to take into account their preferences and actions, which helps to produce more relevant results at speed. Another benefit of Ref2Vec is that this calculation is not compute-heavy, leading to low overhead. With Ref2Vec, you can use Weaviate to provide Recommendation with \"user-as-query\". This is a very common and powerful way to build Home Feed style features in apps. This can be done by sending queries like this to Weaviate:\\n\\n```graphql\\n{\\n  Get {\\n    Product (\\n      nearObject: {\\n        id: \"8abc5-4d5...\" # id for the User object with vector defined by ref2vec-centroid\\n      }\\n    ) {\\n      product_name\\n      price\\n    }\\n  }\\n}\\n```\\n\\nThis short query encapsulates the power of Ref2Vec.', 'gold_answer': 'Ref2Vec is a method that infers a centroid vector from a user\\'s references to other vectors. This vector is updated in real-time to reflect the user\\'s preferences and actions. Ref2Vec integrates with Weaviate through the \"user-as-query\" method, where the user\\'s vector is used as a query to fetch relevant products. This method ensures that the user\\'s vector remains up-to-date with their latest interests, providing personalized recommendations with low computational overhead. Ref2Vec also helps in overcoming the cold-start problem by providing personalization to new users after a few interactions on the app.', 'query': 'What is Ref2Vec, and how does it integrate with Weaviate to provide real-time user-based recommendations?'}\n",
      "\u001b[96mQuery: What is Ref2Vec, and how does it integrate with Weaviate to provide real-time user-based recommendations?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Ref2Vec, short for reference-to-vector, is a method that vectorizes a data object by aggregating its cross-references to other objects. Specifically, it uses the centroid, or average vector, of the cross-referenced vectors to represent the object in focus. In the context of Weaviate, a vector database and search engine, this means that the user vector can be updated in real-time to incorporate their preferences and actions. This allows Weaviate to provide more relevant, real-time recommendations with low computational overhead.\n",
      "\n",
      "The integration of Ref2Vec with Weaviate enables a \"user-as-query\" approach, which is useful for creating recommendation systems such as home feed features in applications. By updating the user's vector in real-time and using it in queries, Ref2Vec captures a user's interests and tendencies across various axes (e.g., product categories, fashion styles). This real-time, dynamic updating aids in generating recommendations that are more closely aligned with the user's current preferences, leading to a more personalized experience. This is achieved with ease in Weaviate, showcasing its ability to provide fast and high-quality recommendations.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Ref2Vec is a feature within Weaviate that provides real-time user-based recommendations by leveraging vector-based user representations. Here's how it works and integrates with Weaviate:\n",
      "\n",
      "1. **Real-Time Updates**: Ref2Vec updates the user vector in real-time to reflect their preferences and actions. This allows for more relevant and timely recommendations.\n",
      "\n",
      "2. **Efficiency**: The computation involved in updating user vectors with Ref2Vec is not compute-heavy, resulting in low overhead.\n",
      "\n",
      "3. **User-as-Query Approach**: Ref2Vec enables recommendations by using the \"user-as-query\" approach, which is common for creating Home Feed style features in applications. It allows the system to understand and respond to individual user preferences dynamically.\n",
      "\n",
      "4. **GraphQL Query**: Ref2Vec supports queries like the following, where a User object is identified, and recommendations are generated based on an aggregated vector representation of the user's interactions:\n",
      "\n",
      "   ```graphql\n",
      "   {\n",
      "     Get {\n",
      "       Product (\n",
      "         nearObject: {\n",
      "           id: \"user-object-id\" # Example ID for the User object\n",
      "         }\n",
      "       ) {\n",
      "         product_name\n",
      "         price\n",
      "       }\n",
      "     }\n",
      "   }\n",
      "   ```\n",
      "\n",
      "5. **Aggregation of Preferences**: It aggregates the user's preferences, like purchased items, into a unified vector representation. This vector captures user tendencies across various axes, such as product categories or styles, enabling personalized recommendations.\n",
      "\n",
      "6. **Recommendation Systems**: Weaviate, known as a vector database and search engine, thus powers high-quality, fast recommendations using Ref2Vec.\n",
      "\n",
      "7. **Cold-Start Problem**: Ref2Vec also helps in overcoming the cold-start problem by providing personalization after just a few user interactions, enabling new users to receive personalized recommendations quickly.\n",
      "\n",
      "This capability allows applications built on Weaviate to offer adaptive and personalized user experiences, enhancing engagement by aligning product recommendations closely with user preferences.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 3 wins\n",
      "Agentic RAG: 6 wins\n",
      "After 9 rounds\n",
      "\n",
      "{'source': 'We have three documents labeled `A`, `B`, and `C` and have run a BM25 and Dense search. In this example, we have set the constant *k* to 0. | BM25 Ranking | Dense Ranking | Results\\n| --- | --- | --- |\\n| A | B | A: 1/1 + 1/3 = 1.3 |\\n| B | C | B: 1/2 + 1/1 = 1.5 |\\n| C | A | C: 1/3 + 1/2 = 0.83|\\n\\nThe above table shows the ranked order of the BM25 and Dense search. To fuse the two lists together, we need to take the sum of the reciprocal ranks. Based on the results, the top document is `Document B` with a ranking of 1.5, then `Document A` at 1.3, and `Document C` at 0.83.', 'gold_answer': 'The BM25 and Dense search rankings are combined using reciprocal ranks by calculating the sum of the reciprocal ranks of each document in both lists. The resulting order of documents, from highest to lowest ranking, is B, A, C.', 'query': 'How are BM25 and Dense search rankings combined using reciprocal ranks, and what is the resulting order of documents A, B, and C?'}\n",
      "\u001b[96mQuery: How are BM25 and Dense search rankings combined using reciprocal ranks, and what is the resulting order of documents A, B, and C?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "In the context provided, BM25 and Dense search rankings are combined using reciprocal ranks. To achieve this, the rank of each document from both BM25 and Dense search is converted into its reciprocal rank (i.e., 1 divided by the rank). Then, the reciprocal ranks from both searches are summed for each document.\n",
      "\n",
      "Here's how it works for documents A, B, and C:\n",
      "1. **Document A:** BM25 Rank = 1, Dense Rank = 3\n",
      "   - Reciprocal sum = \\( \\frac{1}{1} + \\frac{1}{3} = 1.33 \\)\n",
      "2. **Document B:** BM25 Rank = 2, Dense Rank = 1\n",
      "   - Reciprocal sum = \\( \\frac{1}{2} + \\frac{1}{1} = 1.5 \\)\n",
      "3. **Document C:** BM25 Rank = 3, Dense Rank = 2\n",
      "   - Reciprocal sum = \\( \\frac{1}{3} + \\frac{1}{2} = 0.83 \\)\n",
      "\n",
      "When these scores are used to determine the combined ranking, Document B, with a reciprocal sum of 1.5, is ranked first. Document A, with a reciprocal sum of 1.33, comes second, and Document C, with a reciprocal sum of 0.83, is third.\n",
      "\n",
      "Therefore, the resulting order of the documents is B, A, C.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "To combine BM25 and Dense search rankings using reciprocal ranks, you perform the following steps:\n",
      "\n",
      "1. **Determine Ranks from Both Search Types:**\n",
      "   - Each document is ranked according to both the BM25 search and the Dense search.\n",
      "   \n",
      "2. **Calculate Reciprocal Ranks:**\n",
      "   - For each ranking type, determine the reciprocal of the rank for each document.\n",
      "   \n",
      "3. **Combine Reciprocal Ranks:**\n",
      "   - Sum the reciprocal ranks from both search types for each document.\n",
      "\n",
      "Here's an example with three documents A, B, and C:\n",
      "\n",
      "- **BM25 Ranking:**\n",
      "  - A: 1st\n",
      "  - B: 2nd\n",
      "  - C: 3rd\n",
      "\n",
      "- **Dense Ranking:**\n",
      "  - B: 1st\n",
      "  - C: 2nd\n",
      "  - A: 3rd\n",
      "\n",
      "**Reciprocal Rank Calculation:**\n",
      "\n",
      "- Document A: \\( \\frac{1}{1} \\) from BM25 and \\( \\frac{1}{3} \\) from Dense, resulting in \\( 1 + 0.33 = 1.33 \\)\n",
      "- Document B: \\( \\frac{1}{2} \\) from BM25 and \\( \\frac{1}{1} \\) from Dense, resulting in \\( 0.5 + 1 = 1.5 \\)\n",
      "- Document C: \\( \\frac{1}{3} \\) from BM25 and \\( \\frac{1}{2} \\) from Dense, resulting in \\( 0.33 + 0.5 = 0.83 \\)\n",
      "\n",
      "**Final Combined Order:**\n",
      "\n",
      "- Document B comes first with a combined score of 1.5.\n",
      "- Document A follows with a combined score of 1.33.\n",
      "- Document C is last with a combined score of 0.83.\n",
      "\n",
      "Therefore, the resulting order of documents A, B, and C when combining BM25 and Dense search using reciprocal ranks is B, A, C.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"vanilla rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 4 wins\n",
      "Agentic RAG: 6 wins\n",
      "After 10 rounds\n",
      "\n",
      "{'source': 'The IDE shows us the available options and their descriptions. import invertedIndexHintsImgUrl from \\'./img/inverted-index-hints.png\\';\\n\\n<img src={invertedIndexHintsImgUrl} alt=\"Tool tip hints for inverted index configurations\" width=\"85%\"/>\\n\\nTypes are introduced for the data objects as well at creation time, as well as when retrieving them from the database. This means that you can access the properties of the data object directly. So syntax that is currently like this:\\n\\n:::note Classic client syntax\\n```python\\nresponse[\\'data\\'][\\'Get\\'][\\'Article\\'][0][\\'title\\']  # Get the `title` property of the first object\\nresponse[\\'data\\'][\\'Get\\'][\\'Article\\'][0][\\'_additional\\'][\\'id\\']  # Get the ID of the first object\\nresponse[\\'data\\'][\\'Get\\'][\\'Article\\'][0][\\'_additional\\'][\\'generate\\'][\\'singleResult\\']  # Get the generated text from a `singlePrompt` request\\nresponse[\\'data\\'][\\'Get\\'][\\'Article\\'][0][\\'_additional\\'][\\'generate\\'][\\'groupedResult\\']  # Get the generated text from a `groupedTask` request\\n```\\n\\n:::\\n\\nBecome:\\n\\n:::info `Collections` client syntax\\n\\n```python\\nresponse.objects[0].properties[\\'title\\']  # Get the `title` property of the first object\\nresponse.objects[0].uuid  # Get the ID of the first object\\nresponse.objects[0].generated  # Get the generated text from a `singlePrompt` request\\nresponse.generated  # Get the generated text from a `groupedTask` request\\n```\\n\\n:::\\n\\nWe think that these changes will reduce errors, increase productivity, and make the code easier to read and understand. ### Collections-first approach\\n\\nThe other big change is that the `collections` client focuses on individual collections for interaction.', 'gold_answer': \"The syntax differences between the classic client and the collections client for accessing data object properties are as follows:\\n\\n- In the classic client, the syntax for accessing the `title` property of the first object is `response['data']['Get']['Article'][0]['title']`, while in the collections client, the syntax is `response.objects[0].properties['title']`.\\n- In the classic client, the syntax for getting the ID of the first object is `response['data']['Get']['Article'][0]['_additional']['id']`, while in the collections client, the syntax is `response.objects[0].uuid`.\\n- In the classic client, the syntax for getting the generated text from a `singlePrompt` request is `response['data']['Get']['Article'][0]['_additional']['generate']['singleResult']`, while in the collections client, the syntax is `response.objects[0].generated`.\\n- In the classic client, the syntax for getting the generated text from a `groupedTask` request is `response['data']['Get']['Article'][0]['_additional']['generate']['groupedResult']`, while in the collections client, the syntax is `response.generated`.\", 'query': 'What are the syntax differences between the classic client and the collections client for accessing data object properties in the described programming environment?'}\n",
      "\u001b[96mQuery: What are the syntax differences between the classic client and the collections client for accessing data object properties in the described programming environment?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The syntax differences between the classic client and the collections client for accessing data object properties in the described programming environment are as follows:\n",
      "\n",
      "### Classic Client Syntax\n",
      "- Uses dictionary-like keys to access properties:\n",
      "  - For example, to get the `title` property: `response['data']['Get']['Article'][0]['title']`\n",
      "  - To get the ID: `response['data']['Get']['Article'][0]['_additional']['id']`\n",
      "  - To access generated text: `response['data']['Get']['Article'][0]['_additional']['generate']['singleResult']`\n",
      "\n",
      "### Collections Client Syntax\n",
      "- Utilizes object-oriented access:\n",
      "  - Access properties directly from objects list: `response.objects[0].properties['title']`\n",
      "  - Access the ID as: `response.objects[0].uuid`\n",
      "  - Access generated text: `response.objects[0].generated` or `response.generated`\n",
      "- Simplifies the access pattern by reducing the need for deeply nested dictionary keys and special characters like `_additional`.\n",
      "\n",
      "These changes make the code more readable, intuitive, and reduce the potential for errors. Additionally, it supports a collections-first approach where operations are methods originating from a collection object rather than specifying the collection name in every request.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The syntax differences between the classic client and the collections client for accessing data object properties are as follows:\n",
      "\n",
      "### Classic Client Syntax\n",
      "- **Accessing Properties**: Use a chain of method calls with the `with_` prefix. For example, to access the title of the first object:\n",
      "  ```python\n",
      "  response['data']['Get']['Article'][0]['title']\n",
      "  ```\n",
      "- **Using Additional Metadata**: Access additional data such as IDs or generated text via nested dictionaries:\n",
      "  ```python\n",
      "  response['data']['Get']['Article'][0]['_additional']['id']\n",
      "  ```\n",
      "- **Retriever Syntactic Style**: The classic client uses an extensive builder pattern with methods like `.withGenerate()`, `.withNearText()`, etc.\n",
      "\n",
      "### Collections Client Syntax\n",
      "- **Accessing Properties**: Simplifies access by directly using object properties. For example:\n",
      "  ```python\n",
      "  response.objects[0].properties['title']\n",
      "  ```\n",
      "- **Using Additional Metadata**: Access directly as attributes:\n",
      "  ```python\n",
      "  response.objects[0].uuid\n",
      "  response.objects[0].generated\n",
      "  ```\n",
      "- **Focused on Collections**: Each collection is treated as an object. You retrieve or query by calling methods on the collection object, which simplifies the code and removes the necessity of specifying parameters like class name repeatedly.\n",
      "  ```python\n",
      "  articles = client.collection.get('Article')\n",
      "  response = articles.query.fetchObjects(limit=2)\n",
      "  ```\n",
      "- **Return All Non-reference Properties**: By default, all non-reference properties and object IDs are returned, but you can specify properties to optimize the response payload.\n",
      "- **Method Replacements**: The collections syntax replaces the builder-pattern methods with more straightforward, parameterized standalone methods.\n",
      "\n",
      "These changes improve readability, reduce errors, and enhance productivity due to more straightforward interactions and less nested data retrieval compared to the classic client.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 4 wins\n",
      "Agentic RAG: 7 wins\n",
      "After 11 rounds\n",
      "\n",
      "{'source': 'It significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks. 2. It requires much less compute while achieving better performance (<30% of DALLE)\\n3. MM-RAG capable models also generate images much more faithful to the retrieved context - meaning the quality of the generated images is better and grounded in the retrieved context image. 4.', 'gold_answer': 'The model that outperforms DALL-E and CM3 in image and caption generation tasks while requiring less computational power and producing images more faithful to the retrieved context is MM-RAG.', 'query': 'Which model outperforms DALL-E and CM3 in image and caption generation tasks while requiring less computational power and producing images more faithful to the retrieved context?'}\n",
      "\u001b[96mQuery: Which model outperforms DALL-E and CM3 in image and caption generation tasks while requiring less computational power and producing images more faithful to the retrieved context?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The model in question is MM-RAG. It outperforms DALL-E and CM3 in image and caption generation tasks, requires less computational power, and produces images more faithful to the retrieved context.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The model you are referring to is the \"Kosmos-2\" model. Kosmos-2 outperforms models like DALL-E and CM3 in image and caption generation tasks. It requires less computational power and produces images that are more faithful to the retrieved context.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 4 wins\n",
      "Agentic RAG: 8 wins\n",
      "After 12 rounds\n",
      "\n",
      "{'source': '---\\ntitle: An Overview on RAG Evaluation\\nslug: rag-evaluation\\nauthors: [erika, connor]\\ndate: 2023-11-21\\ntags: [concepts]\\nimage: ./img/hero.png\\ndescription: \"Learn about new trends in RAG evaluation and the current state of the art.\"\\n---\\n![hero](img/hero.png)\\n\\n<!-- truncate -->\\n\\nRetrieval Augmented Generation (RAG) is picking up steam as one of the most popular applications of Large Language Models and Vector Databases. RAG is the process of augmenting inputs to a Large Language Model (LLM) with context retrieved from a vector database, like [Weaviate](https://weaviate.io/). RAG applications are commonly used for chatbots and question-answering systems. Like any engineering system, evaluating performance is crucial to the development of RAG applications. The RAG pipeline is broken down into three components: 1.', 'gold_answer': 'Retrieval Augmented Generation (RAG) is a process that uses a vector database to store and retrieve object embeddings, allowing a language model to read relevant information before generating a response. This enables the scaling of the knowledge of large language models without the need for constant training or fine-tuning. The benefits of RAG include scalability, accuracy, controllability, and interpretability. Common applications of RAG include chatbots and question-answering systems.', 'query': 'What is Retrieval Augmented Generation and what are its common applications?'}\n",
      "\u001b[96mQuery: What is Retrieval Augmented Generation and what are its common applications?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Retrieval-Augmented Generation (RAG) is a framework designed to enhance the performance of generative language models by providing them with additional, task-relevant data retrieved from external sources. This framework combines two phases: retrieval and generation. \n",
      "\n",
      "- **Retrieval**: In this phase, the system uses a retrieval component to gather pertinent data from external sources such as databases, files, or online content. These sources provide the additional context required to inform the model’s response.\n",
      "\n",
      "- **Augmentation and Generation**: Once the relevant information is retrieved, it is integrated into the model’s context through a prompt template, allowing the model to generate a response. During this generation phase, the model produces a response that is informed by both its internal knowledge and the newly retrieved external information.\n",
      "\n",
      "Common applications of RAG include:\n",
      "\n",
      "1. **Improving factual accuracy**: By accessing current and specific information, RAG helps the model generate responses that are more factual and accurate than relying solely on pre-existing knowledge.\n",
      "   \n",
      "2. **Reducing hallucinations**: By providing relevant context, RAG reduces the likelihood of the model generating incorrect or fabricated information.\n",
      "   \n",
      "3. **Citations and Evidence**: The ability to cite sources in responses, providing users with traceable evidence of the information presented. \n",
      "\n",
      "4. **Customized Applications**: It is used in applications that require up-to-date information not available in the training data, such as live news updates or personalized recommendation systems.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Retrieval Augmented Generation (RAG) is a method that combines information retrieval and natural language generation. It involves retrieving relevant information from a large collection of documents or data sources and then using this information to generate coherent and contextually relevant text. This approach enhances the capabilities of natural language generation models by providing them with external knowledge, allowing them to produce more accurate and informative content.\n",
      "\n",
      "Common Applications of Retrieval Augmented Generation include:\n",
      "\n",
      "1. **Question Answering Systems**: RAG is used to improve the accuracy of question answering systems by retrieving relevant documents or text snippets before generating an answer, ensuring the response is informed by current and factual data.\n",
      "\n",
      "2. **Chatbots and Conversational Agents**: By accessing up-to-date information, RAG enables chatbots to provide more precise and context-aware responses, leading to more engaging conversations with users.\n",
      "\n",
      "3. **Content Creation**: RAG can assist in creating content for articles, reports, or any textual material by drawing upon a wide range of sources to integrate into the generated text, improving the depth and richness of the material.\n",
      "\n",
      "4. **Research and Information Compilation**: It can automate the process of gathering relevant information from vast datasets, which can then be summarized or used in compiling research papers and reports.\n",
      "\n",
      "5. **Personalized Recommendations**: In applications such as recommendation engines, RAG can tailor suggestions to individuals by combining retrieval mechanisms with generation algorithms to produce personalized content recommendations.\n",
      "\n",
      "6. **Document Summarization**: By retrieving key points from lengthy texts and generating summaries, RAG helps in efficiently summarizing documents while retaining essential information.\n",
      "\n",
      "RAG systems effectively leverage the strengths of both retrieval and generation models to enhance the quality and relevance of the generated output, making them valuable in various domains requiring dynamic and informed text generation.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 4 wins\n",
      "Agentic RAG: 9 wins\n",
      "After 13 rounds\n",
      "\n",
      "{'source': 'More importantly, every tenant has a dedicated high-performance vector index providing query speeds as if the tenant was the only user on your cluster. With more features in the pipeline, such as tenant-specific rate limiting or tenant-specific replication factors, you can customize performance per tenant even further. ### GDPR-compliant and efficient deletes\\nWhen discussing solutions made for many users, our first intuition is to worry about how we onboard and serve them. But deleting them is equally important – both for technical and legal reasons. Take GDPR as an example.', 'gold_answer': 'The system ensures fast query speeds for individual tenants by providing each tenant with a dedicated high-performance vector index and allowing for resource isolation. Upcoming features that will allow further customization of tenant performance include tenant-specific rate limiting and tenant-specific replication factors.', 'query': 'How does the system ensure fast query speeds for individual tenants and what upcoming features will allow further customization of tenant performance?'}\n",
      "\u001b[96mQuery: How does the system ensure fast query speeds for individual tenants and what upcoming features will allow further customization of tenant performance?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The system ensures fast query speeds for individual tenants by providing each tenant with a dedicated high-performance vector index. This setup gives query speeds as if the tenant was the only user on the cluster, thereby optimizing performance (Search Result 4).\n",
      "\n",
      "Upcoming features that will allow further customization of tenant performance include tenant-specific rate limiting and tenant-specific replication factors, which will enable more granular control over each tenant's resource allocation and performance optimization (Search Result 4). Additionally, the feature of inactive tenant support will remove the limitation on the number of tenants per node, potentially further optimizing resource usage (Search Result 1).\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Weaviate ensures fast query speeds for individual tenants through its robust multi-tenancy architecture. Each tenant's data is isolated into separate shards, allowing for high guarantees of data isolation and eliminating cross-querying of results between tenants. The system is capable of supporting over 50,000 active tenants per node, and with a 20-node cluster, it can handle up to 1 million active tenants. This efficient setup allows you to simply add a tenant key without needing to set filters, and Weaviate automatically finds the correct shard for querying the specific tenant, ensuring both speed and efficiency.\n",
      "\n",
      "As for upcoming features that will allow further customization of tenant performance, Weaviate is planning to introduce rebalancing features that will allow tenants to be distributed across nodes as desired. This can be manually controlled or automated by Weaviate. Additionally, support for distinguishing between active and inactive tenants is also in the pipeline. This feature will enable users to optimize resource usage, ensuring that expensive compute and memory resources are reserved for active users, thereby further enhancing cost-efficiency and performance for individual tenant needs.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 4 wins\n",
      "Agentic RAG: 10 wins\n",
      "After 14 rounds\n",
      "\n",
      "{'source': 'The below `curl` command will back up all classes in **W1**, and call the backup `my-very-first-backup`. ```bash\\ncurl \\\\\\n-X POST \\\\\\n-H \"Content-Type: application/json\" \\\\\\n-d \\'{\\n     \"id\": \"my-very-first-backup\"\\n    }\\' \\\\\\nhttp://localhost:8080/v1/backups/filesystem\\n```\\n\\n:::note The `backup_id` must be unique. The ID value is used to create a subdirectory in the backup location, and attempting to reuse an existing ID will cause Weaviate to throw an error. Delete the existing directory if one already exists. :::\\n\\nNow try running `3_backup` yourself to back up data from **W1**.', 'gold_answer': 'To create a unique backup called `my-very-first-backup` for classes in W1 using a curl command, you would use the following command:\\n\\n```bash\\ncurl \\\\\\n-X POST \\\\\\n-H \"Content-Type: application/json\" \\\\\\n-d \\'{\\n     \"id\": \"my-very-first-backup\"\\n    }\\' \\\\\\nhttp://localhost:8080/v1/backups/filesystem\\n```\\n\\nThis command sends a POST request to the Weaviate server to create a backup of all classes in W1. The backup is identified by the unique `id` `my-very-first-backup`.', 'query': 'How do you create a unique backup called `my-very-first-backup` for classes in W1 using a curl command?'}\n",
      "\u001b[96mQuery: How do you create a unique backup called `my-very-first-backup` for classes in W1 using a curl command?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "To create a unique backup called `my-very-first-backup` for classes in W1 using a `curl` command, you can use the following command:\n",
      "\n",
      "```bash\n",
      "curl \\\n",
      "-X POST \\\n",
      "-H \"Content-Type: application/json\" \\\n",
      "-d '{\n",
      "     \"id\": \"my-very-first-backup\"\n",
      "    }' \\\n",
      "http://localhost:8080/v1/backups/filesystem\n",
      "```\n",
      "\n",
      "This command sends a POST request with a JSON payload specifying the unique `id` for the backup to the Weaviate backup service running at `http://localhost:8080`. Make sure that the `backup_id` is unique, as reusing an existing ID will cause an error.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "To create a unique backup named `my-very-first-backup` for the classes in W1 using a curl command, you can use the following command:\n",
      "\n",
      "```bash\n",
      "curl \\\n",
      "-X POST \\\n",
      "-H \"Content-Type: application/json\" \\\n",
      "-d '{\n",
      "     \"id\": \"my-very-first-backup\"\n",
      "    }' \\\n",
      "http://localhost:8080/v1/backups/filesystem\n",
      "```\n",
      "\n",
      "Make sure to adjust the URL `http://localhost:8080` if your Weaviate instance is running on a different host or port. The provided `backup_id` must be unique, as it will be used to create a subdirectory in the backup location.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 4 wins\n",
      "Agentic RAG: 11 wins\n",
      "After 15 rounds\n",
      "\n",
      "{'source': 'Then, you just run the queries, as per usual:\\n```javascript\\nnearText = {\\n    \"concepts\": [\"How to use Hugging Face modules with Weaviate?\"],\\n    \"distance\": 0.6,\\n}\\n\\nresult = (\\n    client.query\\n    .get(\"Notes\", [\\n        \"name\",\\n        \"comment\",\\n        \"_additional {certainty distance} \"])\\n    .with_near_text(nearText)\\n    .do()\\n)\\n```\\n\\n## Summary\\n> Now you can use [Hugging Face](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) or [OpenAI](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) modules in Weaviate to delegate model inference out. Just pick the model, provide your API key and start working with your data. Weaviate optimizes the communication process with the Inference API for you, so that you can focus on the challenges and requirements of your applications. No need to run the Inference API yourself. ## What next\\nCheck out the [text2vec-huggingface](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface) documentation to learn more about the new module.', 'gold_answer': 'To use Hugging Face modules with Weaviate without running the Inference API yourself, you need to have Weaviate version 1.15 or newer. In the configuration, set `DEFAULT_VECTORIZER_MODULE` and `ENABLE_MODULES` to `text2vec-huggingface`. This will enable the Hugging Face module, allowing you to use the Hugging Face Inference service with sentence similarity models to vectorize and query your data directly from Weaviate.', 'query': 'How can I use Hugging Face modules with Weaviate without running the Inference API myself?'}\n",
      "\u001b[96mQuery: How can I use Hugging Face modules with Weaviate without running the Inference API myself?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "To use Hugging Face modules with Weaviate without running the Inference API yourself, you can leverage the built-in support provided by Weaviate's Hugging Face module. Here is what you need to do:\n",
      "\n",
      "1. **Weaviate Version**: Ensure you are using Weaviate version `v1.15` or newer, as this includes support for the Hugging Face module.\n",
      "\n",
      "2. **API Token**: Obtain a Hugging Face API Token by creating an account or logging into Hugging Face and requesting a token from their settings page.\n",
      "\n",
      "3. **Configure Weaviate**:\n",
      "   - Set `text2vec-huggingface` as your default vectorizer module in the Weaviate configuration:\n",
      "     ```yaml\n",
      "     DEFAULT_VECTORIZER_MODULE: text2vec-huggingface\n",
      "     ENABLE_MODULES: text2vec-huggingface\n",
      "     ```\n",
      "\n",
      "4. **Initialize Weaviate Client**: While configuring your Weaviate client (e.g., in Python), add your Hugging Face API token in the headers. Example configuration:\n",
      "   ```python\n",
      "   client = weaviate.Client(\n",
      "       url='http://localhost:8080',\n",
      "       additional_headers={\n",
      "           'X-HuggingFace-Api-Key': 'YOUR-HUGGINGFACE-API-KEY'\n",
      "       }\n",
      "   )\n",
      "   ```\n",
      "\n",
      "5. **Use the Module**: With the configuration set, you can now perform data imports, updates, and queries in Weaviate. You simply need to run the queries as per usual, and Weaviate will handle communicating with the Hugging Face Inference API for you, using the provided API token.\n",
      "\n",
      "By following these steps, you can utilize Hugging Face's model inference capabilities directly within Weaviate, without needing to manage the inference infrastructure yourself.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "To use Hugging Face modules with Weaviate without running the Inference API yourself, you can utilize the Hugging Face module integrated into Weaviate from version 1.15 onwards. Here’s a summary of how you can do it:\n",
      "\n",
      "1. **Using the Hugging Face Module**: Weaviate includes a Hugging Face module, which allows you to use the Hugging Face Inference service directly from the vector database. This means you don't need to run the Inference API yourself. You can use the `text2vec-huggingface` module to delegate your model inference tasks to Hugging Face.\n",
      "\n",
      "2. **API Token**: You will need a Hugging Face API Token. You can request one from Hugging Face and use it to authenticate your requests.\n",
      "\n",
      "3. **Configuration**: To use the Hugging Face module in Weaviate open source (v1.15 or newer), configure it as the default vectorizer:\n",
      "   ```yaml\n",
      "   DEFAULT_VECTORIZER_MODULE: text2vec-huggingface\n",
      "   ENABLE_MODULES: text2vec-huggingface\n",
      "   ```\n",
      "\n",
      "4. **Client Setup**: When setting up your Weaviate client, you'll need to include your Hugging Face API key. For example, in Python:\n",
      "   ```python\n",
      "   client = weaviate.Client(\n",
      "       url='http://localhost:8080',\n",
      "       additional_headers={\n",
      "           'X-HuggingFace-Api-Key': 'YOUR-HUGGINGFACE-API-KEY'\n",
      "       }\n",
      "   )\n",
      "   ```\n",
      "\n",
      "5. **Run Queries**: You can run queries using the selected models through Weaviate that automatically optimizes communications with the Inference API.\n",
      "\n",
      "By following these steps, you can easily integrate Hugging Face models with your Weaviate setup, using their hosted inference services without the need to host anything yourself.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"vanilla rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 5 wins\n",
      "Agentic RAG: 11 wins\n",
      "After 16 rounds\n",
      "\n",
      "{'source': 'And additionally, Weaviate often interacts with external services such as vectorizers or LLMs.\\n\\nSuch complexity makes it important to test the application as a whole, and not just its individual components. This complexity also means that arranging the test suite can be cumbersome with a variety of moving parts that need to be set up and torn down. [Embedded Weaviate](/developers/weaviate/installation/embedded) makes one part of this puzzle much easier, since Weaviate can be instantiated directly from the client. The following is all you need to do to start a Weaviate server:\\n\\n<Tabs groupId=\"languages\">\\n  <TabItem value=\"py\" label=\"Python\">\\n  <FilteredTextBlock\\n    text={PyCode}\\n    startMarker=\"# START Connect\"\\n    endMarker=\"# Client is now ready to accept requests\"\\n    language=\"py\"\\n  />\\n  </TabItem>\\n\\n  <TabItem value=\"js\" label=\"JavaScript/TypeScript\">\\n  <FilteredTextBlock\\n    text={TSCode}\\n    startMarker=\"// START Connect\"\\n    endMarker=\"// Client is now ready to accept requests\"\\n    language=\"js\"\\n  />\\n  </TabItem>\\n</Tabs>\\n\\nThis is not only useful for new contributors to the project, but also for experienced developers. Starting anew as a new contributor, or working from a different machine on occasion, can be a hassle.', 'gold_answer': 'Embedded Weaviate simplifies the setup process for testing the Weaviate application by eliminating the need to set up and tear down a separate service such as a database. This makes the testing process less time-consuming and less prone to errors. Additionally, it allows for a more robust testing process that can verify various operations such as data import, vectorization, and export, as well as search, insertion, updates, deletes, and other operations that are part of the user journey.', 'query': 'How does Embedded Weaviate simplify the setup process for testing the Weaviate application?'}\n",
      "\u001b[96mQuery: How does Embedded Weaviate simplify the setup process for testing the Weaviate application?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Embedded Weaviate simplifies the setup process for testing the Weaviate application by allowing developers to quickly start using Weaviate on their local machines without the need to explicitly download, install, or set up a separate server. This ease of use makes it ideal for testing or prototyping with small datasets. Additionally, it provides portability and the ability to persist data for future invocations or transfer it to another instance. This makes the testing process more straightforward and manageable, even in environments such as Jupyter notebooks, enhancing the overall testing experience.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Embedded Weaviate simplifies the setup process for testing the Weaviate application in several ways:\n",
      "\n",
      "1. **Eliminates the Need for a Separate Server Instance**: Embedded Weaviate allows you to run tests without spinning up a separate Weaviate server instance. This reduction in setup complexity makes it easier to integrate testing directly into development environments, such as local machines or continuous integration pipelines.\n",
      "\n",
      "2. **Fast Local Setup**: It enables you to quickly get started with Weaviate on your local machine without needing to explicitly download, install, or instantiate a server.\n",
      "\n",
      "3. **Integration with Testing Frameworks**: As demonstrated in testing workflows for languages like Python and JavaScript/TypeScript, Embedded Weaviate can be incorporated into automated testing processes. This lowers the barrier to running tests efficiently across different environments, including local setups and CI/CD pipelines.\n",
      "\n",
      "4. **Data Persistence**: Data used during testing is persisted, enabling you to reuse it across different testing sessions or transfer it to another instance, which is particularly beneficial for maintaining test consistency.\n",
      "\n",
      "5. **Utility in Prototyping and Small Datasets**: Embedded Weaviate is ideal for scenarios involving prototyping with small datasets, making it a flexible tool for development and testing purposes, especially when hardware resources are limited. \n",
      "\n",
      "These features collectively contribute to a streamlined and efficient testing setup, making the development and iteration process less cumbersome and more integrated.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 5 wins\n",
      "Agentic RAG: 12 wins\n",
      "After 17 rounds\n",
      "\n",
      "{'source': \"You can reach us through:\\n\\n- Join our dedicated Hacktoberfest channel in our [Discourse community forum](https://forum.weaviate.io/), where we're ready to answer your questions. - Join our dedicated #hacktoberfest channel in our [Weaviate Community Slack](https://weaviate.slack.com/) channel to stay connected and receive real-time support. - Join our [newsletter](https://newsletter.weaviate.io/) and follow us on [Twitter](https://twitter.com/weaviate_io) and [Linkedin](https://www.linkedin.com/company/weaviate-io/mycompany/verification/) to receive updates. - Stay up to date with Weaviate's development by exploring the [Weaviate GitHub Repository](https://github.com/weaviate/weaviate). Don’t forget to give us a ⭐️ while you are there!\\n\\n:::info Pro Tip\\nShare your process online and tag us on [Twitter](https://twitter.com/weaviate_io) and [LinkedIn](https://nl.linkedin.com/company/weaviate-io).\", 'gold_answer': \"To participate in the Weaviate community's Hacktoberfest event, you can join their dedicated Hacktoberfest channel in their Discourse community forum or their #hacktoberfest channel in their Weaviate Community Slack channel. To stay updated on their developments, you can join their newsletter and follow them on Twitter and Linkedin. You can also explore the Weaviate GitHub Repository. They also encourage you to share your process online and tag them on Twitter and LinkedIn.\", 'query': \"How can I participate in the Weaviate community's Hacktoberfest event and stay updated on their developments?\"}\n",
      "\u001b[96mQuery: How can I participate in the Weaviate community's Hacktoberfest event and stay updated on their developments?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "To participate in the Weaviate community's Hacktoberfest event and stay updated on their developments, you can take the following steps:\n",
      "\n",
      "1. **Participate in Hacktoberfest:**\n",
      "   - Contribute to repositories that are part of Weaviate's Hacktoberfest initiative. If you have four pull requests accepted between October 1 and 31, 2023, you can earn unique rewards and possibly some Weaviate swag.\n",
      "   - Explore example use cases and demos provided by Weaviate on their [developers page](https://weaviate.io/developers/weaviate/more-resources/example-use-cases), which are part of a demo sprint aimed at onboarding open-source contributors.\n",
      "\n",
      "2. **Connect with the Community:**\n",
      "   - Join their[Discourse community forum](https://forum.weaviate.io/) and participate in the dedicated Hacktoberfest channel for discussions and questions.\n",
      "   - Engage with the community through the [Weaviate Community Slack](https://weaviate.slack.com/) channel, especially in the #hacktoberfest section for real-time support.\n",
      "   \n",
      "3. **Stay Informed and Updated:**\n",
      "   - Subscribe to the [Weaviate newsletter](https://newsletter.weaviate.io/) and follow them on social media (e.g., [Twitter](https://twitter.com/weaviate_io) and [LinkedIn](https://www.linkedin.com/company/weaviate-io/)) for the latest updates.\n",
      "   - Explore the [Weaviate GitHub Repository](https://github.com/weaviate/weaviate) to stay informed about ongoing developments, and consider giving them a ⭐️ to show support.\n",
      "\n",
      "4. **Attend Events and Utilize Resources:**\n",
      "   - Attend virtual events and workshops conducted by the Weaviate team. For example, sessions like the \"Introduction to Weaviate\" and previews of new APIs are useful for expanding your knowledge.\n",
      "   - Utilize supplementary resources like the [Weaviate YouTube Channel](https://www.youtube.com/@Weaviate) or the [Weaviate Blog](https://weaviate.io/blog) for additional insights and tutorials.\n",
      "\n",
      "By engaging with the community through these channels and contributing to the Hacktoberfest event, you can make the most of your participation in Weaviate's open-source ecosystem.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "To participate in the Weaviate community's Hacktoberfest event and stay updated on their developments, here are some steps and resources you can use:\n",
      "\n",
      "### Participating in Hacktoberfest with Weaviate\n",
      "\n",
      "1. **Hacktoberfest Contribution**: \n",
      "   - Join Weaviate in celebrating Hacktoberfest by contributing to their open-source projects. Participants who have four pull requests (PRs) accepted between October 1 and 31, 2023, will receive a unique digital reward from Hacktoberfest. You might also receive some Weaviate merchandise.\n",
      "   - Contributions can range from minor, non-coding inputs to more substantial technical improvements.\n",
      "\n",
      "2. **Community Engagement**:\n",
      "   - Join the dedicated Hacktoberfest channels in the [Weaviate Discourse community forum](https://forum.weaviate.io/) and [Community Slack](https://weaviate.slack.com/) where you can connect, collaborate, and receive real-time support.\n",
      "   - Share your progress and tag Weaviate on [Twitter](https://twitter.com/weaviate_io) and [LinkedIn](https://nl.linkedin.com/company/weaviate-io).\n",
      "\n",
      "### Staying Updated with Weaviate Developments\n",
      "\n",
      "1. **Events and Webinars**:\n",
      "   - Attend events such as the **Introduction to Weaviate** and **Python `Collections` Client API Preview** sessions to expand your knowledge.\n",
      "   - Follow Weaviate events and meetups to stay connected, such as the **Weaviate World Tour**.\n",
      "\n",
      "2. **Community Resources**:\n",
      "   - Subscribe to the [Weaviate newsletter](https://newsletter.weaviate.io/) and follow Weaviate on social media platforms like [Twitter](https://twitter.com/weaviate_io) and [LinkedIn](https://www.linkedin.com/company/weaviate-io) for regular updates.\n",
      "   - Explore the [Weaviate GitHub Repository](https://github.com/weaviate/weaviate) to stay up to date with the latest features and updates.\n",
      "   - Engage with content on the [Weaviate YouTube Channel](https://www.youtube.com/@Weaviate) and [Weaviate Blog](https://weaviate.io/blog).\n",
      "\n",
      "By engaging in these activities, you can contribute to the Weaviate community, celebrate with them during Hacktoberfest, and keep abreast of their latest developments and innovations.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"vanilla rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 6 wins\n",
      "Agentic RAG: 12 wins\n",
      "After 18 rounds\n",
      "\n",
      "{'source': 'You would need the following ingredients:\\n* Raw Data\\n* Hugging Face API token – which you can request from [their website](https://huggingface.co/settings/tokens)\\n* A working Weaviate instance with the `text2vec-huggingface` enabled\\n\\nThen you would follow these steps. ### Step 1 – initial preparation – create schema and select the hf models\\nOnce you have a Weaviate instance up and running. Define your schema (standard stuff – pick a class name, select properties, and data types). As a part of the schema definition, you also need to provide, which Hugging Face model you want to use for each schema class. This is done by adding a `moduleConfig` property with the `model` name, to the schema definition, like this:\\n```javascript\\n{\\n    \"class\": \"Notes\",\\n    \"moduleConfig\": {\\n        \"text2vec-huggingface\": {\\n            \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",  # model name\\n            ...', 'gold_answer': 'To set up a Weaviate instance with the `text2vec-huggingface` module, follow these steps:\\n\\n1. Set `text2vec-huggingface` as the default vectorizer in the Weaviate configuration by setting the `DEFAULT_VECTORIZER_MODULE` and `ENABLE_MODULES` environment variables to `text2vec-huggingface`.\\n\\n2. Ensure you have the necessary \"ingredients\", which include raw data, a Hugging Face API token, and a working Weaviate instance with the `text2vec-huggingface` module enabled.\\n\\n3. Define your schema by picking a class name, selecting properties, and data types. As part of the schema definition, specify which Hugging Face model you want to use for each schema class by adding a `moduleConfig` property with the `model` name to the schema definition.\\n\\n4. Run queries as usual.', 'query': 'How do you set up a Weaviate instance with the `text2vec-huggingface` module?'}\n",
      "\u001b[96mQuery: How do you set up a Weaviate instance with the `text2vec-huggingface` module?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "To set up a Weaviate instance with the `text2vec-huggingface` module, you can follow these steps:\n",
      "\n",
      "1. **Configure Weaviate:**\n",
      "   - Ensure your Weaviate version is `v1.15` or newer to use the Hugging Face module.\n",
      "   - In your Weaviate configuration, set `text2vec-huggingface` as the default vectorizer module. You can do this by specifying the following in your configuration file:\n",
      "     ```yaml\n",
      "     DEFAULT_VECTORIZER_MODULE: text2vec-huggingface\n",
      "     ENABLE_MODULES: text2vec-huggingface\n",
      "     ```\n",
      "\n",
      "2. **Set up the Weaviate Client:**\n",
      "   - You need a Hugging Face API token for authorization when importing data. Here's how you can configure your Weaviate client in Python:\n",
      "     ```python\n",
      "     client = weaviate.Client(\n",
      "         url='http://localhost:8080',\n",
      "         additional_headers={\n",
      "             'X-HuggingFace-Api-Key': 'YOUR-HUGGINGFACE-API-KEY'\n",
      "         }\n",
      "     )\n",
      "     ```\n",
      "\n",
      "3. **Import Data:**\n",
      "   - Once your client is set up, you can import data into Weaviate using the configured vectorizer. The process for data import would be similar to how you usually import data into Weaviate.\n",
      "\n",
      "These steps will allow you to use the `text2vec-huggingface` module with your Weaviate instance, enabling text vectorization using Hugging Face models.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "To set up a Weaviate instance with the `text2vec-huggingface` module, follow these steps:\n",
      "\n",
      "1. **Configuration**: You need to configure Weaviate to use the `text2vec-huggingface` module as the default vectorizer. Your configuration might look like this in the YAML file:\n",
      "\n",
      "   ```yaml\n",
      "   DEFAULT_VECTORIZER_MODULE: text2vec-huggingface\n",
      "   ENABLE_MODULES: text2vec-huggingface\n",
      "   ```\n",
      "\n",
      "2. **Hugging Face API Token**: You will need a Hugging Face API token to authorize calls for vectorization. Add this token to your Weaviate client configuration. Here's an example in Python:\n",
      "\n",
      "   ```python\n",
      "   client = weaviate.Client(\n",
      "       url='http://localhost:8080',\n",
      "       additional_headers={\n",
      "           'X-HuggingFace-Api-Key': 'YOUR-HUGGINGFACE-API-KEY'\n",
      "       }\n",
      "   )\n",
      "   ```\n",
      "\n",
      "   Replace `'YOUR-HUGGINGFACE-API-KEY'` with your actual Hugging Face API key.\n",
      "\n",
      "3. **Import Data**: Start importing data into Weaviate. The client configuration allows you to import data the same way as always, with Weaviate taking care of the vectorization using the Hugging Face models.\n",
      "\n",
      "4. **Additional Options**: Weaviate also supports custom retrievers and integration with various Hugging Face models. You can customize the use of models by changing paths in the Weaviate configuration.\n",
      "\n",
      "These steps provide a basic outline for integrating the `text2vec-huggingface` module with a Weaviate instance. For more detailed tutorials, you might want to refer to the official Weaviate documentation or related community resources.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 6 wins\n",
      "Agentic RAG: 13 wins\n",
      "After 19 rounds\n",
      "\n",
      "{'source': '`go_memstats_heap_inuse_bytes` should always be present even with an empty schema. ![datadog summary](./img/datadog-summary.png)\\n\\n\\n## Key Metrics\\n\\nBelow are some key Weaviate metrics to monitor. Standard CPU, Disk, Network metrics are also useful as are [Kubernetes\\nevents](https://grafana.com/blog/2023/01/23/how-to-use-kubernetes-events-for-effective-alerting-and-monitoring/). Note that some Weaviate metrics will not appear until an operation has occurred (for instance batch operations). ### Heap Usage\\n\\nFor heap usage, the expectation is the memory will have a standard jagged pattern underload but that memory will drop periodically\\ndue to the Go garbage collection.', 'gold_answer': 'Yes, the `go_memstats_heap_inuse_bytes` metric should be present in Weaviate even if the schema is empty. Its expected behavior under load is to have a standard jagged pattern, growing as more data is imported but dropping periodically due to the Go garbage collection. If the memory usage is consistently close to the GOMEMLIMIT, it may indicate that more resources are needed.', 'query': 'Should the `go_memstats_heap_inuse_bytes` metric be present in Weaviate even if the schema is empty, and what is its expected behavior under load?'}\n",
      "\u001b[96mQuery: Should the `go_memstats_heap_inuse_bytes` metric be present in Weaviate even if the schema is empty, and what is its expected behavior under load?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Yes, the `go_memstats_heap_inuse_bytes` metric should be present in Weaviate even if the schema is empty. This metric is consistent in its presence because it provides insights into memory usage that are crucial for monitoring the health and performance of the application.\n",
      "\n",
      "Under load, the expected behavior of the `go_memstats_heap_inuse_bytes` metric is to exhibit a jagged pattern. This pattern indicates normal memory allocation and deallocation due to operations and the Go garbage collector. The memory usage should show periodic drops as the garbage collector frees memory. If memory does not drop and remains close to the GOMEMLIMIT, it suggests that the application might need more resources.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The `go_memstats_heap_inuse_bytes` metric in Weaviate should be present even if the schema is empty. It is a fundamental metric used to monitor heap memory usage, providing insights into how memory is being utilized by the application. This metric is key for understanding the memory pattern under different load conditions.\n",
      "\n",
      "Under load, the expected behavior for the `go_memstats_heap_inuse_bytes` metric is a standard jagged pattern, reflecting increases in memory use as requests or operations are processed. The memory usage should periodically drop due to garbage collection by the Go runtime. If memory usage does not decrease and remains close to the `GOMEMLIMIT`, it may be necessary to increase the system's resources to prevent out-of-memory (OOM) crashes. Monitoring tools, such as Prometheus, are typically used alongside Weaviate to visualize and keep track of these metrics effectively.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 6 wins\n",
      "Agentic RAG: 14 wins\n",
      "After 20 rounds\n",
      "\n",
      "{'source': 'One of the primary reasons is the computational cost associated with processing and storing longer sequences of tokens. The longer the sequence, the more memory and processing power required to operate on it, which can be a significant challenge for even the most powerful computing systems. The relatively long input window of LLMs is what drives the integration with semantic search. For example, we can use this entire blog post as input in addition to questions if we want the LLM to answer questions such as “What are LLM Chains?” However, when we want to give the LLM an entire book or pages of search results, we need more clever techniques to decompose this task. This is where the `CombineDocuments` chain comes to play! Note, that one method is not better than another, and the performance is dependent solely on your application.', 'gold_answer': 'Techniques like the `CombineDocuments` chain are necessary for processing large texts with LLMs because they help overcome the limitations of LLMs in handling long sequences of tokens. They do this by breaking down larger texts into smaller, manageable chunks that can be processed by the LLM without overwhelming its computational and memory capacities.', 'query': 'Why are techniques like the `CombineDocuments` chain necessary for processing large texts with LLMs?'}\n",
      "\u001b[96mQuery: Why are techniques like the `CombineDocuments` chain necessary for processing large texts with LLMs?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Techniques like the `CombineDocuments` chain are necessary for processing large texts with Large Language Models (LLMs) primarily due to the computational cost and limitations related to processing and storing long sequences of tokens. LLMs have a limited token length they can process at one time, which is a significant challenge when dealing with very large texts like entire books or multiple pages of search results. The `CombineDocuments` chain helps decompose these large tasks into smaller, manageable parts that fit within the LLM's token limit. Hence, these techniques are crucial for efficiently utilizing LLMs without exceeding their technical constraints.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Techniques like the `CombineDocuments` chain are essential for processing large texts with large language models (LLMs) due to several reasons:\n",
      "\n",
      "1. **Token Limitations**: LLMs, like GPT-3 and others, have a maximum token limit they can handle in a single request. For instance, GPT-3's token limit is typically around 4096 tokens. This means that texts exceeding this limit cannot be processed in one go and need to be split into manageable chunks.\n",
      "\n",
      "2. **Memory and Performance Constraints**: Processing very large texts at once may not be efficient or even feasible due to memory and computational constraints. Splitting the text into smaller portions allows for more efficient processing.\n",
      "\n",
      "3. **Parallel Processing**: By dividing a large text into chunks, different parts of the document can be processed in parallel, which can enhance the speed and efficiency of data handling.\n",
      "\n",
      "4. **Contextual Understanding**: Techniques like `CombineDocuments` allow the model to maintain context across different segments, ensuring that the analysis or generation of text remains coherent and connected.\n",
      "\n",
      "5. **Error Handling and Control**: Breaking down documents into smaller segments allows for better error handling and control, enabling the system to manage errors on smaller sections rather than the entire document.\n",
      "\n",
      "6. **Flexibility in Combination**: After processing individual chunks, these techniques facilitate the recombination of processed data into a coherent output, allowing for various transformations or summarizations of the original text.\n",
      "\n",
      "Overall, `CombineDocuments` chains and similar techniques help optimize the interaction between LLMs and large datasets, making it possible to harness the full potential of the models without overwhelming them or breaching their operational limits.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 6 wins\n",
      "Agentic RAG: 15 wins\n",
      "After 21 rounds\n",
      "\n",
      "{'source': 'To see a list of the newly spun up nodes, run:\\n\\n```shell\\nkubectl get nodes -o wide\\n```\\n\\nYou should see an output similar to the following, indicating that three nodes are up and onto which you can deploy Weaviate:\\n\\n```shell\\nNAME           STATUS   ROLES           AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\\nminikube       Ready    control-plane   134m   v1.27.3   192.168.49.2   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\\nminikube-m02   Ready    <none>          134m   v1.27.3   192.168.49.3   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\\nminikube-m03   Ready    <none>          133m   v1.27.3   192.168.49.4   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\\n```\\n\\nNow, add the Weaviate helm repository to your local helm configuration by running:\\n\\n```shell\\nhelm repo add weaviate https://weaviate.github.io/weaviate-helm\\n```\\n\\nAnd save the default configuration with:\\n\\n```shell\\nhelm show values weaviate/weaviate > values.yaml\\n```\\n\\nEdit `values.yaml` by changing the root-level configuration `replicas: 1` for the root image to `replicas: 3`, and save it. ```yaml\\n... # Scale replicas of Weaviate. Note that as of v1.8.0 dynamic scaling is limited\\n# to cases where no data is imported yet. Scaling down after importing data may\\n# break usability.', 'gold_answer': 'To check the status and details of nodes in a Kubernetes cluster, run the command `kubectl get nodes -o wide`. To prepare to deploy Weaviate with three replicas, add the Weaviate helm repository to your local helm configuration by running `helm repo add weaviate https://weaviate.github.io/weaviate-helm`, save the default configuration with `helm show values weaviate/weaviate > values.yaml`, and edit `values.yaml` by changing the root-level configuration `replicas: 1` for the root image to `replicas: 3`. Then, create a namespace for Weaviate and deploy it using the `helm upgrade --install` command.', 'query': 'How can I check the status and details of nodes in a Kubernetes cluster and prepare to deploy Weaviate with three replicas?'}\n",
      "\u001b[96mQuery: How can I check the status and details of nodes in a Kubernetes cluster and prepare to deploy Weaviate with three replicas?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "To check the status and details of nodes in a Kubernetes cluster and prepare to deploy Weaviate with three replicas, follow these steps:\n",
      "\n",
      "1. **Ensure you have the necessary tools:**\n",
      "   - Install `kubectl` to interact with your Kubernetes cluster ([installation guide](https://kubernetes.io/docs/tasks/tools/#kubectl)).\n",
      "   - If running a local test, install `minikube` ([installation guide](https://minikube.sigs.k8s.io/docs/start/)) to create a local Kubernetes cluster.\n",
      "   - Install `helm`, which is used for deploying applications on Kubernetes ([installation guide](https://helm.sh/docs/intro/install)).\n",
      "\n",
      "2. **Set up a Kubernetes cluster with three nodes:**\n",
      "   - If you're using `minikube`, you can create a three-node cluster by executing:\n",
      "     ```shell\n",
      "     minikube start --nodes 3\n",
      "     ```\n",
      "\n",
      "3. **Check the status of the nodes:**\n",
      "   - Use `kubectl` to check the status of the nodes:\n",
      "     ```shell\n",
      "     kubectl get nodes\n",
      "     ```\n",
      "   - This command will list the nodes and their statuses.\n",
      "\n",
      "4. **Deploy Weaviate with Helm:**\n",
      "   - First, create a dedicated namespace for Weaviate:\n",
      "     ```shell\n",
      "     kubectl create namespace weaviate\n",
      "     ```\n",
      "   - Deploy Weaviate with three replicas using Helm:\n",
      "     ```shell\n",
      "     helm upgrade --install \\\n",
      "       \"weaviate\" \\\n",
      "       weaviate/weaviate \\\n",
      "       --namespace \"weaviate\" \\\n",
      "       --values ./values.yaml\n",
      "     ```\n",
      "   - Ensure that your `values.yaml` file is appropriately configured for three replicas.\n",
      "\n",
      "5. **Verify the deployment:**\n",
      "   - Check the status of the Weaviate pods:\n",
      "     ```shell\n",
      "     kubectl get pods -n weaviate\n",
      "     ```\n",
      "   - You should see three Weaviate pods in the \"Running\" state.\n",
      "\n",
      "6. **Expose the Weaviate service:**\n",
      "   - Use `minikube` to expose the Weaviate service, allowing access from your local machine:\n",
      "     ```shell\n",
      "     minikube service weaviate --namespace weaviate\n",
      "     ```\n",
      "   - This command will display the URL you can use to access the Weaviate cluster.\n",
      "\n",
      "Following these steps will prepare your cluster for deploying Weaviate with a configuration that ensures high availability using three replicas.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "To check the status and details of nodes in a Kubernetes cluster and prepare to deploy Weaviate with three replicas, you'll need to follow these steps:\n",
      "\n",
      "### Check the Status and Details of Nodes\n",
      "You can check the status of your nodes using the `kubectl` command-line tool, which interacts with your Kubernetes cluster.\n",
      "\n",
      "1. **List Nodes:**\n",
      "   Run the following command to list all the nodes in your cluster:\n",
      "\n",
      "   ```bash\n",
      "   kubectl get nodes\n",
      "   ```\n",
      "\n",
      "   This command will display the status of each node, including whether they're ready or not.\n",
      "\n",
      "2. **Describe Nodes:**\n",
      "   For more detailed information about a specific node, use:\n",
      "\n",
      "   ```bash\n",
      "   kubectl describe node <node-name>\n",
      "   ```\n",
      "\n",
      "   Replace `<node-name>` with the name of your node. This will provide you with detailed information about the node's conditions, capacity, and other configurations.\n",
      "\n",
      "### Prepare to Deploy Weaviate with Three Replicas\n",
      "To deploy Weaviate with three replicas, you'll prepare a Kubernetes deployment configuration. Here's a general outline for deploying Weaviate:\n",
      "\n",
      "1. **Create a Deployment YAML file:**\n",
      "   Create a file named `weaviate-deployment.yaml`, and include the following configuration:\n",
      "\n",
      "   ```yaml\n",
      "   apiVersion: apps/v1\n",
      "   kind: Deployment\n",
      "   metadata:\n",
      "     name: weaviate\n",
      "   spec:\n",
      "     replicas: 3\n",
      "     selector:\n",
      "       matchLabels:\n",
      "         app: weaviate\n",
      "     template:\n",
      "       metadata:\n",
      "         labels:\n",
      "           app: weaviate\n",
      "       spec:\n",
      "         containers:\n",
      "         - name: weaviate\n",
      "           image: semi-technologies/weaviate:latest\n",
      "           ports:\n",
      "           - containerPort: 8080\n",
      "   ```\n",
      "\n",
      "   This YAML file configures a deployment with three replicas of the Weaviate container, exposing port 8080.\n",
      "\n",
      "2. **Apply the Deployment:**\n",
      "   Use the `kubectl` command to apply the deployment:\n",
      "\n",
      "   ```bash\n",
      "   kubectl apply -f weaviate-deployment.yaml\n",
      "   ```\n",
      "\n",
      "3. **Verify the Deployment:**\n",
      "   Check the status of the deployment to ensure that the pods are running:\n",
      "\n",
      "   ```bash\n",
      "   kubectl get pods\n",
      "   ```\n",
      "\n",
      "### Additional Considerations\n",
      "- **Resource Management:** Make sure your nodes have enough resources (CPU, memory) to accommodate the deployment.\n",
      "- **Networking:** Ensure network policies allow the necessary traffic and that Weaviate can communicate with any other necessary components.\n",
      "- **Persistence:** If Weaviate requires persistent storage, configure PersistentVolumeClaims (PVCs) accordingly.\n",
      "\n",
      "By following these steps, you should be able to check node statuses in your Kubernetes cluster and deploy Weaviate with the specified number of replicas.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"vanilla rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 7 wins\n",
      "Agentic RAG: 15 wins\n",
      "After 22 rounds\n",
      "\n",
      "{'source': 'For more information about this new feature, read this [blog post](/blog/ref2vec-centroid) by Connor Shorten. Weaviate also has a bunch of example use cases on [GitHub](https://github.com/weaviate/weaviate-examples). Find your favorite example, give it a star, and try to recreate it yourself!\\n\\n## Feature Comparison - Library versus Database\\nThe table below summarizes the differences between vector libraries and databases. This is by no means an exhaustive list of features, and not every library or database has the same features. |**Feature**|**Vector Library**|**Vector Database** (Weaviate as an example)|\\n| ----- | ----- | ----- |\\n| Filtering (in combination with Vector Search)| No| Yes|\\n| Updatability (CRUD)| No (some do, e.g. hnswlib)| Yes|\\n| Incremental importing, concurrent reading while importing| No (some do, e.g. hnswlib) | Yes |\\n| Stores objects and vectors | No | Yes|\\n|Speed | Typically faster than full-blown database|  Typically slower than pure library|\\n| Performance optimized for | In-memory similarity search| End2end-callstack, including: vector search, object retrieval from persistent storage, optional inverted index filtering, network requests, etc.', 'gold_answer': 'The differences between vector libraries and vector databases like Weaviate include filtering (supported by Weaviate but not typically by vector libraries), updatability (supported by Weaviate but not typically by vector libraries), incremental importing and concurrent reading while importing (supported by Weaviate but not typically by vector libraries), storage of objects and vectors (supported by Weaviate but not by vector libraries), and speed and performance (vector libraries are typically faster and optimized for in-memory similarity search, while Weaviate is slower and optimized for end-to-end call stack).', 'query': 'What are the differences between vector libraries and vector databases like Weaviate in terms of features such as filtering, updatability, and performance?'}\n",
      "\u001b[96mQuery: What are the differences between vector libraries and vector databases like Weaviate in terms of features such as filtering, updatability, and performance?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Vector libraries and vector databases like Weaviate differ in several key features, including filtering, updatability, and performance. Here's a comparison based on the provided context:\n",
      "\n",
      "1. **Filtering (in combination with Vector Search):**\n",
      "   - **Vector Libraries**: Typically do not support filtering alongside vector search.\n",
      "   - **Vector Databases (e.g., Weaviate)**: Support filtering, allowing for more refined searches by combining vector similarities with other criteria.\n",
      "\n",
      "2. **Updatability (CRUD operations):**\n",
      "   - **Vector Libraries**: Generally do not support CRUD operations. Some libraries, like hnswlib, support certain updates but are not fully capable of managing data dynamically.\n",
      "   - **Vector Databases**: Provide full CRUD support, allowing for creating, reading, updating, and deleting entries, making them more suitable for applications with dynamic data.\n",
      "\n",
      "3. **Incremental Importing and Concurrent Reading:**\n",
      "   - **Vector Libraries**: Typically do not support these features; some exceptions exist (e.g., hnswlib).\n",
      "   - **Vector Databases**: Support incremental importing and allow concurrent read operations, enhancing their usability in production environments.\n",
      "\n",
      "4. **Storage of Objects and Vectors:**\n",
      "   - **Vector Libraries**: Focus on storing in-memory indexes of vector embeddings, primarily for similarity search.\n",
      "   - **Vector Databases**: Store both objects and their vector embeddings, offering a more comprehensive data management solution.\n",
      "\n",
      "5. **Performance:**\n",
      "   - **Vector Libraries**: Generally faster since they are optimized for in-memory similarity searches.\n",
      "   - **Vector Databases**: May be slower than libraries because they handle additional tasks such as object retrieval, network requests, and possibly complex query filtering. They are optimized for end-to-end processing, which includes these additional aspects.\n",
      "\n",
      "6. **Suitability for Use Cases:**\n",
      "   - **Vector Libraries**: Suitable for static data applications where the data does not frequently change.\n",
      "   - **Vector Databases**: Suitable for dynamic environments and production applications where data updates and complex queries are needed. They offer additional benefits like distributed storage and scalability.\n",
      "\n",
      "In summary, the choice between a vector library and a vector database depends on the specific requirements of your application, such as the need for updatability, support for filtering, and the ability to manage both structured and unstructured data at scale.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Here's a comparison of features between vector libraries and vector databases like Weaviate:\n",
      "\n",
      "1. **Filtering**:\n",
      "   - **Vector Libraries**: Typically, they do not support filtering in combination with vector search.\n",
      "   - **Vector Databases (e.g., Weaviate)**: They support filtering alongside vector searches.\n",
      "\n",
      "2. **Updatability (CRUD Operations)**:\n",
      "   - **Vector Libraries**: Generally, they do not offer full CRUD capabilities. Some libraries like hnswlib might have limited support.\n",
      "   - **Vector Databases (e.g., Weaviate)**: Provide full CRUD support, meaning data can be created, read, updated, and deleted.\n",
      "\n",
      "3. **Incremental Importing and Concurrent Reading/Importing**:\n",
      "   - **Vector Libraries**: Usually lack support for these features, although some like hnswlib provide limited capabilities.\n",
      "   - **Vector Databases (e.g., Weaviate)**: Offer robust support, allowing for incremental importing and concurrent reading while data is being imported.\n",
      "\n",
      "4. **Data Storage**:\n",
      "   - **Vector Libraries**: Primarily focus on storing vector embeddings in memory for similarity searches.\n",
      "   - **Vector Databases (e.g., Weaviate)**: Store both the objects and their vector embeddings, enabling rich data storage and retrieval capabilities.\n",
      "\n",
      "5. **Speed and Performance**:\n",
      "   - **Vector Libraries**: Generally faster for raw vector similarity searches due to being in-memory.\n",
      "   - **Vector Databases (e.g., Weaviate)**: May be slower than libraries but are optimized for end-to-end operations, including data retrieval, filtering, and handling network requests alongside vector search.\n",
      "\n",
      "Vector databases like Weaviate are designed for more complex and dynamic use cases, offering robust features suitable for large-scale, real-time applications, whereas vector libraries tend to serve simpler, more static scenarios.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"vanilla rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 8 wins\n",
      "Agentic RAG: 15 wins\n",
      "After 23 rounds\n",
      "\n",
      "{'source': '![Cross-Encoder](./img/cross-encoder.png)\\n\\n*Figure 3 - Representation of a Cross-Encoder model*\\n\\n\\nIf a Cross-Encoder model is trained on a representative training set, it [achieves higher accuracy than Bi-Encoders](https://arxiv.org/abs/1908.10084). However, since you need to use the Cross-Encoder model during a search for every single data item in combination with the query, this method is very inefficient. For a real-life semantic search application – with thousands or millions of objects – this would be impractical, as it would take \"forever\" to perform the search. ## Combining Bi-Encoders and Cross-Encoders\\n\\nWe can combine the two methods to benefit from the strong points of both models! I\\'d like to illustrate this idea with an example. Imagine you are a fisherman who is looking for salmon in a sea full of fish of numerous species.', 'gold_answer': 'Cross-Encoders are considered impractical for semantic search applications with large datasets because they are computationally expensive and time-consuming. For every single data item in the dataset, the Cross-Encoder model needs to be used in combination with the query, making this method very inefficient when dealing with large datasets. The time it would take to perform the search with such large datasets would be impractical.', 'query': 'Why are Cross-Encoders considered impractical for semantic search applications with large datasets?'}\n",
      "\u001b[96mQuery: Why are Cross-Encoders considered impractical for semantic search applications with large datasets?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Cross-Encoders are considered impractical for semantic search applications with large datasets because they require computing the similarity between the query and each individual data item in the dataset. This means that for every search query, the Cross-Encoder model must be applied pairwise with each data item, which is computationally expensive and inefficient when dealing with thousands or millions of data objects. This process would be too slow to be feasible in real-life applications where high speed is crucial. In contrast, Bi-Encoders, although less accurate, are much faster because they produce vector embeddings for each data item independently, allowing for quick comparisons.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Cross-Encoders, while highly accurate, are considered impractical for semantic search applications with large datasets due to their inefficiency in such contexts. The impracticality stems from the fact that Cross-Encoders do not produce vector embeddings for individual data points. Instead, they use a classification mechanism that requires evaluating every single data item in conjunction with the query during a search. This evaluation involves using the Cross-Encoder model to calculate the similarity between the query and each data object as a data pair input, which makes it extremely time-consuming.\n",
      "\n",
      "For large datasets with thousands or millions of objects, this approach is inefficient and can take an impractical amount of time to perform searches. Therefore, while Cross-Encoders can achieve high accuracy, their use in real-life scenarios with large datasets needs to be combined with faster methods like Bi-Encoders. A typical approach is to use Bi-Encoders first to quickly retrieve a list of candidate results and then apply Cross-Encoders to rerank these candidates for higher accuracy. This two-tier approach allows one to leverage the strengths of both models: the speed of Bi-Encoders and the precision of Cross-Encoders.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 8 wins\n",
      "Agentic RAG: 16 wins\n",
      "After 24 rounds\n",
      "\n",
      "{'source': 'In other words, GOMEMLIMIT is precisely the missing piece that we outlined before. If the live heap is low (e.g., 100MB), we can delay the next GC cycle until the heap has doubled (200MB). But if the heap has grown close to our limit (e.g., 4GB), the GC runs more often to prevent us from ever running OOM. ### Why a soft limit? What is the difference between a soft and a hard limit?', 'gold_answer': 'GOMEMLIMIT plays a crucial role in managing garbage collection cycles in the Go runtime. It serves as a soft limit that the memory usage aims to stay within. When the live heap is low, the garbage collector can delay the next cycle until the heap has doubled. However, when the heap is close to the GOMEMLIMIT, the garbage collector runs more frequently to prevent the program from running out of memory.', 'query': 'What is the role of GOMEMLIMIT in garbage collection cycles, and how does it differ when the live heap is low versus when it is near the limit?'}\n",
      "\u001b[96mQuery: What is the role of GOMEMLIMIT in garbage collection cycles, and how does it differ when the live heap is low versus when it is near the limit?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The role of GOMEMLIMIT in garbage collection cycles is to provide a mechanism to control how aggressive the garbage collector (GC) should be, depending on the current usage of memory relative to a specified memory limit. It helps to prevent out-of-memory (OOM) errors by adjusting the behavior of the GC based on available memory.\n",
      "\n",
      "When the live heap memory usage is low, GOMEMLIMIT allows the garbage collector to remain more relaxed, which means that GC cycles will be less frequent and performance might not be significantly impacted because there is plenty of memory available.\n",
      "\n",
      "In contrast, when the live heap memory usage is near the GOMEMLIMIT, the garbage collector becomes more aggressive. This means that GC cycles will occur more frequently to free up memory and prevent exceeding the specified limit, thus avoiding OOM kills and keeping the application within its memory constraints.\n",
      "\n",
      "Overall, GOMEMLIMIT helps to dynamically adjust the intensity of garbage collection based on the current memory situation, providing better control over memory usage in Go applications.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The role of `GOMEMLIMIT` in garbage collection cycles is a significant one, especially in high-memory applications. Introduced in Go 1.19, `GOMEMLIMIT` provides a soft memory cap, which informs the Go runtime about the expected memory usage of your application. This feature allows for more efficient memory management by making the Garbage Collector (GC) more relaxed when there is plenty of RAM available, and more aggressive when memory is sparse.\n",
      "\n",
      "### Differences in GC Behavior:\n",
      "- **When the Live Heap is Low**: If your application's live heap size is well below the `GOMEMLIMIT`, the GC operates more leniently. This means that it doesn't have to run as frequently or aggressively, allowing applications to perform better since CPU resources are not tied up with constant garbage collection processes.\n",
      "  \n",
      "- **When the Live Heap is Near the Limit**: As the memory usage approaches the `GOMEMLIMIT`, the GC becomes more active and aggressive. This is to prevent your application from exceeding the allocated soft memory cap, which can help in avoiding premature Out-Of-Memory (OOM) kills by managing memory more strictly.\n",
      "\n",
      "By adjusting the behavior of the GC based on the current memory usage in relation to `GOMEMLIMIT`, applications can handle fluctuations in memory demand more gracefully, providing stability and performance improvements.\n",
      "\n",
      "Overall, `GOMEMLIMIT` is a game-changer because it provides better control over memory usage, which can be particularly beneficial for high-memory applications.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 8 wins\n",
      "Agentic RAG: 17 wins\n",
      "After 25 rounds\n",
      "\n",
      "{'source': 'In addition to these two models, however, you can use any model from the Hugging Face Hub (or your own) by following [this guide](/developers/weaviate/modules/reader-generator-modules/sum-transformers#use-another-summarization-module-from-hugging-face). Even when looking only at language models that are trained for summarization tasks, there is still a wide range of choices in terms of sheer numbers, which vary in the target domain (e.g. medical, legal, scientific, etc.) and size (number of parameters, i.e. speed). If you have specific needs, we recommend investigating other models. ### Avoid too long an input\\n\\nAll transformer models have a maximum input length size. For example, `bart-large-cnn` has a maximum limit of 1024 tokens, where each token is part of a word (i.e. a few characters).', 'gold_answer': 'To use a different summarization model from the Hugging Face Hub for your project, you would need to follow the guide linked in the context. This guide will provide you with the necessary steps to integrate a different model into your project. If you have your own models, you can upload them to Hugging Face as private modules and use them in your project. However, keep in mind that all transformer models have a maximum input length size, so you will need to ensure your input does not exceed this limit.', 'query': 'How can I use a different summarization model from the Hugging Face Hub for my project?'}\n",
      "\u001b[96mQuery: How can I use a different summarization model from the Hugging Face Hub for my project?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "To use a different summarization model from the Hugging Face Hub for your project, you can follow these steps:\n",
      "\n",
      "1. **Explore and Choose a Model:**\n",
      "   - Visit the Hugging Face model hub and find a summarization model that fits your needs. You can filter models by task, such as \"summarization.\"\n",
      "\n",
      "2. **Get an API Token:**\n",
      "   - Sign up or log in to Hugging Face to obtain a Hugging Face API Token. You can request it here: [Get Hugging Face API Token](https://huggingface.co/login?next=%2Fsettings%2Ftokens).\n",
      "\n",
      "3. **Use the Model with Inference API:**\n",
      "   - Utilize the Hugging Face Inference API to make requests to the model. Ensure you include your API Token in the header of your requests.\n",
      "\n",
      "4. **Integrate with Your Application:**\n",
      "   - Use the model in your application by sending data (text) to the model through the Inference API and receiving the summarized text as a response.\n",
      "\n",
      "5. **Deployment (if needed):**\n",
      "   - If you want to use the model in production, you might consider setting up a server or using a service that handles API requests efficiently, ensuring you have the necessary compute resources to handle inference.\n",
      "\n",
      "By following these steps, you can integrate a Hugging Face summarization model into your project and use it to summarize texts.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "To use a different summarization model from the Hugging Face Hub for your project, you can follow these general steps:\n",
      "\n",
      "1. **Access Models on Hugging Face:**\n",
      "   - Visit the [Hugging Face Models Hub](https://huggingface.co/models) and search for summarization models. You can filter models by task to find those specifically designed for text summarization.\n",
      "\n",
      "2. **Select a Model:**\n",
      "   - Choose a model that best fits your project requirements. Models have varying capabilities and may be fine-tuned for specific kinds of data or languages. Review the model card for details on its performance and intended use.\n",
      "\n",
      "3. **Install Hugging Face Transformers:**\n",
      "   - Ensure you have the Hugging Face Transformers library installed. You can install it using pip:\n",
      "     ```bash\n",
      "     pip install transformers\n",
      "     ```\n",
      "\n",
      "4. **Load the Model:**\n",
      "   - In your Python script, use the Transformers library to load the model. Here is a sample code snippet to get you started:\n",
      "   ```python\n",
      "   from transformers import pipeline\n",
      "\n",
      "   # Load summarization pipeline\n",
      "   summarizer = pipeline(\"summarization\", model=\"model_name_here\")\n",
      "\n",
      "   # Summarize text\n",
      "   text_to_summarize = \"Your text here...\"\n",
      "   summary = summarizer(text_to_summarize)\n",
      "\n",
      "   print(summary)\n",
      "   ```\n",
      "   - Replace `\"model_name_here\"` with the name of the model you selected from the Hugging Face Hub.\n",
      "\n",
      "5. **Use the Model:**\n",
      "   - Run your script to use the summarization model for processing your text data.\n",
      "\n",
      "6. **Hugging Face API Token:**\n",
      "   - For some functionalities or when using hosted inference APIs, you may need a Hugging Face API token. You can obtain one from your Hugging Face account settings.\n",
      "\n",
      "By following these steps, you can effectively incorporate a different summarization model from the Hugging Face Hub into your project, leveraging state-of-the-art NLP capabilities.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 8 wins\n",
      "Agentic RAG: 18 wins\n",
      "After 26 rounds\n",
      "\n",
      "{'source': 'We can then vectorize this text description using off-the-shelf models from OpenAI, Cohere, HuggingFace, and others to unlock semantic search. We recently presented an example of this idea for [AirBnB listings](https://weaviate.io/blog/generative-feedback-loops-with-llms), translating tabular data about each property’s price, neighborhood, and more into a text description. Huge thanks to Svitlana Smolianova for creating the following animation of the concept. <img\\n    src={require(\\'./img/gen-example.gif\\').default}\\n    alt=\"alt\"\\n    style={{ width: \"100%\" }}\\n/>\\n\\n### Text Chunking\\nSimilarly related to the 512 token length for vectorizing text chunks, we may consider using the Large Language Model to identify good places to cut up text chunks. For example, if we have a list of items, it might not be best practice to separate the list into 2 chunks because the first half fell into the tail end of a chunk[:512] loop.', 'gold_answer': 'The strategy for chunking text for vectorization when dealing with a 512 token length limit involves using a Large Language Model to identify suitable places to cut up text chunks. This process, known as \"chunking\", breaks down long documents into smaller sections, each containing an important piece of information. This approach not only helps to stay within the LLMs token limit but also enhances the retrieval of information. It\\'s important to note that the chunking should be done thoughtfully, not just splitting a list of items into 2 chunks because the first half fell into the tail end of a chunk[:512] loop.', 'query': 'What is the strategy for chunking text for vectorization when dealing with a 512 token length limit?'}\n",
      "\u001b[96mQuery: What is the strategy for chunking text for vectorization when dealing with a 512 token length limit?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The strategy for chunking text for vectorization when dealing with a 512 token length limit involves several considerations outlined in the search results. \n",
      "\n",
      "1. **Chunking Strategy**: When dealing with a document longer than the token limit, you should segment it into chunks that are each within the 512 token length. This can be done using a \"late chunking\" method, where contextual information is maintained between neighboring chunks, and embeddings are pooled only after dividing the document into chunks.\n",
      "\n",
      "2. **Dimensional Pooling**: Late chunking involves a pooling step after the initial inference. Instead of pooling all representations into a single vector for the entire document, you segment the text and pool within those segments. This allows the embeddings for each chunk to retain their contextual relevance with their neighboring chunks.\n",
      "\n",
      "3. **Identifying Chunk Boundaries**: It's important to identify good chunk boundaries, potentially using cues for where natural divisions occur, such as sentence ends, paragraph breaks, or logical divisions in the text.\n",
      "\n",
      "4. **Preserving Context**: The aim is to retain enough contextual information for each chunk so that when they are vectorized and stored, they can be effectively retrieved while maintaining the precision of the document's meaning.\n",
      "\n",
      "By following these strategies, long documents can be managed effectively within the constraints of a 512 token length limit, allowing for more precise information retrieval and vectorization.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "When dealing with a 512 token length limit for vectorization, a key strategy involves chunking the text appropriately. Here are some insights from the blog search results:\n",
      "\n",
      "1. **Late Chunking Strategy**: This approach involves maintaining the contextual relationships across the entire document during the embedding process, and only dividing these contextually-rich embeddings into chunks afterwards. This strategy helps balance the precision of full-document embedding with the granularity needed for effective retrieval. Instead of immediately pooling all token embeddings into a single vector, late chunking applies pooling on segments of the text based on token spans or boundary cues. This way, a document is represented by multiple embeddings, each holding contextual information (Search Result 1 and 3).\n",
      "\n",
      "2. **Identifying Chunk Locations**: It's important to choose good places to break up the text. For example, for lists or structured data, it might not be ideal to split in the middle, especially if it breaks the continuity of related items. Using large language models to identify these optimal cut-off points can be beneficial (Search Result 2).\n",
      "\n",
      "3. **Enhanced Retrieval**: By chunking text into smaller sections, it accommodates the token limits and ensures that each chunk retains crucial information for retrieval. This practice enhances the effectiveness of data retrieval when using vector databases (Search Result 5).\n",
      "\n",
      "Implementing a thoughtful chunking strategy ensures that each vectorized chunk preserves enough contextual information, achieving a balance between granularity and precision in retrieval tasks.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"vanilla rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 9 wins\n",
      "Agentic RAG: 18 wins\n",
      "After 27 rounds\n",
      "\n",
      "{'source': 'You can find other dev container templates [here](https://containers.dev/templates). **Step 2. Test the Setup**\\n\\n1. After setting up the environment we tested that everything worked by:\\nCreate a [dummy endpoint](https://github.com/weaviate/howto-weaviate-retrieval-plugin/commit/57c00bc85aea3a155d330f0de72525ee26a665d1#diff-c16fbf0c6f7b90a46b94b36f88893c2d174476088608841f7254afba0e81373d) which will simply respond with a `{“Hello”: “World”}` object when called. ```python\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n@app.get(\"/\")\\ndef read_root():\\n    \"\"\"\\n    Say hello to the world\\n    \"\"\"\\n    return {\"Hello\": \"World\"}\\n```\\n\\n2.', 'gold_answer': 'To create a dummy endpoint in FastAPI that returns `{\"Hello\": \"World\"}` when accessed, you need to follow these steps:\\n\\n1. Import the FastAPI module: `from fastapi import FastAPI`\\n2. Create an instance of the FastAPI class: `app = FastAPI()`\\n3. Define a route that responds to HTTP GET requests at the root (\"/\") URL. This is done by using the `@app.get(\"/\")` decorator followed by a function that returns the desired message. The function could look like this:\\n```python\\ndef read_root():\\n    \"\"\"\\n    Say hello to the world\\n    \"\"\"\\n    return {\"Hello\": \"World\"}\\n```\\nSo, the complete code would look like this:\\n```python\\nfrom fastapi import FastAPI\\n\\napp = FastAPI()\\n\\n@app.get(\"/\")\\ndef read_root():\\n    \"\"\"\\n    Say hello to the world\\n    \"\"\"\\n    return {\"Hello\": \"World\"}\\n```\\nWhen this code is run and the application is accessed at its root URL, it will respond with `{\"Hello\": \"World\"}`.', 'query': 'How do you create a dummy endpoint in FastAPI that returns `{\"Hello\": \"World\"}` when accessed?'}\n",
      "\u001b[96mQuery: How do you create a dummy endpoint in FastAPI that returns `{\"Hello\": \"World\"}` when accessed?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "To create a dummy endpoint in FastAPI that returns `{\"Hello\": \"World\"}` when accessed, you can use the following code:\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "@app.get(\"/\")\n",
      "def read_root():\n",
      "    \"\"\"\n",
      "    Say hello to the world\n",
      "    \"\"\"\n",
      "    return {\"Hello\": \"World\"}\n",
      "```\n",
      "\n",
      "This code sets up a simple FastAPI application with one endpoint at the root (`/`). When accessed, this endpoint returns a JSON response with the content `{\"Hello\": \"World\"}`.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "To create a dummy endpoint in FastAPI that returns `{\"Hello\": \"World\"}`, you need to follow these steps:\n",
      "\n",
      "1. Install FastAPI and an ASGI server like Uvicorn if you haven't already:\n",
      "\n",
      "   ```bash\n",
      "   pip install fastapi uvicorn\n",
      "   ```\n",
      "\n",
      "2. Create a Python file (e.g., `main.py`) with the following content:\n",
      "\n",
      "   ```python\n",
      "   from fastapi import FastAPI\n",
      "\n",
      "   app = FastAPI()\n",
      "\n",
      "   @app.get(\"/\")\n",
      "   async def read_root():\n",
      "       return {\"Hello\": \"World\"}\n",
      "   ```\n",
      "\n",
      "3. Run the FastAPI application using Uvicorn:\n",
      "\n",
      "   ```bash\n",
      "   uvicorn main:app --reload\n",
      "   ```\n",
      "\n",
      "4. Access the endpoint by navigating to `http://127.0.0.1:8000/` in your web browser or using a tool like `curl` or Postman. You should see the response `{\"Hello\": \"World\"}`.\n",
      "\n",
      "This code creates a simple FastAPI application with one GET endpoint at the root URL `/`, which returns a JSON object `{\"Hello\": \"World\"}` when accessed.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 9 wins\n",
      "Agentic RAG: 19 wins\n",
      "After 28 rounds\n",
      "\n",
      "{'source': 'Head to the [documentation](/developers/weaviate/configuration/backups) for a more in-depth overview and instructions. ## Reduced memory usage\\n\\n![Reduced memory usage](./img/reduced-memory-usage.jpg)\\n\\nAs part of the continuous effort to make Weaviate faster, leaner and more powerful, we introduced new optimizations to use less RAM without sacrificing performance. ### Thread pooling optimization\\n\\nFirst, we set our sights on parallel imports, where we introduced thread pooling to reduce memory spikes while importing data. Previously if you had, e.g., 8 CPUs and would import from 4 client threads, each client request would run with a parallelization factor of 8 (one per CPU core). So, in the worst case, you could end up with 32 parallel imports (on a machine with \"only\" 8 CPUs).', 'gold_answer': 'Weaviate has introduced thread pooling optimization to manage memory usage during parallel data imports. This optimization ensures that the parallelization does not exceed the number of CPU cores, thus providing maximum performance without unnecessary memory usage.', 'query': 'What optimization has Weaviate introduced to manage memory usage during parallel data imports?'}\n",
      "\u001b[96mQuery: What optimization has Weaviate introduced to manage memory usage during parallel data imports?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Weaviate introduced thread pooling optimization to manage memory usage during parallel data imports. This optimization reduces memory spikes by ensuring that the number of parallel imports does not exceed the number of CPU cores available. This way, Weaviate achieves maximum performance without unnecessary memory usage during parallel data imports.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Weaviate has introduced thread pooling optimization to manage memory usage during parallel data imports. Previously, if multiple client threads were importing data in parallel, each would run with a parallelization factor equal to the number of CPU cores, potentially leading to memory spikes without performance gains. The update ensures that parallelization does not exceed the number of CPU cores, achieving maximum performance without unnecessary memory usage.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 9 wins\n",
      "Agentic RAG: 20 wins\n",
      "After 29 rounds\n",
      "\n",
      "{'source': 'Similarly to the original Gorilla paper’s use of Abstract Syntax Tree evaluation, we are also considering an n-gram match where we construct keywords for each query such as “bm25”, “query”, “title” and check how many are contained in the generated query. We can also use the finer-grained perplexity metric that measures the log probability of the ground truth tokens at each step of decoding. We are currently using a simple greedy decoding algorithm to sample from the LoRA fine-tuned LlaMA 7B LLM. To ground the evaluation discussion further, let’s take a look at an incorrect query:\\n\\n```graphql\\n{\\n\\tGet {\\n\\t\\tJobListing(\\n\\t\\t\\tbm25: {query: “software”}\\n\\t\\t\\twhere: {path: [“salary”], operator: GreaterThan, valueNumber: 50000}\\n\\t\\t){\\n\\t\\ttitle\\n\\t\\tdescription\\n\\t\\tisRemote\\n\\t\\tpostedBy {\\n\\t\\t\\tname\\n\\t\\t  }\\n\\t\\t}\\n\\t}\\n}\\n```\\n\\nAlmost there! But unfortunately the missing comma from the `bm25` to `where` query will prevent this query from successfully executing. As discussed we may have other cases where although the syntax is correct and the query executes, it does not achieve what was specified in the natural language command.', 'gold_answer': 'The syntax error in the provided GraphQL query example is the missing comma between the `bm25` and `where` arguments in the `JobListing` function. This error could potentially affect the evaluation of n-gram matches by causing inaccurate keyword construction.', 'query': 'What is the syntax error in the provided GraphQL query example related to the evaluation of n-gram matches?'}\n",
      "\u001b[96mQuery: What is the syntax error in the provided GraphQL query example related to the evaluation of n-gram matches?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The syntax error in the provided GraphQL query example is the missing comma between the `bm25` and `where` fields. The correct syntax requires that each field or argument in a list is separated by a comma to clearly delineate them. Here's the problematic part of the query:\n",
      "\n",
      "```graphql\n",
      "JobListing(\n",
      "  bm25: {query: “software”}\n",
      "  where: {path: [“salary”], operator: GreaterThan, valueNumber: 50000}\n",
      ")\n",
      "```\n",
      "\n",
      "It should include a comma after the closing curly brace of the `bm25` argument like this:\n",
      "\n",
      "```graphql\n",
      "JobListing(\n",
      "  bm25: {query: “software”},\n",
      "  where: {path: [“salary”], operator: GreaterThan, valueNumber: 50000}\n",
      ")\n",
      "```\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The provided GraphQL query example has a syntax error due to a missing comma between the `bm25` and `where` clauses. This missing comma will prevent the query from executing successfully. Here's the incorrect part of the query for reference:\n",
      "\n",
      "```graphql\n",
      "{\n",
      "\tGet {\n",
      "\t\tJobListing(\n",
      "\t\t\tbm25: {query: “software”}\n",
      "\t\t\twhere: {path: [“salary”], operator: GreaterThan, valueNumber: 50000}\n",
      "\t\t){\n",
      "\t\ttitle\n",
      "\t\tdescription\n",
      "\t\tisRemote\n",
      "\t\tpostedBy {\n",
      "\t\t\tname\n",
      "\t\t  }\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "```\n",
      "\n",
      "The correct syntax requires a comma between the `bm25` clause and the `where` clause.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 9 wins\n",
      "Agentic RAG: 21 wins\n",
      "After 30 rounds\n",
      "\n",
      "{'source': '#### Solution\\nWe addressed each of the points above individually and improved the overall MTTR substantially:\\n\\n- A deduplication process was added, so that large WALs with a lot of updates (i.e. redundant data) could be reduced to only the necessary information. - The recovery process now runs in parallel. If there are multiple places that require recovery, they can each recover independently, without one recovery having to wait for the other. - A mechanism was added that flushes any memtable that has been idle (no writes) for 60s or more. In addition to speeding up the recovery, this change also ensures that no recovery is needed at all in many cases.', 'gold_answer': 'The MTTR was improved in the system by adding a deduplication process to reduce the size of the WALs, making the recovery process run in parallel, and adding a mechanism that flushes idle memtables. These changes resulted in a faster recovery process and in many cases, eliminated the need for recovery.', 'query': 'How was the MTTR improved in the system with regards to handling large write-ahead logs?'}\n",
      "\u001b[96mQuery: How was the MTTR improved in the system with regards to handling large write-ahead logs?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The Mean Time to Recovery (MTTR) in the system, with regards to handling large write-ahead logs, was substantially improved by implementing several enhancements:\n",
      "\n",
      "1. **Deduplication process**: This was added to reduce large Write-Ahead Logs (WALs) by eliminating redundant data, leaving only the necessary information to be processed during recovery.\n",
      "\n",
      "2. **Parallel recovery process**: Instead of a single-threaded recovery process, the updated system allows multiple recovery operations to run in parallel. This means different parts of the system can recover independently, eliminating bottlenecks where one recovery would have to wait for another to complete.\n",
      "\n",
      "3. **Idle memtable flushing**: The system now automatically flushes any memtable that has been inactive (no writes) for 60 seconds or more. This change not only speeds up the recovery process but, in many cases, ensures that recovery is not needed at all by maintaining recent data on disk.\n",
      "\n",
      "As a result of these improvements, recovery time in even the worst-case scenarios went from many hours to just a few seconds.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The Mean Time to Recovery (MTTR) in systems handling large write-ahead logs was initially slow due to the following reasons:\n",
      "\n",
      "1. **Large Unflushed Memtables**: If Weaviate faced an unexpected crash, the Write-Ahead Log (WAL) ensured no data was lost. However, unflushed memtables could grow significantly in size, leading to a large amount of data to be recovered post-crash.\n",
      "\n",
      "2. **Single-Threaded Recovery Process**: The recovery process was single-threaded, which contributed to the slow recovery time.\n",
      "\n",
      "To improve MTTR, adjustments likely included optimizing the flushing process to handle memtables more efficiently, possibly adopting multi-threaded recovery processes to speed up the rebooting and recovery phase after a crash.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"vanilla rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 10 wins\n",
      "Agentic RAG: 21 wins\n",
      "After 31 rounds\n",
      "\n",
      "{'source': '---\\ntitle: Weaviate 2023 Recap\\nslug: 2023-recap\\nauthors: [femke]\\ndate: 2023-12-26\\ntags: []\\nimage: ./img/hero.png\\ndescription: \"A reflection on 2023 from team Weaviate!\"\\n---\\n![hero](img/hero.png)\\n\\n<!-- truncate -->\\n\\nIt’s hard to imagine that less than a year ago, so very few people even knew about the concept of vector databases and how AI could benefit from them. Those who did still had many questions about how they worked and whether they could at all be helpful. Meanwhile, curiosity and interest in AI spiked, especially after OpenAI launched ChatGPT. Curiosity has sped up our progress and made more people aware of the opportunities AI offers, transforming our landscape. Let\\'s all take a moment to reflect and appreciate the start of a momentous change in how we can communicate, learn, teach, and collaborate so much faster and more effectively by leveraging AI.', 'gold_answer': 'The title of the Weaviate team\\'s reflection on the year 2023 is \"Weaviate 2023 Recap\".', 'query': \"What is the title of the Weaviate team's reflection on the year 2023?\"}\n",
      "\u001b[96mQuery: What is the title of the Weaviate team's reflection on the year 2023?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The context provided does not contain explicit information on a Weaviate team's reflection on the year 2023, only plans and projections for that year. It appears the correct title for the reflection you are asking about is not directly available in the results. There is a blog post titled \"What to expect from Weaviate in 2023,\" but it focuses on future developments rather than a reflection. Therefore, I cannot determine the exact title of the reflection from the context provided. It may be helpful to search for more recent 2023 content from Weaviate for their team reflections.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "I couldn't find a specific reflection blog post by the Weaviate team for the year 2023. Would you like me to search with a different query or assist you in another way?\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 10 wins\n",
      "Agentic RAG: 22 wins\n",
      "After 32 rounds\n",
      "\n",
      "{'source': '<br/>\\nThen you could cross-reference it with another dashboard, to see that the slow down began when the import reached 120 million objects.<br/>\\nIn two steps, you could narrow down the issue to a specific area, which would get you a lot closer to finding the solution. Or you could use that data to share it with the Weaviate team to get help. ### Try it yourserlf\\nHere is an [example project](https://github.com/weaviate/weaviate-examples/tree/main/monitoring-prometheus-grafana), it contains:\\n\\n* `docker-compose.yml` that spins up Weaviate (without any modules),\\n* a **Prometheus** instance,\\n* and a **Grafana** instance. Just spin everything up, run a few queries and navigate to the Grafana instance in the browser to see the dashboard. ### Learn more\\nTo learn more, see the [documentation](/developers/weaviate/configuration/monitoring).', 'gold_answer': 'The slow down began at 120 million imported objects according to the document.', 'query': 'At what number of imported objects did the slow down begin according to the document?'}\n",
      "\u001b[96mQuery: At what number of imported objects did the slow down begin according to the document?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The slow down in importing objects began when the database reached around 25 million objects.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The document suggests that there was pretty much no slow down in the import times regardless of the quantity of objects already imported, as indicated by a linear slope on the graph showing the number of vectors imported over time.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 10 wins\n",
      "Agentic RAG: 23 wins\n",
      "After 33 rounds\n",
      "\n",
      "{'source': \"1**: *Suppose we have vectors $x$ and $y$ represented in their original space. We apply a compression function $C$ to obtain a shorter representation of $x$ ($x'$) and $y$ ($y'$) on a compressed space but would require a decompression function $C'$ from the compressed space into the original space to be able to use the original distance function. In this case we would obtain $x''$ and $y''$ from $x'$ and $y'$ respectively and apply the distance on the approximations of the original $x$ and $y$ so $d(x,y)=d(x'',y'') + \\\\delta$ where $\\\\delta$ is the distortion added to the distance calculation due of the reconstruction of the original vectors. The compression/decompression mechanisms should be such that the distortion is minimized.*\\n\\n![comp2](./img/image2.jpg)\\n**Fig. 2**: *Suppose we have vectors $x$ and $y$ represented in their original space.\", 'gold_answer': 'The impact of vector compression and decompression on the accuracy of distance calculations between the original vectors is represented by the distortion term delta (δ). This term represents the inaccuracy introduced into the distance calculation due to the reconstruction of the original vectors from their compressed forms. The goal of the compression and decompression mechanisms is to minimize this distortion, thereby maximizing the accuracy of the distance calculations.', 'query': 'What is the impact of vector compression and decompression on the accuracy of distance calculations between the original vectors?'}\n",
      "\u001b[96mQuery: What is the impact of vector compression and decompression on the accuracy of distance calculations between the original vectors?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The impact of vector compression and decompression on the accuracy of distance calculations primarily relates to the distortion that is introduced when reconstructing the original vectors from their compressed representations. When vectors \\( x \\) and \\( y \\) are compressed, then decompressed, you obtain approximations \\( x'' \\) and \\( y'' \\) instead of the exact original vectors. As a result, any distance computed between these decompressed vectors will have a distortion factor, \\(\\delta\\), compared to the distance calculated between the original vectors. The extent of this distortion depends on the effectiveness of the compression and decompression functions in retaining the original information. Ideally, these mechanisms should minimize distortion to maintain the accuracy of distance calculations and, consequently, the similarity measures derived from them.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The impact of vector compression and decompression on the accuracy of distance calculations involves several key considerations:\n",
      "\n",
      "1. **Distortion in Distance Calculation**: When vectors are compressed, a new distance metric in the compressed space is often required. This introduces a distortion term (\\(\\delta\\)) in the distance calculation, i.e., \\(d(x,y) = d'(x',y') + \\delta\\), where \\(d\\) is the original distance metric and \\(d'\\) is the distance metric on compressed vectors \\(x'\\) and \\(y'\\). The goal is to minimize this distortion to maintain accuracy.\n",
      "\n",
      "2. **Lossy Compression**: Some compression techniques are lossy, meaning they do not perfectly preserve the original data. For instance, a technique that condenses each vector dimension to a single bit can significantly reduce space usage but makes reversing or perfectly reconstructing the original vector impossible.\n",
      "\n",
      "3. **Decompression Requirement**: In some compression schemes, you may need to decompress back to the original vector space to use the traditional distance metrics, adding potential for distortion and computational overhead.\n",
      "\n",
      "4. **Trade-Off Between Space and Accuracy**: Compression aims to maintain a \"good-enough\" representation of original vectors, balancing reduced storage space and computational efficiency with potentially decreased accuracy in distance calculations.\n",
      "\n",
      "Ultimately, the choice of compression technique and the associated distance metrics should aim to minimize these distortions to maintain accuracy as closely as possible to the original dataset.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 10 wins\n",
      "Agentic RAG: 24 wins\n",
      "After 34 rounds\n",
      "\n",
      "{'source': \"It is one of my favorite topics about Go memory management, and I'd be happy to write this post. Please let me know. For this one, it's enough to understand that sometimes heap-allocations happen even when we think they shouldn't. That is important to know because those allocations will put pressure on the GC, which is required for an unexpected OOM situation. ## Why would you run OOM – even when there is enough memory available?\", 'gold_answer': 'Heap allocations in Go can lead to out-of-memory errors even when there appears to be enough memory available due to a combination of factors. These include unexpected heap allocations where temporary memory \"escapes\" onto the heap, inefficiencies in memory usage due to the Go runtime only being able to set relative GC targets before Go 1.19, and pressure on the Garbage Collector due to these unexpected heap allocations. These factors can lead to the Garbage Collector being unable to keep up with the rate of memory allocation, resulting in an out-of-memory situation.', 'query': 'Why can heap allocations in Go lead to out-of-memory errors even when there appears to be enough memory available?'}\n",
      "\u001b[96mQuery: Why can heap allocations in Go lead to out-of-memory errors even when there appears to be enough memory available?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Heap allocations in Go can lead to out-of-memory (OOM) errors even when there appears to be enough memory available due to several factors:\n",
      "\n",
      "1. **Garbage Collector Pressure**: Heap allocations increase the load on the garbage collector (GC). When memory is allocated on the heap, it persists beyond the function's execution until it is explicitly cleaned up by the GC. If the GC cannot collect memory efficiently, it can lead to an unexpected OOM situation (Search Result 1).\n",
      "\n",
      "2. **Dynamic Allocations**: The Go runtime might perform dynamic allocations that are larger than anticipated, especially when dealing with data structures like arrays or slices whose sizes can change. This can result in higher memory usage than expected (Search Result 2).\n",
      "\n",
      "3. **Memory Management Complexity**: Heap allocations are more complex and expensive than stack allocations because the runtime needs to find contiguous memory segments that can fit new variables. This complexity can lead to scenarios where memory is technically available but not efficiently usable, causing OOM errors (Search Result 3).\n",
      "\n",
      "Overall, managing heap allocations effectively is crucial to prevent these issues, and tools like GOMEMLIMIT have been introduced to help manage memory usage more predictably (Search Result 5).\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Heap allocations in Go can lead to out-of-memory (OOM) errors even when there appears to be enough memory available for several reasons:\n",
      "\n",
      "1. **Garbage Collection (GC) Overhead**: Go uses a garbage collector to manage memory allocation and deallocation. If the heap grows too large, it can put pressure on the GC, leading to frequent GC cycles and potential OOM errors, especially if the GC cannot reclaim memory fast enough to keep up with allocations.\n",
      "\n",
      "2. **Fragmentation**: Memory fragmentation can occur when the memory space is broken into small, non-contiguous blocks. This can prevent Go from allocating large contiguous blocks of memory needed for certain operations, leading to OOM.\n",
      "\n",
      "3. **Memory Limits**: The Go runtime or the environment it's running in might impose certain limits on the amount of heap memory that can be used. For example, cloud providers or container runtimes may configure memory limits, and exceeding those can trigger OOM errors.\n",
      "\n",
      "4. **Memory Leak**: If there are memory leaks in the code (i.e., memory that is allocated but not properly released), the heap size will grow over time and could eventually lead to OOM errors.\n",
      "\n",
      "5. **Concurrency Issues**: In concurrent applications, improper synchronization can lead to higher-than-expected heap usage due to unplanned data retention or copying.\n",
      "\n",
      "6. **Configuration and Settings**: The Go runtime can be influenced by various environmental and build configurations that might affect memory allocation behavior and lead to unexpected issues when resources seem available.\n",
      "\n",
      "Understanding these factors can help diagnose and potentially alleviate the causes of out-of-memory errors in Go applications. Optimizing memory usage patterns or fine-tuning GC settings may also be beneficial.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 10 wins\n",
      "Agentic RAG: 25 wins\n",
      "After 35 rounds\n",
      "\n",
      "{'source': 'This pillar is all about performance. The first big step will be the move towards a [Native Roaring Bitmap Index](https://github.com/weaviate/weaviate/issues/2511). In the most extreme case, this new index time can speed up filtered vector search [by a factor of 1000](https://twitter.com/etiennedi/status/1621180981519458305). But it doesn’t stop there; we are already thinking about the next steps. Whether you want faster aggregations or new types of specialized indexes, we will ensure you can hit all your p99 latency targets with Weaviate.', 'gold_answer': 'The expected performance improvement for filtered vector search in Weaviate with the new Native Roaring Bitmap Index is up to a factor of 1000, in the most extreme case.', 'query': 'What is the expected performance improvement for filtered vector search in Weaviate with the new Native Roaring Bitmap Index?'}\n",
      "\u001b[96mQuery: What is the expected performance improvement for filtered vector search in Weaviate with the new Native Roaring Bitmap Index?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The expected performance improvement for filtered vector search in Weaviate with the new Native Roaring Bitmap Index is by a factor of 1000 in the most extreme case.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The expected performance improvement for filtered vector search in Weaviate with the new Native Roaring Bitmap Index can be as much as 1000 times faster in certain cases. This significant acceleration allows for much quicker filtered searches within Weaviate's vector database.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 10 wins\n",
      "Agentic RAG: 26 wins\n",
      "After 36 rounds\n",
      "\n",
      "{'source': 'This case is quite similar to our discussion of Multi-Index Routing and we can similarly evaluate generations with a prompt that explains the needs for SQL and Vector Databases and then asks the LLM whether the router made the right decision. We can also use the RAGAS Context Relevance score for the results of the SQL query. <img\\n  src={require(\\'./img/sql-router.png\\').default}\\n  alt=\"SQL Router Query Engine\"\\n  style={{ maxWidth: \"60%\" }}\\n/>\\n\\nConcluding our discussion of “From RAG to Agent Evaluation”, we believe that it is still too early to tell what the common patterns will be for agent use. We have intentionally shown the multi-hop query engine and query router because these are relatively straightforward to understand. Once we add more open-ended planning loops, tool use and the associated evaluation of how well the model can format API requests to the tool, and more meta internal memory management prompts such as the ideas in MemGPT, it is very difficult to provide a general abstraction around how Agents will be evaluated.', 'gold_answer': \"The considerations for evaluating agents in the context of Multi-Index Routing and SQL and Vector Databases include the complexity of the agent's tasks, the relevance of the context in which the query is made, the ability of the agent to decompose a question into sub-questions and combine the answers to answer the original question, and the use of vector databases to provide LLMs with long-term memory.\", 'query': 'What are the considerations for evaluating agents in the context of Multi-Index Routing and SQL and Vector Databases as discussed in the document?'}\n",
      "\u001b[96mQuery: What are the considerations for evaluating agents in the context of Multi-Index Routing and SQL and Vector Databases as discussed in the document?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "In the context of evaluating agents for Multi-Index Routing, SQL, and Vector Databases, there are several considerations to take into account:\n",
      "\n",
      "1. **Decision Making**: It's important to assess whether the agent or routing system made the correct decision when choosing between an SQL or Vector Database for a query. This involves understanding the needs and requirements for each type of database.\n",
      "\n",
      "2. **Evaluation Metrics**: The RAGAS Context Relevance score can be used to evaluate the results of an SQL query, helping to determine whether the routing decision was appropriate.\n",
      "\n",
      "3. **Complexity of Evaluation**: As systems evolve from RAG (Retrieval-Augmented Generation) to more complex agent-based systems, the evaluation becomes more challenging. These systems may involve multi-hop query engines, open-ended planning loops, tool use, and API request formatting, which require a nuanced and diverse evaluation framework.\n",
      "\n",
      "4. **Integration and Performance**: In the case of vector databases, evaluations should consider factors like latency, memory footprint, recall and retrieval relevance, cost efficiency, and integration capabilities with existing technologies like AI and embedding models.\n",
      "\n",
      "5. **Functionalities of Vector Databases**: Vector databases provide comprehensive functionalities, managing high-dimensional vector embeddings for scalable similarity searches, which is crucial in AI-native applications handling unstructured data.\n",
      "\n",
      "Overall, when evaluating agents that interact with SQL and vector databases, it's important to consider how well the agents make routing decisions, the effectiveness of their query results, and their ability to handle the complexity of evolving query systems. Additionally, specific performance and integration metrics for each type of database must be considered.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "When evaluating agents in the context of Multi-Index Routing and SQL and Vector Databases, several considerations are highlighted in the documents. Here are the key points:\n",
      "\n",
      "1. **Evaluation of Router Decisions**: When using Multi-Index Routing, the evaluation can involve creating prompts that explain the specific needs for SQL and Vector Databases. The Large Language Model (LLM) is then asked whether the router made the right decision. This involves understanding the context and the requirements for the retrieval system.\n",
      "\n",
      "2. **Context Relevance Score**: The RAGAS Context Relevance score is used to assess the results of SQL queries. This score helps in determining whether the query results are relevant to the context provided by the input query.\n",
      "\n",
      "3. **Complexities in Agent Evaluation**: As the systems get more complex with open-ended planning loops and tool uses, especially with features like multi-hop query engines and query routers, providing a generalized abstraction around evaluating agents becomes challenging. The evaluation process may need to incorporate various dimensions such as tool use efficiency, API request formatting, and internal memory management.\n",
      "\n",
      "4. **Agent Architecture**: Agent systems involve several components including LLMs with roles and tasks, memory management (both short-term and long-term), and planning processes like reflection, self-criticism, and query routing. The evaluation of agents might involve assessing how these elements are coordinated and executed.\n",
      "\n",
      "5. **Vector Database Selection**: When dealing with Vector Databases, considerations include latency of searches, memory footprint scaling, relevancy of retrieval, cost efficiency, and how well these databases integrate with existing AI tools and applications.\n",
      "\n",
      "6. **Efficiency of Vector Databases**: AI-native vector databases are advantageous over traditional SQL databases due to their efficient handling of vector embeddings and retrieval via techniques like Approximate Nearest Neighbor (ANN) algorithms.\n",
      "\n",
      "These considerations indicate a comprehensive approach to evaluating agent performance within complex systems, focusing on both the decision-making processes and technical functionalities of the infrastructure used for Multi-Index Routing, SQL, and Vector Databases.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 10 wins\n",
      "Agentic RAG: 27 wins\n",
      "After 37 rounds\n",
      "\n",
      "{'source': 'Send the relevant properties for vectorization to Cohere\\n1. Store the data object and index the vector in Weaviate\\n\\n![Weaviate Import with Cohere](./img/cohere-import.png)\\n\\nThis allows Weaviate to efficiently store and query your data. ### Query\\nWhen you query Weaviate, Weaviate will:\\n1. Receive the query\\n1. Send the query to Cohere to vectorize it\\n1.', 'gold_answer': 'When Weaviate processes and vectorizes queries using Cohere, it follows these steps: \\n1. Weaviate receives the query.\\n2. The query is sent to Cohere for vectorization.\\n3. Cohere vectorizes the data using Large Language Models.\\n4. The returned query vector is used by Weaviate to search in the vector index.\\n5. Weaviate returns the matched objects. \\nThis process demonstrates a division of roles where Cohere handles the vectorization of the data and Weaviate manages the process, stores the data and vectors, and ensures production-ready performance.', 'query': 'How does Weaviate process and vectorize queries using Cohere?'}\n",
      "\u001b[96mQuery: How does Weaviate process and vectorize queries using Cohere?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Weaviate processes and vectorizes queries using Cohere by interacting with Cohere’s AI platform to generate vector embeddings. When a query is received by Weaviate, it is sent to Cohere to be vectorized. Here’s a summary of the process:\n",
      "\n",
      "1. **Receive the Query**: Weaviate first receives the query input from the user.\n",
      "2. **Send to Cohere for Vectorization**: The query is then sent to Cohere, where it is transformed into a vector representation using Cohere's language models.\n",
      "3. **Automated Communication**: This vectorization process is automated and handled through the configuration settings in Weaviate, which abstracts the details of the Cohere model.\n",
      "\n",
      "The integration is facilitated by a module like `text2vec-cohere`, which allows for the seamless use of Cohere’s large language models to handle complex natural language processing queries directly within Weaviate. The resulting vectors are used to search and match against existing data vectors stored in Weaviate, enabling efficient query processing and retrieval.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Weaviate processes and vectorizes queries using Cohere through a sequence of orchestrated events. Here's a breakdown of how this process works:\n",
      "\n",
      "1. **Reception of Query**: When you send a query to Weaviate, it receives the user's query text.\n",
      "\n",
      "2. **Query Vectorization**: Weaviate sends this query text to Cohere, a model provider, to vectorize the query. Cohere uses large language models to generate a query vector.\n",
      "\n",
      "3. **Search**: Once the query is vectorized, Weaviate uses this vector to search within its vector index.\n",
      "\n",
      "4. **Return Results**: The system returns the objects from the database that match the query vector.\n",
      "\n",
      "Weaviate is responsible for orchestrating the process, storing the data, and handling the vector search through its production-ready performance system. Cohere, on the other hand, is responsible for the actual vectorization of both data and queries using its sophisticated models. \n",
      "\n",
      "Additionally, the entire process is automated. When data objects are imported or modified within Weaviate, it will automatically communicate with Cohere to obtain the necessary vectors and store them.\n",
      "\n",
      "This integration enables Weaviate to function efficiently as a real-time vector database, supporting full CRUD capabilities and ensuring the continuous operation of its database even when new data is added or existing data is modified.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 10 wins\n",
      "Agentic RAG: 28 wins\n",
      "After 38 rounds\n",
      "\n",
      "{'source': 'Typically this is in the context of recommendation in which we have metadata about users, as well as the documents or items. So for example, say we have features that describe a Users looking for Movies such as:\\n\\nUser Features - (Age, Gender, Location, Occupation, Preferences)\\nMovie Features - (Release Year, Genre, Box Office, Duration). So together, the Metadata ranker takes as input something like: [Age, Gender, Location, Occupation, Preferences, Release year, Genre, Box Office, Duration] and predicts a score of how much this User will like the movie. We can fix the User features and rotate in each Document to get a score for each of the candidate movies (retrieved with something like ref2vec) to rank with. In addition to vectors, Weaviate also enables storing metadata features about objects such as `price`, or `color`.', 'gold_answer': 'The Metadata ranker predicts which movies a user will like by taking into account both user features (such as age, gender, location, occupation, and preferences) and movie features (such as release year, genre, box office, and duration). It fixes the user features and rotates in each movie to get a score for each of the candidate movies. This score is then used to rank the movies in order of predicted preference for the user, providing a personalized and context-aware recommendation.', 'query': 'How does the Metadata ranker predict which movies a user will like based on their personal features and movie characteristics?'}\n",
      "\u001b[96mQuery: How does the Metadata ranker predict which movies a user will like based on their personal features and movie characteristics?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The Metadata ranker predicts which movies a user will like by utilizing both user features and movie characteristics. It takes into account metadata such as user features (Age, Gender, Location, Occupation, Preferences) and movie features (Release Year, Genre, Box Office, Duration). The ranker combines these features to predict a score that represents the likelihood of a user liking a particular movie. By fixing the user features and evaluating different movies, the ranker can assign scores to each potential movie. This approach enables a personalized recommendation by considering symbolic and contextual features to rank the relevance of the movies for a specific user.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The Metadata Ranker predicts which movies a user will like by using both user and movie features. Here's how it works:\n",
      "\n",
      "1. **User Features**: This includes personal features such as age, gender, location, occupation, and preferences.\n",
      "   \n",
      "2. **Movie Features**: These are characteristics of the movies, such as release year, genre, box office performance, and duration.\n",
      "\n",
      "3. **Prediction Process**: The Metadata Ranker takes these combined features as input, forming a vector that looks like this: `[Age, Gender, Location, Occupation, Preferences, Release year, Genre, Box Office, Duration]`.\n",
      "\n",
      "4. **Scoring**: With this input, the Metadata Ranker predicts a score of how much the user is likely to enjoy the movie. By fixing the user features and rotating through different movies, the system can predict scores for each movie, which can then be used to rank the movies for the user.\n",
      "\n",
      "5. **Integration with Weaviate**: This process can be enhanced with Weaviate, which enables storing metadata features about objects, providing a more context-aware and personalized movie recommendation experience.\n",
      "\n",
      "In summary, the Metadata Ranker uses a combination of user and movie features to make predictions about a user's movie preferences, helping to provide personalized recommendations.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 10 wins\n",
      "Agentic RAG: 29 wins\n",
      "After 39 rounds\n",
      "\n",
      "{'source': 'Please boost relevance based on recency and if the Author is “Connor Shorten”. ```\\n\\nEach search result then comes packaged with their associated metadata in a key-value array. This offers the additional benefit of allowing business practitioners to easily swap out the ranking logic. This also holds the benefit of dramatically increasing the interpretability of recommendation systems, since LLMs can easily be prompted to provide an explanation of the ranking in addition to the ranking itself. ## Search Result Compression\\n\\nTraditionally, search results are presented to human users as a long list of relevant websites or passages.', 'gold_answer': 'The benefits of packaging search results with associated metadata in a key-value array include the ability to easily swap out the ranking logic, increased interpretability of recommendation systems, and the provision of a more personalized and context-aware search experience.', 'query': 'What are the benefits of packaging search results with associated metadata in a key-value array?'}\n",
      "\u001b[96mQuery: What are the benefits of packaging search results with associated metadata in a key-value array?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Packaging search results with associated metadata in a key-value array offers several benefits:\n",
      "\n",
      "1. **Flexibility in Ranking Logic**: It allows business practitioners to easily change the ranking logic without modifying the core search algorithms. This flexibility means that the ranking criteria can be adjusted based on different business needs or user preferences.\n",
      "\n",
      "2. **Enhanced Interpretability**: Key-value pairs make it easier for large language models (LLMs) to provide explanations for search rankings. By structuring metadata in an interpretable format, LLMs can be prompted to explain why certain results were prioritized over others, increasing transparency in the recommendation system.\n",
      "\n",
      "3. **Personalization and Context Awareness**: By incorporating metadata into the ranking process, search results can be more personalized and context-aware. Metadata rankers can consider various user and document characteristics (like age, gender, preferences, etc.) to tailor results to the specific needs and context of the user.\n",
      "\n",
      "4. **Improved User Experience**: By using metadata to enhance the relevance of search results, users receive more pertinent and contextually appropriate information, improving the overall user engagement and satisfaction.\n",
      "\n",
      "Overall, packaging search results with metadata in a key-value array maximizes the efficacy of search systems by enabling flexible, interpretable, and personalized search experiences.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Packaging search results with associated metadata in a key-value array offers several benefits:\n",
      "\n",
      "1. **Flexibility in Ranking Logic**: By packaging metadata with search results, it becomes easier for business practitioners to customize and swap out the ranking logic based on different criteria. This flexibility allows businesses to prioritize certain outcomes or change strategies without a complete overhaul of the system.\n",
      "\n",
      "2. **Improved Interpretability**: Having metadata linked to each search result significantly increases the interpretability of recommendation systems. This is especially useful when using large language models (LLMs), as they can provide not only the ranking but also an explanation of why certain results were ranked higher.\n",
      "\n",
      "3. **Semantic Search Enhancement**: Using LLMs in conjunction with key-value metadata arrays allows for a more nuanced search experience, improving search capabilities such as retrieval-augmented generation, and better query understanding.\n",
      "\n",
      "4. **Efficiency in Search Result Management**: The approach facilitates better management of search results, such as compressing them and easily handling updates.\n",
      "\n",
      "These benefits make key-value array packaging of search results a robust method to manage and interpret data, while also enhancing the functionality and adaptability of search systems.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"vanilla rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 11 wins\n",
      "Agentic RAG: 29 wins\n",
      "After 40 rounds\n",
      "\n",
      "{'source': '<div align=\"center\">\\n\\n![\"Bob talks about feedback loops\"](img/bob-gen-ai.jpg)\\n##### Our CEO Bob talking about the next level of vector database capabilities - Generative Feedback Loops\\n\\n</div>\\n\\nWe also got tons of great feedback on our open-source RAG app, Verba, which we used to demo Weaviate. It’s something you can use to quickly build RAG apps that fit your use case. Check out [our intro video](https://youtu.be/IiNDCPwmqF8?si=cZxfsnY6vW75c4BD) or dive right in to [our GitHub repo](https://youtu.be/IiNDCPwmqF8?si=cZxfsnY6vW75c4BD)!\\n\\n<div align=\"center\">\\n\\n![\"Adam demos Verba\"](img/adam-verba.jpg)\\n##### Adam from our Developer Growth team demoing Verba, powered by Weaviate!\\n\\n</div>\\n\\nIt was great hearing about the different use cases and industries in which folks are building AI apps – these included chatbots and search applications for things like customer support, document search, image search, and even video search. A common pattern we saw was that organizations of all sizes are evaluating vector database platforms and want guidance on how to find the right database for them. One of our recommendations is to look for a vector database purpose-built for AI.', 'gold_answer': 'The CEO who discussed the next level of vector database capabilities related to Generative Feedback Loops is Bob.', 'query': 'Who is the CEO that discussed the next level of vector database capabilities related to Generative Feedback Loops?'}\n",
      "\u001b[96mQuery: Who is the CEO that discussed the next level of vector database capabilities related to Generative Feedback Loops?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The context provided does not specify the name of a CEO discussing the next level of vector database capabilities related to Generative Feedback Loops. The information given focuses on explaining Generative Feedback Loops, their applications, and examples.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The search did not return specific information about a CEO discussing the next level of vector database capabilities related to Generative Feedback Loops. The blog posts focused more on explaining what Generative Feedback Loops are and how they can be applied, particularly in ecommerce and data ingestion contexts, but did not mention any CEO statements.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 11 wins\n",
      "Agentic RAG: 30 wins\n",
      "After 41 rounds\n",
      "\n",
      "{'source': \"**Automatic Deserialization**: Thanks to the integration with Confluent's Schema Registry, the data in the Kafka message has been automatically deserialized. Each key in the message's data becomes its own property in the corresponding Weaviate object. For example, **`city`**, **`first_name`**, and **`last_name`** are all individual properties in Weaviate, making the data immediately usable for your AI applications. ## Closing CTAs\\n\\nNot yet a Weaviate customer? Kickstart your AI journey with a [free sandbox environment](https://console.weaviate.cloud/) in Weaviate Cloud Services today - no credit card required.\", 'gold_answer': \"Weaviate handles Kafka message data with Confluent's Schema Registry integration by automatically extracting the schema ID from the message's value and querying the Schema Registry to retrieve the associated schema. This schema is then used to deserialize the rest of the data, ensuring accurate data streaming into Weaviate. Each key in the Kafka message's data becomes its own property in the corresponding Weaviate object, making the data immediately usable for AI applications. Before streaming data into Weaviate, a schema is created in Weaviate to define the structure of the Kafka messages that will be stored.\", 'query': \"How does Weaviate handle Kafka message data with Confluent's Schema Registry integration?\"}\n",
      "\u001b[96mQuery: How does Weaviate handle Kafka message data with Confluent's Schema Registry integration?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Weaviate handles Kafka message data by integrating with Confluent's Schema Registry to facilitate automatic deserialization of the messages. Here's how it works:\n",
      "\n",
      "1. **Schema Extraction and Deserialization**: The integration automatically extracts the schema ID from a Kafka message's value. It queries the Schema Registry to retrieve the associated schema. This schema is then used to deserialize the message data, ensuring accurate streaming into Weaviate.\n",
      "\n",
      "2. **Automatic Property Mapping**: After deserialization, each key in the Kafka message data becomes its own property in the corresponding Weaviate object. This makes the data immediately usable within Weaviate.\n",
      "\n",
      "3. **Real-time Streaming**: The data is streamed from Confluent Cloud to Weaviate using a Spark environment. A specific function writes the DataFrame to Weaviate, utilizing the `confluent-connector` library.\n",
      "\n",
      "4. **Data and Metadata Storage**: In Weaviate, all fields from the Kafka message are stored, and they are prefixed with `_kafka` to aid in data lineage. This includes metadata such as the Kafka key, offset, partition, schema ID, timestamp, etc., alongside user data.\n",
      "\n",
      "This integration allows Weaviate to seamlessly handle and store Kafka message data, making it ready for use in AI applications.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Weaviate integrates with Confluent's Schema Registry to facilitate the streaming of Kafka message data into its system. Here’s how Weaviate handles this process:\n",
      "\n",
      "1. **Schema Integration**: The integration with Confluent's Schema Registry allows Weaviate to automatically extract the schema ID from the Kafka message's value. It then queries the Schema Registry to retrieve the associated schema for that message.\n",
      "\n",
      "2. **Deserialization**: The retrieved schema is used to deserialize the message data. This ensures that each element within the Kafka message is appropriately converted into properties within Weaviate. For instance, fields like `city`, `first_name`, and `last_name` in the Kafka message are converted into their corresponding properties in the Weaviate object.\n",
      "\n",
      "3. **Data Structure in Weaviate**: Once deserialized, the data is structured into Weaviate, with Kafka message metadata (such as `_kafka_key`, `_kafka_offset`, `_kafka_partition`, etc.) included. This is prefixed with `_kafka` to maintain data lineage, enabling the ability to trace the origin of each object back to its Kafka source.\n",
      "\n",
      "4. **Schema Creation**: Before starting the data streaming process, a schema defining the structure of the incoming Kafka messages must be created within Weaviate. This schema ensures that the data is organized and vectorized correctly for AI applications within Weaviate.\n",
      "\n",
      "This seamless integration allows for efficient data streaming and utilization within Weaviate, making the data immediately usable for various AI applications.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 11 wins\n",
      "Agentic RAG: 31 wins\n",
      "After 42 rounds\n",
      "\n",
      "{'source': 'A dataset labeled for Question Answering is very different from Image Classification, or even Duplicate Question Detection. ### Task Benchmarks\\nTwo excellent benchmarks that collect Supervised Learning tasks to evaluate Sentence Transformers are Knowledge Intensive Language Tasks ([KILT](https://ai.facebook.com/tools/kilt/)) and Benchmarking Information Retrieval ([BEIR](https://arxiv.org/abs/2104.08663)). **KILT** uses the same domain for all tasks, Wikipedia. Each task is labeled with a different task: Slot Filling, Question Answering, Dialogue, Fact Checking, or Entity Linking. ![KILT Benchmark](./img/KILT-min-correct.png)\\n\\n**BEIR** tests many different tasks: Fact Checking, Citation Prediction, Duplicate Question Retrieval, Argument Retrieval and more.', 'gold_answer': 'The tasks included in the KILT benchmark for evaluating Sentence Transformers are Slot Filling, Question Answering, Dialogue, Fact Checking, and Entity Linking.', 'query': 'What are the tasks included in the KILT benchmark for evaluating Sentence Transformers?'}\n",
      "\u001b[96mQuery: What are the tasks included in the KILT benchmark for evaluating Sentence Transformers?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "The KILT benchmark includes the following tasks for evaluating Sentence Transformers:\n",
      "\n",
      "1. Slot Filling\n",
      "2. Question Answering\n",
      "3. Dialogue\n",
      "4. Fact Checking\n",
      "5. Entity Linking\n",
      "\n",
      "All tasks in the KILT benchmark use Wikipedia as their domain.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "The KILT (Knowledge Intensive Language Tasks) benchmark is a collection of datasets designed for evaluating models on a variety of knowledge-intensive tasks. For evaluating Sentence Transformers, the KILT benchmark includes several tasks that cover categories such as fact verification, open-domain question answering, slot filling, and entity linking. Specifically, the tasks included are:\n",
      "\n",
      "1. **Fact Verification**: \n",
      "   - FEVER: A dataset for verifying factual claims against a corpus of documents.\n",
      "\n",
      "2. **Open-Domain Question Answering**:\n",
      "   - Natural Questions: A dataset involving questions that people might ask and using Wikipedia as a reference to find the answers.\n",
      "   - TriviaQA: A collection of trivia questions and their corresponding answers from various sources.\n",
      "\n",
      "3. **Slot Filling**:\n",
      "   - T-REx: Uses facts extracted from knowledge bases and requires models to fill in missing slots in sentences.\n",
      "   - zsRE (Zero-shot Relation Extraction): Encompasses a set of relations and requires the model to extract objects given a subject and a relation type.\n",
      "\n",
      "4. **Entity Linking**:\n",
      "   - AIDA CoNLL-YAGO: Entity linking by associating text mentions to Wikipedia entities.\n",
      "\n",
      "These tasks are designed to test the ability of language models to retrieve, understand and apply knowledge from text. They often require the model to access external knowledge bases and demonstrate reasoning abilities.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 11 wins\n",
      "Agentic RAG: 32 wins\n",
      "After 43 rounds\n",
      "\n",
      "{'source': '---\\ntitle: How to choose a Sentence Transformer from Hugging Face\\nslug: how-to-choose-a-sentence-transformer-from-hugging-face\\nauthors: [connor]\\ndate: 2022-10-04\\ntags: [\\'integrations\\']\\nimage: ./img/hero.png\\ndescription: \"Learn about the various Sentence Transformers from Hugging Face!\"\\n---\\n![How to choose a Sentence Transformer from Hugging Face](./img/hero.png)\\n\\n<!-- truncate -->\\n\\n[Weaviate](/developers/weaviate/) has recently unveiled a new module which allows users to easily [integrate models from Hugging Face](/blog/hugging-face-inference-api-in-weaviate) to vectorize their data and incoming queries. At the time of this writing, there are [over 700 models](https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=downloads) that can be easily plugged into Weaviate. You may ask: **Why are there so many models and how do they differ?**<br/>\\nAnd more importantly: **How to choose a Sentence Transformer for Semantic Search?**\\n\\nThere are too many models to summarize in one flowchart. So instead, we will describe factors that differentiate these models and give you tools to **choose the perfect model for your use case**. <img\\n    src={require(\\'./img/huggingface-sentence-transformers.png\\').default}\\n    alt=\"Weaviate Sentence Transformers\"\\n    style={{ maxWidth: \"70%\" }}\\n/>\\n\\n## Differences in Deep Learning models\\n\\nNot too long ago, Deep Learning models were typically differentiated based on architectural decisions.', 'gold_answer': 'To choose the right Sentence Transformer model from Hugging Face for Semantic Search, you need to identify the model that was trained on the most similar domain and task to your use case. You can do this by looking at the color-coded details provided by Hugging Face for each model. Choose a model that was trained on a dataset that closely matches your domain and is in the same language as your data.', 'query': 'How do I choose the right Sentence Transformer model from Hugging Face for Semantic Search?'}\n",
      "\u001b[96mQuery: How do I choose the right Sentence Transformer model from Hugging Face for Semantic Search?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "To choose the right Sentence Transformer model from Hugging Face for semantic search, consider the following factors:\n",
      "\n",
      "1. **Use Case Alignment**: Identify the specific domain and task you need the model for. Models differ mainly based on the data they were trained on, so finding a model that aligns closely with your data and task is crucial.\n",
      "\n",
      "2. **Pre-trained vs. Custom Models**: If your use case matches a commonly addressed problem, a pre-trained model might suffice. If your task is specialized or out-of-domain, consider fine-tuning a model on your own dataset.\n",
      "\n",
      "3. **Model Differences**: Understand that models vary due to different architecture choices, training data, and objectives. Consider these differences when selecting a model to ensure it fits your requirements.\n",
      "\n",
      "4. **Scale and Complexity**: Evaluate the scale of the dataset the models were trained on, and whether your application can handle the model's complexity and resource demands.\n",
      "\n",
      "5. **Experimentation**: Given the large number of available models, experimenting with a few top candidates can be practical. Use metrics like model performance, accuracy, and speed to narrow down the choices. \n",
      "\n",
      "6. **Use Resources for Guidance**: Refer to relevant resources, such as the Hugging Face documentation or blog posts, for guidance and insights on using Semantic Search with Sentence Transformers.\n",
      "\n",
      "For more details, you might find useful guidance in blog posts or articles discussing how to choose a sentence transformer model specific to semantic search.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "To choose the right Sentence Transformer model from Hugging Face for Semantic Search, you should consider the following factors:\n",
      "\n",
      "1. **Model Architecture**: Different architectures might be optimized for varying tasks. It's important to understand the basic structure of the model and how it affects performance.\n",
      "\n",
      "2. **Training Data**: Look at the data the model was trained on. Some models might be trained on general data, while others may have specialized datasets that match your use case.\n",
      "\n",
      "3. **Task Specialization**: Some models are designed for specific tasks such as semantic search, sentiment analysis, etc. Ensure the model’s task aligns with your needs.\n",
      "\n",
      "4. **Performance Metrics**: Check metrics like accuracy, precision, recall, and F1-score on relevant datasets to compare models.\n",
      "\n",
      "5. **Use Case**: Consider whether the model aligns with your particular semantic search use case. If your dataset is significantly different from the training data, you may need to fine-tune the model.\n",
      "\n",
      "6. **Availability on Hugging Face**: There are over 700 models on Hugging Face, allowing flexibility in choice for various applications.\n",
      "\n",
      "By focusing on these factors, you can choose a model that best fits your application and maximizes the performance of your semantic search tasks.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"vanilla rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 12 wins\n",
      "Agentic RAG: 32 wins\n",
      "After 44 rounds\n",
      "\n",
      "{'source': 'With easy access to data streams from across their entire business, Weaviate users can now:\\n\\n* **Create a real-time knowledge base:**\\nBuild a shared source of real-time truth for all your operational and analytical data, no matter where it lives for sophisticated model building and fine-tuning. Think business competitive analysis dashboards that are updated with latest market news updates. * **Bring real-time context at query time:**\\nConvert raw data into meaningful chunks with real-time enrichment and continually update your embedding databases for your GenAI use cases. Think real-time filtering based on region, demographics, personas in online shopping, etc. * **Build governed, secured, and trusted AI:**\\nEstablish data lineage, quality and traceability, providing all your teams with a clear understanding of data origin, movement, transformations and usage.', 'gold_answer': 'The new capabilities available to Weaviate users for managing their business data include creating a real-time knowledge base, bringing real-time context at query time, building governed, secured, and trusted AI, and a new backup feature that allows backing up and restoring data.', 'query': 'What are the new capabilities available to Weaviate users for managing their business data?'}\n",
      "\u001b[96mQuery: What are the new capabilities available to Weaviate users for managing their business data?\u001b[0m\n",
      "\u001b[96mVanilla RAG Response:\n",
      "\u001b[0m\n",
      "Weaviate has introduced several new capabilities for users managing their business data:\n",
      "\n",
      "1. **Integration with Confluent Cloud**: This integration allows Weaviate users to access data streams from across their entire business, enabling the creation of real-time, contextual, and trustworthy knowledge bases for AI applications. This provides a continuous supply of AI-ready data for developing sophisticated customer experiences.\n",
      "\n",
      "2. **Deployment Flexibility**: Users can deploy Weaviate in multiple ways to meet compliance and data privacy requirements. This includes self-hosting using Docker or Kubernetes, deploying into a Virtual Private Cloud (VPC) within AWS or GCP, or using Weaviate Cloud for reduced management overhead.\n",
      "\n",
      "3. **Advanced Search and Querying**: Weaviate has implemented a custom HNSW indexing algorithm, enabling highly performant querying over datasets. This facilitates various business use cases at scale.\n",
      "\n",
      "4. **Pay-as-you-grow Pricing Model**: Weaviate Cloud offers a new pricing model that charges on a per-dimension basis, making it more affordable for enterprises of any scale to leverage the vector database. Users only pay for what they use, benefiting from NoOps capabilities and dedicated customer support.\n",
      "\n",
      "These capabilities empower businesses to effectively manage their data, enhance their AI applications, and meet compliance needs while optimizing costs.\n",
      "\u001b[96mAgentic RAG Response:\n",
      "\u001b[0m\n",
      "Weaviate has introduced several new capabilities and integrations to help users manage their business data more effectively:\n",
      "\n",
      "1. **Integration with Confluent Cloud**: Weaviate users now have access to data streams from across their entire business, enabling the creation of real-time, contextual, and trustworthy knowledge bases to fuel AI applications. This integration bridges the legacy and modern data stack, providing a continuous supply of AI-ready data for the development of sophisticated customer experiences with constant awareness of what's happening in the business.\n",
      "\n",
      "2. **Enterprise Deployment Flexibility**: Weaviate provides flexible deployment options, allowing users to install Weaviate on dedicated compute resources on AWS, GCP, or Azure for scalable production use cases. There's also a Bring Your Own Cloud option, which allows businesses to use their own VPCs and private compute resources, offering support to ensure reliability and smooth operation.\n",
      "\n",
      "3. **Data Privacy and Compliance**: Weaviate supports a range of deployment models that cater to data privacy and compliance needs. It can be self-hosted, deployed in a user's own VPC via AWS or GCP, or hosted on Weaviate Cloud to reduce management overhead while maintaining compliance.\n",
      "\n",
      "4. **Enhanced Search and Performance**: Weaviate has implemented a custom HNSW indexing algorithm, enabling highly performant querying over datasets suitable for various business use cases. This enhances Weaviate's core competency in search.\n",
      "\n",
      "These capabilities make Weaviate a robust choice for enterprises looking to enhance their data management and AI application development.\n",
      "\u001b[96mJudged Winner to be:\u001b[0m\n",
      "{\"winner\":\"agentic rag\"}\n",
      "\u001b[96mThe current score is:\u001b[0m\n",
      "Vanilla RAG: 12 wins\n",
      "Agentic RAG: 33 wins\n",
      "After 45 rounds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load queries\n",
    "\n",
    "compare_system_responses = \"\"\"Assess the responses from two systems and determine which one had the better response:\n",
    "\n",
    "[[ answer from vanilla rag system ]]\n",
    "{vanilla_rag_response}\n",
    "\n",
    "[[ answer from agentic rag system ]]\n",
    "{agentic_rag_response}\n",
    "\n",
    "[[ winning system ]]\n",
    "\"\"\"\n",
    "\n",
    "vanilla_rag_scores, agentic_rag_scores = [], []\n",
    "vanilla_rag_wins = 0\n",
    "agentic_rag_wins = 0\n",
    "\n",
    "for row in ds:\n",
    "    query = row[\"query\"]\n",
    "    print(f\"\\033[96mQuery: {query}\\033[0m\")\n",
    "    \n",
    "    vanilla_rag_response = vanilla_rag(\n",
    "        search_query=query,\n",
    "        lm_service=lm_service\n",
    "    )\n",
    "    print(\"\\033[96mVanilla RAG Response:\\n\\033[0m\")\n",
    "    print(vanilla_rag_response)\n",
    "    \n",
    "    agentic_rag_response = lm_service.generate_with_function_calling_loop(\n",
    "        prompt=query,\n",
    "        tools=tools,\n",
    "        tools_mapping=tools_mapping\n",
    "    )\n",
    "\n",
    "    print(\"\\033[96mAgentic RAG Response:\\n\\033[0m\")\n",
    "    print(agentic_rag_response)\n",
    "    \n",
    "    formatted_compare_system_responses = compare_system_responses.format(\n",
    "        vanilla_rag_response=vanilla_rag_response,\n",
    "        agentic_rag_response=agentic_rag_response\n",
    "    )\n",
    "    \n",
    "    winner = lm_service.generate_with_output_model(\n",
    "        prompt=formatted_compare_system_responses,\n",
    "        output_model=Winner\n",
    "    )\n",
    "    print(\"\\033[96mJudged Winner to be:\\033[0m\")\n",
    "    print(winner)\n",
    "\n",
    "    winner = json.loads(winner)[\"winner\"]\n",
    "    \n",
    "    if winner == \"vanilla rag\":\n",
    "        vanilla_rag_wins += 1\n",
    "    else:\n",
    "        agentic_rag_wins += 1\n",
    "        \n",
    "    print(\"\\033[96mThe current score is:\\033[0m\")\n",
    "    print(f\"Vanilla RAG: {vanilla_rag_wins} wins\")\n",
    "    print(f\"Agentic RAG: {agentic_rag_wins} wins\")\n",
    "    print(f\"After {len(vanilla_rag_scores) + 1} rounds\\n\")\n",
    "\n",
    "    # Save results\n",
    "    vanilla_rag_scores.append(RAGEvalModel(\n",
    "        query=query,\n",
    "        response=vanilla_rag_response,\n",
    "        win=(winner == \"vanilla rag\")\n",
    "    ))\n",
    "    \n",
    "    agentic_rag_scores.append(RAGEvalModel(\n",
    "        query=query,\n",
    "        response=agentic_rag_response, \n",
    "        win=(winner == \"agentic rag\")\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Win Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agentic RAG win rate: 73.33%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Agentic RAG win rate: {agentic_rag_wins / 45 * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
