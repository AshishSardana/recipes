{"relevant_passages": ["That [distance can be calculated in multiple ways](/blog/distance-metrics-in-vector-search), one of the simplest being \"The sum of the absolute differences between elements at position `i` in each vector\" (recall that all vectors have the same fixed length). Let's look at some numbers (no math, promise!) and illustrate with another text example:\n\nObjects (data): words including `cat`, `dog`, `apple`, `strawberry`, `building`, `car`\n\nSearch query: `fruit`\n\nA set of simplistic vector embeddings (with only 5 dimensions) for the objects and the query could look something like this:\n\n| Word               | Vector embedding                |\n|--------------------|---------------------------------|\n| `cat`              | `[1.5, -0.4, 7.2, 19.6, 20.2]`  |\n| `dog`              | `[1.7, -0.3, 6.9, 19.1, 21.1]`  |\n| `apple`            | `[-5.2, 3.1, 0.2, 8.1, 3.5]`    |\n| `strawberry`       | `[-4.9, 3.6, 0.9, 7.8, 3.6]`    |\n| `building`         | `[60.1, -60.3, 10, -12.3, 9.2]` |\n| `car`              | `[81.6, -72.1, 16, -20.2, 102]` |\n| **Query: `fruit`** | `[-5.1, 2.9, 0.8, 7.9, 3.1]`    |\n\nIf we look at each of the 5 elements of the vectors, we can see quickly that `cat` and `dog` are much closer than `dog` and `apple` (we don\u2019t even need to calculate the distances). In the same way, `fruit` is much closer to `apple` and `strawberry` than to the other words, so those will be the top results of the \u201cfruit\u201d query. But where do these numbers come from? That\u2019s where the real magic is, and where advances in modern deep learning have made a huge impact."], "query": "Which words are most similar to the search query 'fruit' based on their vector embeddings??"}
{"relevant_passages": ["## Overview\n![Overview](./img/hugging-face-module-overview.png)\n\nThe Hugging Face module is quite incredible, for many reasons. ### Public models\nYou get access to over 1600 pre-trained [sentence similarity models](https://huggingface.co/models?pipeline_tag=sentence-similarity). No need to train your own models, if there is already one that works well for your use case. In case you struggle with picking the right model, see our blog post on [choosing a sentence transformer from Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). ### Private models\nIf you have your own models, trained specially for your data, then you can upload them to Hugging Face (as private modules), and use them in Weaviate."], "query": "How many pre-trained sentence similarity models does Hugging Face offer??"}
{"relevant_passages": ["Sometimes when multiple batches contained identical objects with the same UUID, they could be added more than once to Weaviate, each time with different DocIDs. This, in turn, could cause issues within Weaviate. Luckily, we've addressed this issue without sacrificing performance (yay!\ud83e\udd73). Here's our journey that got us to the current solution. ## Our initial solutions\nIn the initial solution, we added a lock (sync.Mutex in Go), so that now only a single goroutine can hold the lock, check for duplicate UUIDs, and assign DocIDs. This lock makes sure that the race does not occur anymore, but as an unintended side-effect the import time increased by ~20% due to lock-congestion. Upon further consideration, our team concluded that while using a single lock is effective, it's also overkill."], "query": "What was the initial solution to prevent identical objects with the same UUID from being added multiple times to Weaviate, and what side-effect did it have??"}
{"relevant_passages": ["*Note. Hugging Face Inference uses [a pay-per-use pricing model](https://huggingface.co/inference-api#pricing).<br />\nMake sure to study it well before you run a big job.*\n\nTo learn more, head to the [HuggingFace Module docs page](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-huggingface). ## Other improvements and bug fixes\n\n![Other improvements and bug fixes](./img/smaller-improvements.jpg)\n\nAnd, of course, there are many other improvements and bug fixes that went into this release. You can find the complete list and the relevant links in the [release notes](https://github.com/weaviate/weaviate/releases/tag/v1.15.0). ## Enjoy\nWe hope you enjoy all the new features, performance improvements, memory savings and bug fixes that made this the best Weaviate release yet!\ud83d\udd25\n\nimport ShareFeedback from '/_includes/share-feedback.md';\n\n<ShareFeedback />"], "query": "Where can I find the pricing model for Hugging Face Inference API??"}
{"relevant_passages": ["---\ntitle: Weaviate 1.18 release\nslug: weaviate-1-18-release\nauthors: [jp, erika, zain, dan]\ndate: 2023-03-07\ntags: ['release']\nimage: ./img/hero.png\ndescription: \"Weaviate 1.18 introduces Faster Filtering through Bitmap Indexing, HNSW-PQ, Cursor API, and more! Learn all about it.\"\n---\n\nimport Core118 from './_core-1-18-include.mdx' ;\n\n<Core118 />\n\nimport WhatsNext from '/_includes/what-next.mdx'\n\n<WhatsNext />\n\nimport Ending from '/_includes/blog-end-oss-comment.md' ;\n\n<Ending />"], "query": "What features were introduced in the Weaviate 1.18 release on March 7, 2023?"}
{"relevant_passages": ["Just add one of the following properties to the `POST payload`:\n* `include` - an array class names we want to backup or restore\n* `exclude` - an array class names we don't want to backup or restore\n\nFor example, you can create a backup that includes Cats, Dogs and Meerkats. ```js\nPOST /v1/backups/gcs\n{\n  \"id\": \"first_backup\",\n  \"include\": [\"Cats\", \"Dogs\", \"Meerkats\"]\n}\n```\n\nThen restore all classes, excluding Cats:\n\n```js\nPOST /v1/backups/gcs/first_backup/restore\n{\n  \"exclude\": [\"Cats\"]\n}\n```\n\n### Other use cases\nIt might not be immediately obvious, but you can use the above workflow to migrate your data to other environments. So, if one day you find yourself with an environment that is not set up for what you need (i.e. not enough resources). Then create a backup, and restore it in the new environment. \ud83d\ude09\n\n### Follow up\nAre you ready to set up backups for your environment?"], "query": "How can I create a backup of specific classes using the POST payload in my API request??"}
{"relevant_passages": ["using a special algorithm, the database find the [closest](/blog/distance-metrics-in-vector-search) vectors to the given vector computed for the query. The quality of the search depends crucially on the quality of the model - this is the \"secret sauce\", as many models are [still closed source](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither). The speed of the search depends crucially on Weaviate, which is open-source and [continuously improving its performance](/blog/weaviate-1-18-release). ## What exactly are vector embeddings? Vectors are numeric representations of data that capture certain features of the data."], "query": "What factors are crucial for the quality and speed of vector searches in databases??"}
{"relevant_passages": ["Thanks to the advances in machine learning in the past decade and the commoditization of AI-first database technologies, you can start using it in your business tomorrow. import WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What recent developments in AI technology can be applied to businesses today??"}
{"relevant_passages": ["If multiple recoveries were required, they would happen in sequence. On a large machine, this could mean that startup would be slow, yet only one of many CPU cores was utilized. - The data structure used to hold the recovered items was never intended to hold many items. Each additional insertion would degrade its performance. As a result, the larger the WAL to recover, the slower the recovery would become."], "query": "Why does the recovery process from a large WAL lead to slow startup times on a machine with multiple CPU cores?"}
{"relevant_passages": ["### Indexing Knobs\nFor the sake of designing RAG systems, the most important indexing knob looks like vector compression settings. Launched in March 2023, Weaviate 1.18 introduced Product Quantization (PQ). PQ is a vector compression algorithm that groups contiguous segments of a vector, clusters their values across the collection, and then reduces the precision with centroids. For example, a contiguous segment of 4 32-bit floats requires 16 bytes to represent, a segment length of 4 with 8 centroids results in only needing 1 byte, a 16:1 memory reduction. Recent advances in PQ Rescoring help significantly with recall loss from this compression, but is still an important consideration with very high levels of compression."], "query": "What vector compression algorithm was introduced in Weaviate 1.18 to reduce memory usage for RAG systems, and what is its impact on recall?"}
{"relevant_passages": ["[Cross Encoders](#cross-encoders) (collapsing the use of Large Language Models for ranking into this category as well)\n1. [Metadata Rankers](#metadata-rankers)\n1. [Score Rankers](#score-rankers)\n\n## Cross Encoders\nCross Encoders are one of the most well known ranking models for content-based re-ranking. There is quite a collection of pre-trained cross encoders available on [sentence transformers](https://www.sbert.net/docs/pretrained_cross-encoders.html). We are currently envisioning interfacing cross encoders with Weaviate using the following syntax."], "query": "What are Cross Encoders, and how can they be interfaced with Weaviate??"}
{"relevant_passages": ["This process is known as \u201cde-noising\u201d, is illustrated below and, is carried out for each image in the training set with multiple levels of random noise added. Once the diffusion model is trained in this way it becomes an expert at taking images that are less likely to be seen in the dataset (noisy images) and incrementally turning them into something that is more likely to be seen in the training set. By teaching the model to \u201cde-noise\u201d images we have developed a way to alter images to make them more like images from the training set. ![denoising gif](./img/denoise.gif)\n*[Source](https://yang-song.net/blog/2021/score/)*\n\n![denoising images](./img/denoisingimage.png)\n*[Source](https://huggingface.co/blog/annotated-diffusion)*\n\nNow if we take this trained diffusion model and just give it a random static image and run the de-noising process it will transform the static image into an image that resembles images in the training set!\n\n![noising denoising images](./img/noising_denoising.png)\n\n## How Text Prompts Control the Image Generation Process\n\nSo far we have explained the general idea behind how diffusion models can start off from static noise and incrementally alter the pixel values so that the picture all together gains meaning and follows the distribution of the training set. However, another important detail is that most diffusion models don\u2019t just spit out random images that look like training set images, they allow us to add a text prompt that can control the specific types of images are generated."], "query": "What is the purpose of the \"de-noising\" process in diffusion models, and how does it relate to generating images from text prompts??"}
{"relevant_passages": ["The following image presents a high-level illustration of chunking text. ![chunking](img/chunk.png)\n\n### Retrieval \nThere are four major knobs to tune in Retrieval: Embedding models, Hybrid search weighting, whether to use AutoCut, and Re-ranker models. Most RAG developers may instantly jump to tuning the embedding model used, such as OpenAI, Cohere, Voyager, Jina AI, Sentence Transformers, and many others! Developers also need to consider the dimensionality of the models and how it affects the PQ compression. The next key decision is how to weight the aggregation of sparse and dense retrieval methods in Hybrid Search. The weighting is based on the `alpha` parameter."], "query": "What are the four major knobs to tune in Retrieval for RAG development?"}
{"relevant_passages": ["If you would like to follow along, the Jupyter notebook and data are available [here](https://github.com/weaviate/weaviate-examples/tree/main/text2vec-behind-curtain). You can use our free [Weaviate Cloud Services](https://console.weaviate.cloud) (WCS) sandbox, or set up your own Weaviate instance also. > Note: The vectorization is done by [Weaviate \u201ccore\u201d](https://github.com/weaviate/weaviate) and not at the client level. So even though we use Python examples, the principles are universally applicable. Let's get started!\n\n## Text2vec: behind the scenes\n\nWeaviate's `text2vec-*` modules transform text data into dense vectors for populating a Weaviate database."], "query": "Where can I find the Jupyter notebook and data to learn about Weaviate's text vectorization, and where is the vectorization process executed??"}
{"relevant_passages": ["This includes [improved APIs](https://github.com/weaviate/weaviate-python-client/issues/205) on the client side, new modules, for example, for [generative search](/developers/weaviate/modules/reader-generator-modules/generative-openai), and improvements to our existing modules. <br></br>\n\n### Community\n![community](./img/community.png)\n\nThe most important pillar is all of you \u2013 our community. This includes both free, open-source users that self-host their Weaviate setup, as well as paid enterprise users and anyone using our Weaviate-as-a-Service offerings. We value your feedback and love that you are part of shaping our future. Last year we introduced our [dynamic roadmap page](/developers/weaviate/roadmap) that allows you to create and upvote your favorite feature requests."], "query": "What new features were added to the Weaviate Python client as mentioned in the document??"}
{"relevant_passages": ["## Exploring the Power of Vector Databases\n\nThe year 2023 was all about dynamic experimentation at Weaviate. Vector databases became a strong and recognized foundation in building ever more effective AI applications, enabling **chatbots,** **agents,** and **advanced** **search systems**. ### Online Hackathons\n\nOur 2023 global online hackathons proved to be vibrant innovation hubs, fostering diversity and inclusion in collaborative work. We teamed up with friends from [Cohere](https://cohere.com/), [LangChain](https://www.langchain.com/), [AutoGPT](https://autogpt.net/), [lablab.ai](https://lablab.ai/), [SuperAGI](https://superagi.com/), and many others. ![hackathons](img/hackathons.png)\n\n### In-person Hackathons\nWhether you're a beginner just diving into the world of coding, a passionate AI enthusiast, or a seasoned expert in the field, in-person events create a burst of energy and creativity into everyone's personal AI journey."], "query": "What advancements did Weaviate make in the field of AI applications in 2023??"}
{"relevant_passages": ["---\ntitle: Why is Vector Search so fast? slug: why-is-vector-search-so-fast\nauthors: [laura]\ndate: 2022-09-13\ntags: ['search']\nimage: ./img/hero.png\ndescription: \"Vector Databases can run semantic queries on multi-million datasets in milliseconds. How is that possible?\"\n---\n![Why is Vector Search so fast?](./img/hero.png)\n\n<!-- truncate -->\n\n## Why is this so incredibly fast? Whenever I talk about vector search, I like to demonstrate it with an example of a semantic search. To add the wow factor, I like to run my queries on a Wikipedia dataset, which is populated with over 28 million paragraphs sourced from Wikipedia."], "query": "How can vector databases perform semantic searches on datasets as large as Wikipedia's 28 million paragraphs so quickly??"}
{"relevant_passages": ["We were able to import 200 million objects and more, while the import performance remained constant throughout the process. [See more on github](https://github.com/weaviate/weaviate/pull/1976). ### Drastically improved Mean-Time-To-Recovery (MTTR)\nWeaviate `1.14` fixes an issue where a crash-recovery could take multiple minutes, or even hours in some extreme cases. It is now a matter of just seconds. So even in the rare event that your instance crashes, it will come back up very quickly."], "query": "What improvements were made to Weaviate's Mean-Time-To-Recovery in version 1.14?"}
{"relevant_passages": ["Taken directly from the paper, \u201cOur findings indicate that cross-encoder re-rankers can efficiently be improved without additional computational burden and extra steps in the pipeline by explicitly adding the output of the first-stage ranker to the model input, and this effect is robust for different models and query types\u201d. Taking this a bit further, [Dinh et al.](https://arxiv.org/abs/2206.06565) shows that most tabular machine learning tasks can be translated to text and benefit from transfer learning of text-based models. Many of these metadata rankers may also take in something like a collaborative filtering score that is based on this user\u2019s history, as well as other users on the platform \u2014 another interesting feature to think of interfacing this way. The main point being, maybe we can just add these meta features to our [query, document] representation and keep the Zero-Shot party going. We recently had an interesting discussion about metadata ranking and future directions for ranking models broadly on our latest Weaviate podcast! \ud83d\udc49 Check it out [here](https://www.youtube.com/watch?v=aLY0q6V01G4)\n\n## Score Rankers\nScore rankers describe using either a classifier to detect things, or a regression model to score things, about our candidate documents to rank with."], "query": "How can cross-encoder re-rankers be improved without additional computational costs according to recent research findings??"}
{"relevant_passages": ["When compressing a vector we find the closest centroid per segment and return an array of bytes with the indexes of the centroids per segment. When decompressing a vector, we concatenate the centroids encoded by each byte on the array. The explanation above is a simplification of the complete algorithm, as we also need to be concerned about performance for which we need to address duplication of calculations, synchronization in multithreading and so on. However what we have covered above should be sufficient to understand what you can accomplish using the PQ feature released in v1.18. Let's see some of the results HNSW implemented with PQ in Weaviate can accomplish next! If you're interested in learning more about PQ you can refer to the documentation [here](/developers/weaviate/concepts/vector-index#hnsw-with-product-quantizationpq)."], "query": "What is the PQ feature in Weaviate v1.18 used for in vector compression??"}
{"relevant_passages": ["Take the longer start-up time for example. Adding replication caused node-level start-up time to increase in our experiment. But the end result was that a hundred percent of requests succeeded. In other words, the end user would not have noticed anything was going on. And again, what are peace of mind and avoiding the wrath of angry users during downtime worth to you??"], "query": "What was the impact on node-level start-up time and request success rate after adding replication in the experiment??"}
{"relevant_passages": ["Due to its relatively high memory footprint, HNSW is only cost-efficient in high-throughput scenarios. However, HNSW is inherently optimized for in-memory access. Simply storing the index or vectors on disk or memory-mapping the index kills performance. This is why we will offer you not just one but two memory-saving options to index your vectors without sacrificing latency and throughput. In early 2023, you will be able to use Product Quantization, a vector compression algorithm, in Weaviate for the first time."], "query": "When is Product Quantization be available in Weaviate for vector compression?"}
{"relevant_passages": ["Now armed with queries, answers, gold documents, and negatives, ARES fine-tunes lightweight classifiers for **context relevance**, **answer faithfulness**, and **answer relevance**. The authors experiment with fine-tuning [DeBERTa-v3-large](https://huggingface.co/microsoft/deberta-v3-large), which contains a more economical 437 million parameters, with each classifier head sharing the base language model, adding 3 total classification heads. The ARES system is then evaluated by dividing the synthetic data into a train-test split and comparing the fine-tuned judges with zero-shot and few-shot GPT-3.5-turbo-16k judges, finding that the fine-tuned models perform significantly better. For further details, such as a novel use of confidence intervals with prediction powered inference (PPI) and more experimental details, please see the paper from [Saad-Falcon et al](https://arxiv.org/abs/2311.09476). To better understand the potential impact of LLMs for evaluation, we will continue with a tour of the existing methods for benchmarking RAG systems and how they are particularly changed with LLM Evaluation."], "query": "What system uses DeBERTa-v3-large to fine-tune classifiers for context relevance, answer faithfulness, and answer relevance, and is compared with GPT-3.5-turbo-16k judges?"}
{"relevant_passages": ["### Stuffing\n\n<img\n    src={require('./img/stuffing.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nStuffing takes the related documents from the database and stuffs them into the prompt. The documents are passed in as context and go into the language model (the robot). This is the simplest method since it doesn\u2019t require multiple calls to the LLM. This can be seen as a disadvantage if the documents are too long and surpass the context length. ### Map Reduce\n\n<img\n    src={require('./img/map-reduce.gif').default}\n    alt=\"alt\"\n    style={{ width: \"100%\" }}\n/>\n\nMap Reduce applies an initial prompt to each chunk of data."], "query": "What is the method called that involves inserting related documents directly into the prompt for a language model, and what is its main limitation??"}
{"relevant_passages": ["The problem is that moving vectors to disk would have higher latency costs since we would then need lots of disk reads. The proposed solution by [DiskANN](https://suhasjs.github.io/files/diskann_neurips19.pdf) is to store large complete representations of vectors on disk and keep a compressed representation of them in memory. The compressed representation is used to sort the vectors while searching for the nearest neighbors, and the complete representation is fetched from disk every time we need to explore a new vector from the sorted list. In plain English, we start our search from our root in the graph. From there, we get a set of neighbor candidates."], "query": "What technique does DiskANN use to reduce latency when searching for nearest neighbors with vectors stored on disk??"}
{"relevant_passages": ["Your answer to the question must be grounded in the provided search results and nothing else!!\u201d. As described earlier, Few-Shot Examples describes collecting a few manually written examples of question, context, answer pairs to guide the language model\u2019s generation. Recent research such as [\u201cIn-Context Vectors\u201d](https://arxiv.org/abs/2311.06668) are further pointing to the importance of guiding latent space like this. We were using GPT-3.5-turbo to generate Weaviate queries in the Weaviate Gorilla project and performance skyrocketed once we added few-shot examples of natural language to query translations. Lastly, there is increasing interest in fine-tuning LLMs for RAG applications."], "query": "What impact did few-shot examples have on the performance of GPT-3.5-turbo in the Weaviate Gorilla project?"}
{"relevant_passages": ["---\ntitle: Authentication in Weaviate (videos)\nslug: authentication-in-weaviate\nauthors: [jp]\ndate: 2023-04-25\nimage: ./img/hero.png\ntags: ['concepts']\ndescription: \"Videos on authentication: an overview, how to log in, how to set it up, and core concepts - including recommendations.\"\n\n---\n\n![Authentication in Weaviate](./img/hero.png)\n\n<!-- truncate -->\n\nimport ReactPlayer from 'react-player/lazy'\n\n## Overview\n\nAuthentication is one of those topics that we get quite a few questions about. And we can see why. It's a big, complex topic, and even within Weaviate, there are many options available which can make it seem quite confusing. The core concept of authentication is relatively simple. When a client (e.g. a Weaviate client) sends a request to a server (e.g. a Weaviate database), it includes a \"secret\" that provides some assurances to Weaviate as to who that request is coming from, so that it can operate on that information."], "query": "What are the core concepts of authentication in Weaviate, and where can I find videos explaining ways to set it up and log in?"}
{"relevant_passages": ["An `alpha` of 0 is pure bm25 and an alpha of 1 is pure vector search. Therefore, the set `alpha` is dependent on your data and application. Another emerging development is the effectiveness of zero-shot re-ranking models. Weaviate currently offers 2 [re-ranking models from Cohere](https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/reranker-cohere): `rerank-english-v2.0` and `rerank-multilingual-v2.0`. As evidenced from the name, these models mostly differ because of the training data used and the resulting multilingual capabilities."], "query": "What are the two re-ranking models from Cohere that Weaviate offers for search customization?"}
{"relevant_passages": ["So let's look behind the curtain, and see if we can reproduce the magic. More specifically, let's try to reproduce Weaviate's output vector for each object by using an external API. ![pulling back the curtains](./img/pulling-back-the-curtains-text2vec.png)\n\n### Matching Weaviate's vectorization\n\nWe know that the vector for each object corresponds to its text. What we don't know is how, exactly. As each object only contains the two properties, `question` and `answer`, let's try concatenating our text and comparing it to Weaviate's."], "query": "How can I reproduce Weaviate's output vector for an object using an external API??"}
{"relevant_passages": ["If we compress the vectors then the memory requirements goes down to the 4730 MB to 12367 MB range. After compression, recall drops to values ranging from 0.8566 to 0.9702. Latency rises up to the 1039 to 2708 microsends range. For Gist we would require roughly 4218 MB to 5103 MB of memory to index our data using uncompressed HNSW. This version would give us recall ranging from 0.7446 to 0.9962 and latencies ranging from 2133 to 15539 microseconds."], "query": "What is the range of memory requirements for compressed vectors according to the document?"}
{"relevant_passages": ["6**: *Time (min) to fit the Product Quantizer with 200,000 vectors and to encode 9,990,000 vectors, all compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 6 dimensions per segment. The points in the curve are obtained varying the amount of centroids.*\n\n![res4](./img/image8.png)\n**Fig. 7**: *Average time (microseconds) to calculate distances from query vectors to all 9,990,000 vectors compared to the recall achieved. The different curves are obtained varying the segment length (shown in the legend) from 1 to 6 dimensions per segment."], "query": "How does varying the segment length affect the time efficiency and recall when using a Product Quantizer with 200,000 vectors for fitting and 9,990,000 vectors for encoding??"}
{"relevant_passages": ["### The issue\nFast forward, we identified two key issues. First, the Go library that we used for reading binary data (`binary.read`) isn't optimized for how we use it in Weaviate, as it makes many temporary memory allocations. Second, for every object in the aggregation, we would allocate new memory on the heap, process the read, and release the memory. <!-- TODO: add a picture with cakes -->\nThis is a bit like, if we want to eat a cake, we need to put it on a plate, eat the cake and then put the plate in the sink to wash. Now, if we want to eat a million cakes, we will be either very busy washing dishes or have a million plates in the sink (or even run out of plates).<br/>\nI am sure you would rather spend more time eating cakes than dealing with plates."], "query": "What are the two key issues with the Go library and memory allocation in Weaviate as described in the document??"}
{"relevant_passages": ["Finally, we can jump on a bike to reach our local destination. For a better understanding, consider the below graphic, which shows a graph with all the connections generated using 1000 objects in two dimensions. <img\n    src={require('./img/vamana-graph.png').default}\n    alt=\"Vamana graph with 1000 objects\"\n    style={{ maxWidth: \"50%\" }}\n/>\n\nIf we iterate over it in steps \u2013 we can analyze how Vamana navigates through the graph. <img\n    src={require('./img/vamana-graph-animated.gif').default}\n    alt=\"Vamana graph - animated in 3/6/9 steps\"\n    style={{ maxWidth: \"50%\" }}\n/>\n\nIn the **first step**, you can see that the entry point for the search is in the center, and then the long-range connections allow jumping to the edges. This means that when a query comes, it will quickly move in the appropriate direction.<br/>\nThe **second**, **third**, and **final steps** highlight the nodes reachable within **three**, **six**, and **nine** hops from the entry node."], "query": "How does the Vamana algorithm navigate through a graph with 1000 objects in steps??"}
{"relevant_passages": [":::info Glossary\n- **Node**: A single machine in a cluster. It often refers to a physical or virtual machine that runs part of an application or service. - **Pod**: A Kubernetes term for a group of one or more containers, with shared storage/network, and a specification for how to run the containers. Pods are the smallest deployable units in Kubernetes. - **Tenant**: In the context of Weaviate, an isolated environment or subsection within the system, designed to separate data and access between different end users or groups."], "query": "What is the definition of a \"Pod\" in Kubernetes??"}
{"relevant_passages": ["Some models, such as [CLIP](https://openai.com/blog/clip/), are capable of vectorizing multiple data types (images and text in this case) into one vector space, so that an image can be searched by its content using only text. ## Vector embeddings with Weaviate\n\nFor this reason, Weaviate is configured to support many different vectorizer models and vectorizer service providers. You can even [bring your own vectors](/developers/weaviate/starter-guides/custom-vectors), for example if you already have a vectorization pipeline available, or if none of the publicly available models are suitable for you. For one, Weaviate supports using any Hugging Face models through our `text2vec-hugginface` module, so that you can [choose one of the many sentence transformers published on Hugging Face](/blog/how-to-choose-a-sentence-transformer-from-hugging-face). Or, you can use other very popular vectorization APIs such as OpenAI or Cohere through the [`text2vec-openai`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-openai) or [`text2vec-cohere`](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-cohere) modules."], "query": "What vectorization models and service providers does Weaviate support, and can it integrate with Hugging Face, OpenAI, or Cohere??"}
{"relevant_passages": ["The company preferred an open source platform, and since they would be indexing millions of products, they needed a solution that was both high-performing and cost-efficient at scale. ## Selecting Weaviate as the vector database of choice \nAfter a thorough evaluation of a handful of open and closed-source vector databases, the team decided that Weaviate was the best-fit solution for their needs. They cited the following reasons for choosing Weaviate:  \n\n* Open source, with an active community and managed cloud offering. * Comprehensive documentation and strong support for popular LLMs and multi-modal models. * Direct integration of machine learning models using a module system, with the ability to easily swap out and experiment with different models."], "query": "Why did the company choose Weaviate as their vector database solution?"}
{"relevant_passages": ["As we have mentioned before, this is not the final solution where we still do not move information to disk. Our final solution would use a compressed version of the vectors to guide the exploration and fetch the fully described data (vectors and graph) from disk as needed. With the final approach we will achieve better compression rate but also better recall since the uncompressed vectors will be used to correct the compression distortion. A final remark. Notice how the compression rate is better on Gist."], "query": "What is the proposed final solution for handling data that involves compression and disk storage as mentioned in the document??"}
{"relevant_passages": ["The schema is the place to define, among other things, the data type and vectorizer to be used, as well as cross-references between classes. As a corollary, the vectorization process can be modified for each class by setting the relevant schema options. In fact, you can [define the data schema](/developers/weaviate/manage-data/collections) for each class individually. All this means that you can also use the schema to tweak Weaviate's vectorization behavior. The relevant variables for vectorization are `dataType` and those listed under `moduleConfig` at both the class level and property level."], "query": "How can I customize the vectorization process for a class in Weaviate's schema??"}
{"relevant_passages": ["Later on you will see an example of how the partitioning bricks identified the elements in a research paper. Cleaning the data is an important step before passing it to an NLP model. The cleaning brick can \u2018sanitize\u2019 your text data by removing bullet points, extra whitespaces, and more. Staging is the last brick and it helps to prepare your data as input into downstream systems. It takes a list of document elements as input and returns a formatted dictionary as output."], "query": "What are the functions of partitioning bricks, cleaning brick, and staging in the context of preparing data for an NLP model??"}
{"relevant_passages": ["The idea behind conditioning the images on a text prompt requires another model, one that is trained on images along with their captions. Examples of this data are shown below:\n\n![MS COCO Image Caption Dataset](./img/mscoco.png)\n\nThis model learns to relate descriptions of an image in text format with the image representation itself. In doing so it gives us a way to represent our written prompts as vectors that also capture the visual meaning behind the prompt. We can then pass these prompt vectors into our diffusion model along with the noised images during the training process. This allows us to tame the image generation process of the diffusion model by specifying to the model what types of images in the training set to resemble as it alters the pixels step by step."], "query": "How are text prompts used to guide image generation in a diffusion model??"}
{"relevant_passages": ["Let us know in the comments below!\n\n\nimport WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "What component is included for user interaction in the comments section of a web page??"}
{"relevant_passages": ["I recommend checking out the GitHub repository to test this out yourself!\n\n## Additional Resources\n\u2022 [LangChain Guide](https://www.commandbar.com/blog/langchain-projects) by Paul from CommandBar. import StayConnected from '/_includes/stay-connected.mdx'\n\n<StayConnected />"], "query": "Where can I find the LangChain guide by Paul from CommandBar??"}
{"relevant_passages": ["## Q&A style questions on your own dataset answered in milliseconds\nWeaviate now allows you to get to sub-50ms results by using transformers on your own data. You can learn more about Weaviate\u2019s speed in combination with transformers in [this article](https://towardsdatascience.com/a-sub-50ms-neural-search-with-distilbert-and-weaviate-4857ae390154). import WhatNext from '/_includes/what-next.mdx'\n\n<WhatNext />"], "query": "How can Weaviate achieve sub-50ms search results with transformers??"}
{"relevant_passages": ["This means that holding the vectors in memory requires 1,000,000 x 128 x 4 bytes = 512,000,000 bytes. Additionally, a graph representation of neighborhoods is built when indexing. The graph represents the k-nearest neighbors for each vector. To identify each neighbor we use an `int64`, meaning we need 8 bytes to store each of the k-nearest neighbors per vector. The parameter controlling the size of the graph is `maxConnections`."], "query": "How much memory is required to store the k-nearest neighbors for each vector if each neighbor is identified by an int64??"}
{"relevant_passages": ["For example, I can query Weaviate for articles related to: \"urban planning in Europe\", and the vector database (in the case of my demo \u2013 [Weaviate](/developers/weaviate/)) responds with a series of articles about the topic, such as \"The cities designed to be capitals\".<br/>\nYou can try it for yourself by following this [link](https://link.weaviate.io/3LiVxqp), which is already pre-populated with the above question. Press the play button, to see the magic happen. Here is the thing, finding the correct answer in a gigantic repository of unstructured data is not the most impressive part of this demonstration (I mean, it is very impressive), but it is the \ud83d\ude80 speed at which it all happens. It takes a fraction of a second for the UI to show the results. We are talking about a semantic search query, which **takes milliseconds** to find an answer in a dataset containing **28 million paragraphs**."], "query": "How fast can Weaviate perform a semantic search on a dataset of 28 million paragraphs for articles about urban planning in Europe??"}
{"relevant_passages": ["### Improved performance for large scale data imports\nWeaviate `1.14` significantly improves the performance of data imports for large datasets. Note that the performance improvements should be noticeable for imports of over 10 million objects. Furthermore, this update enables you to import over 200 million objects into the database. #### Problem\nBefore, the HNSW index would grow in constant intervals of 25,000 objects. This was fine for datasets under 25 million objects."], "query": "What improvements does Weaviate version 1.14 offer for large scale data imports?"}
{"relevant_passages": ["At Weaviate, we pride ourselves on our research acumen and on providing state-of-the-art solutions. So we took time to explore these solutions to identify and evaluate the right building blocks for Weaviate's future. Here we share some of our findings from this research. ## On the HNSW vs. Vamana comparison\nAs the first step to disk-based vector indexing, we decided to explore Vamana \u2013 the algorithm behind the DiskANN solution."], "query": "Which algorithm did Weaviate explore as a first step towards disk-based vector indexing??"}
{"relevant_passages": ["Accuracy is measured based on Recall. Recall in vector indexing measures how many of the ground truth nearest neighbors determined by brute force are returned from the approximate indexing algorithm. This is distinct from how \u201cRecall\u201d is typically used in Information Retrieval to reference how many of the relevant documents are returned from the search. Both are typically measured with an associated @K parameter. The interesting question in the full context of a RAG stack is then: **When do ANN accuracy errors manifest in IR errors?** For example, we may be able to get 1,000 QPS at 80% recall versus 500 QPS at 95% recall, what is the impact of this on the search metrics presented above such as Search nDCG or an LLM Recall score?"], "query": "What is the impact of different ANN recall rates on search metrics in a RAG stack?"}
{"relevant_passages": ["<figure>\n\n![vectorspace](./img/vectorspace.png)\n<figcaption>Figure 1. Shows a joint embedding space of a multimodal model that understands both text and images. Notice how objects that are similar are closer together and dissimilar objects are farther apart, this means that the model preserves semantic similarity within and across modalities.</figcaption>\n\n</figure>\n\n## Addressing Challenges of Learning Multimodal Embeddings\n\n### 1. Lack of Rich & Aligned Multimodal Datasets\n\nCollecting and preparing multimodal datasets is very challenging. Each modality requires specific data collection techniques and preprocessing steps."], "query": "What is a challenge in collecting and preparing multimodal datasets for learning embeddings?"}
{"relevant_passages": ["### Generation\nWhen it comes to Generation, the obvious first place to look is the choice of LLM. For example, you have options from OpenAI, Cohere, Facebook, and many open-source options. It is also helpful that many LLM frameworks like [LangChain](https://www.langchain.com/) and [LlamaIndex](https://www.llamaindex.ai/), and [Weaviate\u2019s generate module](/developers/weaviate/modules/reader-generator-modules/generative-openai) offer easy integrations into various models. The model that you choose can be dependent on whether you want to keep your data private, the cost, resources, and more. A common LLM specific knob that you can tune is the temperature."], "query": "What are some frameworks that provide easy integration with various language learning models?"}
{"relevant_passages": ["If we compress the vectors then the memory requirements goes down to 610 MB to 1478 MB range. After compression recall, drops to values ranging from 0.9136 to 0.9965. Latency rises up to the 401 to 1937 microsends range. For DeepImage96 we would require roughly 9420 MB to 15226 MB of memory to index our data using uncompressed HNSW. This version would give us recall ranging from 0.8644 to 0.99757 and latencies ranging from 827 to 2601 microseconds."], "query": "What is the range of memory requirements for indexing data with compressed vectors compared to using uncompressed HNSW for DeepImage96?"}
{"relevant_passages": ["3**: *We are compressing a 128 dimensions vector into a 32 bytes compressed vector. For this, we define 32 segments meaning the first segment is composed of the first four dimensions, the second segment goes from dimension 5th to 8th and so on. Then for each segment we need a compression function that takes a four dimensional vector as an input and returns a byte representing the index of the center which best matches the input. The decompression function is straightforward, given a byte, we reconstruct the segment by returning the center at the index encoded by the input.*\n\nA straightforward encoding/compression function uses KMeans to generate the centers, each of which can be represented using an id/code and then each incoming vector segment can be assigned the id/code for the center closest to it. Putting all of this together, the final algorithm would work as follows: Given a set of N vectors, we segment each of them producing smaller dimensional vectors, then apply KMeans clustering per segment over the complete data and find 256 centroids that will be used as predefined centers."], "query": "How can a 128-dimensional vector be compressed into a 32-byte representation using segmentation and KMeans clustering??"}
{"relevant_passages": ["Then when a query arrives, Weaviate traverses the index to obtain a good approximated answer to the query in a fraction of the time that a brute-force approach would take. [HNSW](/developers/weaviate/concepts/vector-index#hnsw) is the first production-ready indexing algorithm we implemented in Weaviate. It is a robust and fast algorithm that builds a hierarchical representation of the index **in memory** that could be quickly traversed to find the k nearest neighbors of a query vector. ## Need for disk solutions\nThere are other challenges to overcome. Databases have grown so fast that even the above-described algorithms will not be enough."], "query": "What indexing algorithm does Weaviate use to quickly traverse its index in memory??"}
{"relevant_passages": ["<video width=\"100%\" autoplay loop controls>\n  <source src={hacktober_demo} type=\"video/mp4\" />\nYour browser does not support the video tag. </video>\n\n\n2. Ping us on the project\u2019s issue, saying you're interested and which parts of the issue you would like to contribute to. 3. Open the PR as instructed in the [Weaviate Contributor Guide](https://weaviate.io/developers/contributor-guide)."], "query": "How do I contribute to the Weaviate project during Hacktoberfest?"}
{"relevant_passages": ["The PQ centroids fit with the first K vectors that enter Weaviate may be impacted by a significant shift in the data distribution. The continual training of machine learning models has a notorious \u201ccatastrophic forgetting\u201d problem where training on the newest batch of data harms performance on earlier batches of data. This is also something that we are considering with the design of re-fitting PQ centroids. ## From RAG to Agent Evaluation\nThroughout the article, we have been concerned with **RAG**, rather than **Agent** Evaluation. In our view **RAG** is defined by the flow of Index, Retrieve, and Generate, whereas **Agents** have a more open-ended scope."], "query": "What is the impact of data distribution shifts on the fitting of PQ centroids in Weaviate, and how does it relate to catastrophic forgetting in machine learning?"}
{"relevant_passages": ["For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n\n![perf2](./img/image13.png)\n**Fig. 12**: *The chart shows Recall (vertical axis) Vs Indexing time (in minutes, on the horizontal axis). For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n\n![perf3](./img/image14.png)\n**Fig. 13**: *The chart shows Recall (vertical axis) Vs Latency (in microseconds, on the horizontal axis). For this experiment we have added 200,000 vectors using the normal HNSW algorithm, then we switched to compressed and added the remaining 800,000 vectors.*\n\n![perf4](./img/image15.png)\n**Fig."], "query": "How many vectors were added using the normal HNSW algorithm and the compressed method in the experiment, and what do the charts in the figures illustrate??"}
{"relevant_passages": ["Partitioning 2. Cleaning, 3. Staging. Partitioning bricks take an unstructured document and extract structured content from it. It takes the document and breaks it down into elements like `Title`, `Abstract`, and `Introduction`."], "query": "What process is used to extract structured content like `Title`, `Abstract`, and `Introduction` from an unstructured document??"}
{"relevant_passages": ["### Implementation observations\n\nWe initially implemented the Vamana algorithm as described, resulting in very good recall results. Yet the latency was not good at all. We have since realized that the performance decay was due to many set operations making the algorithm perform poorly as is. In our revised implementation, we have modified the algorithm a bit to keep a copy of visited and current nodes on a single sorted list. Also, as the parameter L grows, the search on the sets becomes more expensive, so we already decided to keep a bit-based representation of the vectors residing on the sets, which made a huge impact performance-wise."], "query": "What modifications were made to the original Vamana algorithm to improve its performance in terms of latency??"}
{"relevant_passages": ["But once the database got to around 25 million objects, adding new objects would be significantly slower. Then from 50\u2013100m, the import process would slow down to a walking pace. #### Solution\nTo address this problem, we changed how the HNSW index grows. We implemented a relative growth pattern, where the HNSW index size increases by either 25% or 25'000 objects (whichever is bigger). ![HNSW index growth chart](./img/hnsw-index-growth.jpg)\n\n#### Test\nAfter introducing the relative growth patterns, we've run a few tests."], "query": "What solution was implemented to improve the performance of adding new objects to a database with over 25 million objects?"}
{"relevant_passages": ["This will allow us to package the backup and run Weaviate in the last step directly from the backup. In the environment variables, we set a CLUSTER_HOSTNAME, an arbitrary name you can set to identify a cluster. ```yaml\nenvironment:\n  TRANSFORMERS_INFERENCE_API: 'http:loadbalancer:8080'\n  QUERY_DEFAULTS_LIMIT: 25\n  AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n  PERSISTENCE_DATA_PATH: '/var/lib/weaviate'\n  DEFAULT_VECTORIZER_MODULE: 'text2vec-transformers'\n  ENABLE_MODULES: 'text2vec-transformers'\n  CLUSTER_HOSTNAME: '63e2f234026d'\n```\n\n*Docker environment setup*\n\nWe will also set the location of the volume outside Weaviate, in this case the data will be stored in the /var/weaviate folder\n\n```yaml\nvolumes:\n  - /var/weaviate:/var/lib/weaviate\n```\n\n*Volumes for backup*\n\nYou can find the complete Docker Compose file we've used here. ## Query the Data\nThe current Weaviate setup has two modules enabled: semantic search and Q&A. The modules can be used for different types of queries."], "query": "What is the value of the CLUSTER_HOSTNAME environment variable in the Weaviate Docker setup??"}
{"relevant_passages": ["He needed to implement semantic search, a system that could interpret a user\u2019s intent rather than rely on exact keyword matches. And they needed a system that could index and search multimodal (text and image) data across millions of objects. Based on prior experience with semantic search and vector embeddings, Marais decided a vector database was the tool for the job. Marais quickly put together a checklist of priorities for their vector database selection. Moving fast was important, so ease of use was at the top of the list."], "query": "What were Marais's priorities when selecting a vector database for semantic search and multimodal data indexing?"}
{"relevant_passages": ["Latency when retrieving the hundred approximate nearest neighbors.*\n\n## Vamana implementation details\nWe have also included a development implementation of the Vamana indexing algorithm in Weaviate. For the algorithm to perform well, such an implementation needs careful attention to the optimization of the code. The original algorithm from Microsoft rests upon the greedy search and the robust prune methods, which are described in the [DiskANN paper](https://suhasjs.github.io/files/diskann_neurips19.pdf) as follows:\n\n![Vamana algorithm](./img/vamana-algorithm.png)\n\n### In plain English\nThese pseudo-code snippets are notoriously difficult to read, so here's a plain-English explanation of how Vamana works. The greedy search algorithm is used to find the solution for a query. The main idea is to start looking for the best points iteratively from the entry point."], "query": "What algorithm does Weaviate use to optimize the retrieval of the hundred approximate nearest neighbors??"}
{"relevant_passages": ["Here are some key differences between Vamana and HNSW:\n\n### Vamana indexing - in short:\n* Build a random graph. * Optimize the graph, so it only connects vectors close to each other. * Modify the graph by removing some short connections and adding some long-range edges to speed up the traversal of the graph. ### HNSW indexing - in short:\n* Build a hierarchy of layers to speed up the traversal of the nearest neighbor graph. * In this graph, the top layers contain only long-range edges."], "query": "What are the key differences between Vamana and HNSW indexing methods??"}
{"relevant_passages": [":::note Proposed feature\nTo reduce the length of this time, there is a proposed feature to proactively start repairing those inconsistencies (i.e. perform asynchronous replication). If this is important, please [upvote the feature here](https://github.com/weaviate/weaviate/issues/2405). :::\n\nBut we think that the cost of high availability is worth these prices. We take system availability seriously, and architect Weaviate according to this philosophy. This is one of the reasons that we use [leaderless replication](/developers/weaviate/concepts/replication-architecture/cluster-architecture#leaderless-design), and why replication in the first place is so important to us - because it enables our users to have robust systems on which they can rely."], "query": "What is the proposed feature in Weaviate to handle repairing inconsistencies, and how can users show their support for it??"}
{"relevant_passages": ["If we use `maxConnections = 64` when indexing Sift1M we end up with 1,000,000 x 64 x 8 bytes = 512,000,000 bytes also for the graph. This would bring our total memory requirements to around ~1 GB in order to hold both, the vectors and the graph, for 1,000,000 vectors each with 128 dimensions. 1 GB doesn't sound too bad! Why should we go through the trouble of compressing the vectors at all then!? Sift1M is a rather small dataset. Have a look at some of the other experimental datasets listed in Table 1 below; these are much bigger and this is where the memory requirements can begin to get out of hand."], "query": "How much memory is required to hold both the vectors and the graph for the Sift1M dataset with 1,000,000 vectors each of 128 dimensions using 64 max connections, and why might one consider compressing the vectors??"}
{"relevant_passages": ["---\ntitle: HNSW+PQ - Exploring ANN algorithms Part 2.1\nslug: ann-algorithms-hnsw-pq\nauthors: [abdel]\ndate: 2023-03-14\ntags: ['research']\nimage: ./img/hero.png\ndescription: \"Implementing HNSW + Product Quantization (PQ) vector compression in Weaviate.\"\n---\n![HNSW+PQ - Exploring ANN algorithms Part 2.1](./img/hero.png)\n\n<!-- truncate -->\n\nWeaviate is already a very performant and robust [vector database](https://weaviate.io/blog/what-is-a-vector-database) and with the recent release of  v1.18 we are now bringing vector compression algorithms to Weaviate users everywhere. The main goal of this new feature is to offer similar performance at a fraction of the memory requirements and cost. In this blog we expand on the details behind this delicate balance between recall performance and memory management. In our previous blog [Vamana vs. HNSW - Exploring ANN algorithms Part 1](/blog/ann-algorithms-vamana-vs-hnsw), we explained the challenges and benefits of the Vamana and HNSW indexing algorithms."], "query": "What are the details of implementing HNSW+PQ vector compression in Weaviate as discussed in the blog post from March 14, 2023??"}
{"relevant_passages": ["In this blog post, we will show you how to ingest PDF documents with Unstructured and query in Weaviate. :::info\nTo follow along with this blog post, check out this [repository](https://github.com/weaviate-tutorials/how-to-ingest-pdfs-with-unstructured). :::\n\n## The Basics\nThe data we\u2019re using are two research papers that are publicly available. We first want to convert the PDF to text in order to load it into Weaviate. Starting with the first brick (partitioning), we need to partition the document into text."], "query": "How do you convert PDF documents to text for ingestion into Weaviate??"}
{"relevant_passages": ["Notice the first peak in memory at the beginning. This was the memory used for loading the first fifth of the data plus compressing the data. We then wait for the garbage collection cycle to claim the memory back after which we send the remaining data to the server. Note that the peaks in the middle are due to memory not being freed by the garbage collection process immediately. At the end you see the actual memory used after the garbage collection process clean everything up."], "query": "What causes the initial and subsequent peaks in memory usage during the data handling process described, and what is the state of memory after garbage collection??"}
{"relevant_passages": ["While all the experiments above have been carried out directly on the algorithm, the table below is constructed using data collected from interactions with the Weaviate server. This means the latency is calculated end to end (with all the overhead on communication from clients). |                         |                         | Latency (ms) | Memory required (GB) | Memory to host the vectors (GB) |\n|-------------------------|-------------------------|--------------|----------------------|---------------------------------|\n| 10M vectors from Sphere | Uncompressed            | 119          | 32.54                | 28.66                           |\n|                         | Compressed<br/>(4-to-1) | 273 (x2.29)  | 10.57 (32.48%)       | 7.16 (24.98%)                   |\n\n**Tab. 4**: *Summary of latency and memory requirements on the 10M Sphere subset. Additionally we show the speed down rate in latency and the percentage of memory, compared to uncompressed, needed to operate under compressed options.*\n\n## Conclusions\n\nIn this post we explore the journey and details behind the HNSW+PQ feature released in Weaviate v1.18."], "query": "What are the latency and memory savings when using the compressed option for the 10M vectors from Sphere in Weaviate v1.18?"}
{"relevant_passages": ["There are also attempts to collect richer multimodal datasets such as the [EGO4D](https://ego4d-data.org/docs/) which collects multiple data modalities for a given scenario. EGO4D actually captures motion/tactile data by capturing accelerometer and gyroscopic data temporally aligned with video captures of activities! The problem with this is that generating a rich multimodal dataset is very costly and even impractical in some cases. ### 2. Model Architecture\n\nDesigning the architecture for a single model that can process multimodal data is difficult. Usually, machine learning models are specialists in one domain of data - for example computer vision or natural language."], "query": "What is the EGO4D dataset, and what types of data does it capture?"}
{"relevant_passages": ["run the experiments, and 3. return a high quality report to the human user. Weights & Biases has paved an incredible experiment tracking path for training deep learning models. We expect interest to accelerate in this kind of support for RAG experimentation with the knobs and metrics we have outlined in this article. There are a couple of directions we are watching this evolve in."], "query": "What tool is mentioned for tracking deep learning model training experiments, and what type of experimentation does it support?"}
{"relevant_passages": ["---\ntitle: Weaviate 2023 Recap\nslug: 2023-recap\nauthors: [femke]\ndate: 2023-12-26\ntags: []\nimage: ./img/hero.png\ndescription: \"A reflection on 2023 from team Weaviate!\"\n---\n![hero](img/hero.png)\n\n<!-- truncate -->\n\nIt\u2019s hard to imagine that less than a year ago, so very few people even knew about the concept of vector databases and how AI could benefit from them. Those who did still had many questions about how they worked and whether they could at all be helpful. Meanwhile, curiosity and interest in AI spiked, especially after OpenAI launched ChatGPT. Curiosity has sped up our progress and made more people aware of the opportunities AI offers, transforming our landscape. Let's all take a moment to reflect and appreciate the start of a momentous change in how we can communicate, learn, teach, and collaborate so much faster and more effectively by leveraging AI."], "query": "What are the key highlights from the Weaviate team's 2023 recap??"}
{"relevant_passages": ["Although a bigger machine (see below) is needed for importing the data, the serving is done on a 12 CPU, 100 GB RAM, 250Gb SSD Google Cloud VM with 1 x NVIDIA Tesla P4. The ML-models used are [multi-qa-MiniLM-L6-cos-v1](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1) and [bert-large-uncased-whole-word-masking-finetuned-squad](https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad) both are available as [pre-built modules](/developers/weaviate/modules/retriever-vectorizer-modules/text2vec-transformers#pre-built-images) in Weaviate. \ud83d\udcc4 The complete dataset and code is open-source and available [on GitHub](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate). ![Demo GIF of Weaviate using the Wikipedia dataset](./img/weaviate-using-the-Wikipedia-dataset.gif)\n*Example semantic search queries in Weaviate's GraphQL interface \u2014 GIF by Author*\n\n## Importing the Data In Two Steps\n> You can also directly import a backup into Weaviate without doing the import your self as outlined [here](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate/tree/main#step-3-load-from-backup). To import the data we use two different methods."], "query": "What are the specifications of the Google Cloud VM used for serving in the Weaviate semantic search through Wikipedia project??"}
{"relevant_passages": ["<br/>\nIt compares the vector values dimension by dimension and returns a total count of differing values. The fewer differences, the closer the vectors. For example, the Hamming distance for the below vectors is **2**, which is the count of differing values. * A `[1, 9, 3, 4, 5]`\n* B `[1, 2, 3, 9, 5]`\n\n### Manhattan distance\nThe Manhattan distance (also known as L1 norm and Taxicab Distance) - calculates the distance between a pair of vectors, as if simulating a route for a Manhattan taxi driver driving from point A to point B - who is navigating the **streets of Manhattan** with the grid layout and one-way streets. For each difference in the compared vectors, the taxi driver needs to make a turn, thus making the ride this much longer."], "query": "What is the Hamming distance between the vectors [1, 9, 3, 4, 5] and [1, 2, 3, 9, 5]??"}
{"relevant_passages": ["What does replication get us? A big one is *availability*. With no replication, any node being down will make its data unavailable. But in a Kubernetes setup composed of say, three Weaviate nodes (three Kubernetes \u201cpods\u201d) and a replication factor of three, you can have any one of the three nodes down and still reach consensus. This reflects Weaviate\u2019s leaderless replication architecture, meaning any node can be down without affecting availability at a cluster level as long as the right data is available somewhere."], "query": "How does replication enhance data availability in a Weaviate Kubernetes cluster??"}
{"relevant_passages": ["---\ntitle: Pulling back the curtains on text2vec\nslug: pulling-back-the-curtains-on-text2vec\nauthors: [jp]\ndate: 2023-01-10\ntags: ['integrations', 'concepts']\nimage: ./img/hero.png\ndescription: Ever wonder how Weaviate turns objects into vectors, behind-the-scenes? Find out in this post!\n---\n\n<!-- truncate -->\n\n![Pulling back the curtains on text2vec](./img/hero.png)\n\nYou probably know that Weaviate converts a text corpus into a set of vectors - each object is given a vector that captures its 'meaning'. But you might not know exactly how it does that, or how to adjust that behavior. Here, we will pull back the curtains to examine those questions, by revealing some of the mechanics behind `text2vec`'s magic. First, we will reproduce Weaviate's output vector using only an external API."], "query": "How does Weaviate convert text into vectors using `text2vec`, and how can this process be adjusted??"}
{"relevant_passages": ["<!-- truncate -->\n\n\n## 1. Install the client library\n\nThe Python and TypeScript client libraries support running Weaviate embedded on Linux, and starting with versions 3.21.0 and 1.2.0 respectively, on macOS as well. <Tabs groupId=\"languages\">\n  <TabItem value=\"py\" label=\"Python\">\n\n  ```bash\n  pip install weaviate-client  --upgrade\n  ```\n\n  </TabItem>\n\n  <TabItem value=\"js\" label=\"JavaScript/TypeScript\">\n\n  ```bash\n  npm install weaviate-ts-embedded typescript ts-node jest  # also install support for TypeScript and Jest testing\n  ```\n\n  </TabItem>\n</Tabs>\n\n\n## 2. Run the code\n\n<Tabs groupId=\"languages\">\n  <TabItem value=\"py\" label=\"Python\">\n\n  Save as `embedded.py` and run `python embedded.py`:\n  <br/>\n\n  <FilteredTextBlock\n    text={PyCode}\n    startMarker=\"# START 10lines\"\n    endMarker=\"# END 10lines\"\n    language=\"py\"\n  />\n  </TabItem>\n\n  <TabItem value=\"js\" label=\"JavaScript/TypeScript\">\n\n  Save as `embedded.ts` and run `node --loader=ts-node/esm embedded.ts`:\n  <br/>\n\n  <FilteredTextBlock\n    text={TSCode}\n    startMarker=\"// START 10lines\"\n    endMarker=\"// END 10lines\"\n    language=\"js\"\n  />\n  </TabItem>\n</Tabs>\n\n\n## <i class=\"fa-solid fa-screwdriver-wrench\"></i> How does this work? Essentially, what happens behind the scenes is that the client library downloads the server binary, spawns it in a separate process, connects to it, then terminates it on exit."], "query": "What versions of the Weaviate client libraries support running embedded on macOS, and how do you install and run them??"}
{"relevant_passages": ["}\n    },\n    \"vectorizer\": \"text2vec-huggingface\",  # vectorizer for hugging face\n   ... }\n```\n\n*If you are wondering, yes, you can use a different model for each class.*\n\n### Step 2 \u2013 cook for some time \u2013 import data\nStart importing data into Weaviate. For this, you need your Hugging Face API token, which is used to authorize all calls with \ud83e\udd17. Add your token, to a Weaviate client configuration. For example in Python, you do it like this:\n\n```javascript\nclient = weaviate.Client(\n    url='http://localhost:8080',\n    additional_headers={\n        'X-HuggingFace-Api-Key': 'YOUR-HUGGINGFACE-API-KEY'\n    }\n)\n```\nThen import the data the same way as always."], "query": "How do you add a Hugging Face API token to a Weaviate client configuration in Python??"}
{"relevant_passages": ["To see a list of the newly spun up nodes, run:\n\n```shell\nkubectl get nodes -o wide\n```\n\nYou should see an output similar to the following, indicating that three nodes are up and onto which you can deploy Weaviate:\n\n```shell\nNAME           STATUS   ROLES           AGE    VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME\nminikube       Ready    control-plane   134m   v1.27.3   192.168.49.2   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\nminikube-m02   Ready    <none>          134m   v1.27.3   192.168.49.3   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\nminikube-m03   Ready    <none>          133m   v1.27.3   192.168.49.4   <none>        Ubuntu 22.04.2 LTS   5.15.49-linuxkit   docker://24.0.4\n```\n\nNow, add the Weaviate helm repository to your local helm configuration by running:\n\n```shell\nhelm repo add weaviate https://weaviate.github.io/weaviate-helm\n```\n\nAnd save the default configuration with:\n\n```shell\nhelm show values weaviate/weaviate > values.yaml\n```\n\nEdit `values.yaml` by changing the root-level configuration `replicas: 1` for the root image to `replicas: 3`, and save it. ```yaml\n... # Scale replicas of Weaviate. Note that as of v1.8.0 dynamic scaling is limited\n# to cases where no data is imported yet. Scaling down after importing data may\n# break usability."], "query": "How can I list the current nodes in a Kubernetes cluster and prepare to deploy Weaviate with three replicas??"}
{"relevant_passages": ["With these changes alone, we achieved performance close to Microsoft's implementation, as shown in **(fig. 3)** and **(fig. 4)**. Note that the implementation from Microsoft is in C++, and our implementation is in Go.\n\nWe evaluated different data structures and achieved the best performance by:\n* Keeping a sorted array-based set. * Making insertions use binary search."], "query": "How did the Go implementation achieve performance close to Microsoft's C++ implementation??"}
{"relevant_passages": ["In 2020, Google released [Meena](https://blog.research.google/2020/01/towards-conversational-agent-that-can.html), a conversational agent. The goal of Meena was to show that it can have **sensible** and **specific** conversations. To measure the performance of the open-domain chatbots, they introduced the Sensibleness and Specificity Average (SSA) evaluation metrics. The bot\u2019s response was measured by its sensibleness, meaning it needed to make sense in context and be specific (specificity average). This ensures the output is comprehensive without being too vague."], "query": "What is the Sensibleness and Specificity Average (SSA) introduced by Google for?"}
{"relevant_passages": ["What if you have a case where you have a large-scale dataset but very few queries? What if cost-effectiveness is more of a priority than the lowest possible latencies? We believe there is a need for other index types besides the battle-tested HNSW implementation. But cost-effectiveness can never justify sacrificing the user experience. As a result, we are working on establishing a new type of vector index that combines the low operating cost of SSD-based solutions with the ease of use of existing in-memory solutions."], "query": "What new type of vector index is being developed to be cost-effective for large-scale datasets with few queries while still being user-friendly??"}
{"relevant_passages": ["* Join our community on [Slack](https://weaviate.io/slack) or [the forum](https://forum.weaviate.io/), where we can all talk about vector databases. * Reach out to us on [Twitter](https://twitter.com/weaviate_io). import Ending from '/_includes/blog-end-oss-comment.md'\n\n<Ending />"], "query": "How can I join a community to discuss vector databases and follow their updates on Twitter??"}
{"relevant_passages": ["The first approach seems more feasible and thus we will explore it next. In addition to the above mentioned problem, we also need the new functions to operate efficiently so the overall performance of the system doesn't suffer. Keep in mind that for a single query, any ANN algorithm would require distance calculations so by adding unnecessary complexity to the distance function we might severely damage the latency of querying or the overall time to index vectors. For these reasons we need to select the compression mechanism very carefully! There already exist many compression mechanisms, of which a very popular one is Product Quantization(PQ). This is the compression algorithm we chose to implement in Weaviate with v1.18."], "query": "What compression algorithm was chosen for implementation in Weaviate v1.18 to ensure efficient operation of new functions without compromising system performance??"}
{"relevant_passages": ["### Announcement\n\n\ud83c\udf89We are happy to share that all Weaviate `v1.15` binaries and distributions have been **compiled with Go 1.19** which comes with **GOMEMLIMIT**. Now, you can set your **soft memory cap** by setting the `GOMEMLIMIT` environment variable like this:\n\n```\nGOMEMLIMIT=120GiB\n```\n\nFor more information, see the [Docker Compose environment variables](/developers/weaviate/installation/docker-compose#environment-variables) in the docs. ## Faster imports for ordered data\n\n![Faster imports for ordered data](./img/ordered-imports.png)\n\nWeaviate `v1.5` introduced an **LSM store** (Log-Structured Merge Trees) to increase write-throughput. The high-level idea is that writes are batched up in logs, sorted into a **Binary Search Tree** (BST) structure, and then these batched-up trees are merged into the tree on disk. ### The Problem\n\nWhen importing objects with an inherent order, such as timestamps or row numbers that increase monotonously, the BST becomes unbalanced: New objects are always inserted at the \"greater than\" pointer / right node of the BST."], "query": "What environment variable can be set to define a soft memory cap in Weaviate v1.15??"}
{"relevant_passages": ["This lets you find the right balance between the **recall tradeoff** (the fraction of results that are the true top-k nearest neighbors), **latency**, **throughput** (queries per second) and **import time**.*<br/>\n*For a great example, check [Weaviate benchmarks](/developers/weaviate/benchmarks/ann#sift1m-1m-128d-vectors-l2-distance), to see how three parameters \u2013 [efConstruction, maxConnections and ef](/developers/weaviate/benchmarks/ann#what-is-being-measured) - affect recall, latency, throughput and import times.*\n\n### Examples of ANN algorithms\nExamples of ANN methods are:\n* **trees** \u2013 e.g. [ANNOY](https://github.com/spotify/annoy) (Figure 3),\n* **proximity** **graphs** - e.g. [HNSW](https://arxiv.org/abs/1603.09320) (Figure 4),\n* **clustering** - e.g. [FAISS](https://github.com/facebookresearch/faiss),\n* **hashing** - e.g. [LSH](https://en.wikipedia.org/wiki/Locality-sensitive_hashing),\n* **vector compression** - e.g. [PQ](https://ieeexplore.ieee.org/document/5432202) or [SCANN](https://ai.googleblog.com/2020/07/announcing-scann-efficient-vector.html). ![ANNOY](./img/ann-annoy.png)<br/>\n*[Figure 3 - Tree-based ANN search]*\n\nWhich algorithm works best depends on your project. Performance can be measured in terms of latency, throughput (queries per second), build time, and accuracy (recall). These four components often have a tradeoff, so it depends on the use case which method works best. So, while ANN is not some magic method that will always find the true k nearest neighbors in a dataset, it can find a pretty good approximation of the true k neighbors."], "query": "What factors should be considered when evaluating the performance of approximate nearest neighbor search algorithms??"}
{"relevant_passages": ["* [**RAG Metrics**](#rag-metrics): Common metrics used to evaluate Generation, Search, and Indexing and how they interact with each other. * [**RAG: Knobs to Tune**](#rag-knobs-to-tune): What decision makes RAG systems perform significantly different from one another? * [**Orchestrating Tuning**](#tuning-orchestration): How do we manage tracking experimental configurations of RAG systems? * [**From RAG to Agent Evaluation**](#from-rag-to-agent-evaluation): We define RAG as a three step procss of index, retrieve, and generate. This section describes when a RAG system becomes an Agent system and how Agent Evaluation differs."], "query": "What are the common metrics used to evaluate the performance of RAG systems in AI?"}
{"relevant_passages": ["---\ntitle: Ranking Models for Better Search\nslug: ranking-models-for-better-search\nauthors: [connor, erika]\ndate: 2023-04-11\nimage: ./img/hero.png\ntags: ['search']\ndescription: \"Learn about the different ranking models that are used for better search.\"\n\n---\n\n![ranking models animation](./img/hero.png)\n<!-- truncate -->\n\nWhether searching to present information to a human, or a large language model, quality matters. One of the low hanging fruit strategies to improve search quality are ranking models. Broadly speaking, ranking models describe taking the query and each candidate document, one-by-one, as input to predict relevance. This is different from vector and lexical search where representations are computed offline and indexed for speed. Back in August, we [published](/blog/cross-encoders-as-reranker) our thoughts on Cross Encoder Ranking."], "query": "What are ranking models, and how do they differ from vector and lexical search in improving search quality??"}
{"relevant_passages": ["Approximate nearest neighbor errors are typically measured by finding pareto-optimal points that trade off accuracy with queries per second and recall, ANN recall being the ground truth nearest neighbors to a query, rather than documents labeled as \u201crelevant\u201d to the query. ### Generation Metrics\nThe overall goal of a RAG application is to have a helpful output that uses the retrieved context for support. The evaluation must consider that the output has used the context without directly taking it from the source, avoiding redundant information, as well as preventing incomplete answers. To score the output, there needs to be a metric that covers each criteria. [Ragas](https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics) introduced two scores to measure the performance of an LLM output: faithfulness and answer relevancy."], "query": "What metrics are introduced by Ragas to measure the performance of an LLM output in RAG applications?"}
{"relevant_passages": ["This case is quite similar to our discussion of Multi-Index Routing and we can similarly evaluate generations with a prompt that explains the needs for SQL and Vector Databases and then asks the LLM whether the router made the right decision. We can also use the RAGAS Context Relevance score for the results of the SQL query. <img\n  src={require('./img/sql-router.png').default}\n  alt=\"SQL Router Query Engine\"\n  style={{ maxWidth: \"60%\" }}\n/>\n\nConcluding our discussion of \u201cFrom RAG to Agent Evaluation\u201d, we believe that it is still too early to tell what the common patterns will be for agent use. We have intentionally shown the multi-hop query engine and query router because these are relatively straightforward to understand. Once we add more open-ended planning loops, tool use and the associated evaluation of how well the model can format API requests to the tool, and more meta internal memory management prompts such as the ideas in MemGPT, it is very difficult to provide a general abstraction around how Agents will be evaluated."], "query": "What are the considerations for evaluating agents in the context of Multi-Index Routing and SQL and Vector Databases?"}
{"relevant_passages": ["This is especially interesting when tuning a multi-tenant use case with new versus power users. Within ANN indexing, we have PQ\u2019s hyperparameters of (segments, centroids, and the training limit). HNSW comes with (ef, efConstruction, and maxConnections). * Retrieval: Choosing an embedding model, weighting hybrid search, choosing a re-ranker, and dividing collections into multiple indexes. * Generation: Choosing an LLM and when to make the transition from Prompt Tuning to Few-Shot Examples, or Fine-Tuning."], "query": "What are the hyperparameters associated with PQ and HNSW in ANN indexing, and what considerations are involved in retrieval and generation for multi-tenant use cases?"}
{"relevant_passages": ["One on hand, the Zero-Shot LLMs out there such as GPT-4, Command, Claude, and open-source options such as Llama-2 and Mistral perform fairly well when they have **oracle context**. Thus, there is a massive opportunity to **focus solely on the search half**. This requires finding the needle in the haystack of configurations that trade-off ANN error from PQ or HNSW trade-offs, with embedding models, hybrid search weightings, and re-ranking as described earlier in the article. Weaviate 1.22 introduces Async Indexing with corresponding node status APIs. Our hope with partnerships focusing on RAG evaluation and tuning orchestration, is that they can use these APIs to see when an index is finished building and then run the test. This is particularly exciting when considering interfaces between these node statuses and tuning orchestration per tenant where some tenants may get away with brute force, and others need to find the right embedding model and HNSW configuration for their data."], "query": "What new feature does Weaviate 1.22 introduce to assist with RAG evaluation and tuning orchestration?"}
{"relevant_passages": ["But that is content for another article.\ud83d\ude09*\n\n## Learn more\nThe Weaviate Core team is currently working on research and implementation for other ANN algorithms. We are going to publish some of our findings in the next couple of weeks. So, stay tuned for more content on the topic. Until then, you can:\n* Listen to a [podcast about ANN Benchmarks](https://youtu.be/kG3ji89AFyQ) with Connor and Etienne from Weaviate. * Check out the [Getting Started with Weaviate](/developers/weaviate/quickstart) guide and begin building amazing apps with Weaviate."], "query": "What upcoming content is the Weaviate Core team planning to publish regarding ANN algorithms??"}
{"relevant_passages": ["### The solution\n\nAt the beginning of August, the Go team released `Go 1.19`, which introduced `GOMEMLIMIT`. `GOMEMLIMIT` turned out to be a **game changer for high-memory applications**. With GOMEMLIMIT we can provide a soft memory cap, which tells Go how much memory we expect the application to need. This makes the GC more relaxed when RAM is plentiful and more aggressive when memory is scarce. To learn more about memory management, GC and GOMEMLIMIT, check out [this article](/blog/gomemlimit-a-game-changer-for-high-memory-applications), which explains it all in more depth."], "query": "What feature introduced in Go 1.19 helps manage memory for high-memory applications??"}
{"relevant_passages": ["It\u2019s just that easy. Before you rush off to switch on replication, though, stick with us to read about the trade-offs and our recommendations. \ud83d\ude09\n\n## Trade-offs & discussions\n\nWhile replication and high availability are wonderful, we won\u2019t quite pretend that it comes for free. Having additional replicas of course means that there are more tenants and objects overall. Although they are duplicated, they are just as *real* as objects as any others."], "query": "What are some trade-offs to consider before enabling replication for high availability??"}
{"relevant_passages": ["{search_results}\u201d. We similarly encourage readers to check out some of the prompts available in Ragas [here](https://github.com/explodinggradients/ragas/tree/main/src/ragas/metrics). Another metric worth exploring is LLM Wins, where an LLM is prompted with: \u201cBased on the query {query}, which set of search results are more relevant? Set A {Set_A} or Set B {Set_B}. VERY IMPORTANT! Please restrict your output to \u201cSet A\u201d or \u201cSet B\u201d."], "query": "How can you determine which set of search results is more relevant using the LLM Wins metric?"}
{"relevant_passages": ["This collapses the binary tree into a linked list with `O(n)` insertion rather than the `O(log n)` promise of the BST. ### The Fix\n\nTo address that in Weaviate `v1.15`, we've extended the BST with a **self-balancing Red-black tree**. Through rotations of the tree at insert, [Red-black trees](https://www.programiz.com/dsa/red-black-tree) ensure that no path from the root to leaf is more than twice as long as any other path. This achieves O(log n) insert times for ordered inserts. ![Red-black tree demonstration](./img/red-black-tree.gif)\n*A visual representation of how the RBT works.*\n\nYou can try it yourself [here](https://www.cs.usfca.edu/~galles/visualization/RedBlack.html)."], "query": "What feature was introduced in Weaviate `v1.15` to ensure `O(log n)` insertion times for ordered inserts??"}
{"relevant_passages": ["There is no performance gain if we have more parallelization than there are available CPUs. However, each thread needs additional memory. So with 32 parallel imports, we had the worst of both worlds: High memory usage and no performance gains beyond 8. With the fix, even if you import from multiple clients, Weaviate automatically handles the parallelization to ensure that it does not exceed the number of CPU cores. As a result, you get the maximum performance without \"unnecessary\" memory usage. ### HNSW optimization\n\nNext, we optimized memory allocations for the HNSW (vector) index."], "query": "How does Weaviate ensure optimal performance and memory usage during parallel imports??"}
{"relevant_passages": ["Though we still have a ways to go on this journey, as noted earlier, but what we've accomplished thus far can add loads of value to Weaviate users. This solution already allows for significant savings in terms of memory. When compressing vectors ranging from 100 to 1000 dimensions you can save half to three fourths of the memory you would normally need to run HNSW. This saving comes with a little cost in recall or latency. The indexing time is also larger but to address this we've cooked up a second encoder, developed specially by Weaviate, based on the distribution of the data that will reduce the indexing time significantly."], "query": "What are the benefits and trade-offs of the new solution for compressing vectors in Weaviate?"}
{"relevant_passages": ["![Manhattan taxi driver](./img/manhatten-distance-cars.png)\n\nThe Manhattan distance is calculated by adding up the differences between vector values. Following our previous example:\n* A `[1, 9, 3, 4, 5]`\n* B `[1, 2, 3, 9, 5]`\n\nWe can calculate the Manhattan distance in these steps:\n1. distance = `|1-1| + |9-2| + |3-3| + |4-9| + |5-5|`\n1. distance = `0 + 7 + 0 + 5 + 0`\n1. distance = `12`\n\nFor a deeper dive into the Hamming and Manhattan distances, check out our [blog post on distance metrics](/blog/distance-metrics-in-vector-search)."], "query": "How is the Manhattan distance between the vectors `[1, 9, 3, 4, 5]` and `[1, 2, 3, 9, 5]` calculated??"}
{"relevant_passages": ["We are already working on a fully disk-based solution which we will release in late 2023. <br></br>\n\n### Improving our Client and Module Ecosystem\n![client modules](./img/client-modules.png)\n\nSo far, we have only discussed features related to Weaviate Core, the server in your setup. But the Weaviate experience is more than that. Modules allow you to integrate seamlessly with various embedding providers, and our language clients make Weaviate accessible right from your application. This year, we will further improve both."], "query": "When is the fully disk-based solution for Weaviate expected to be released??"}
{"relevant_passages": ["Within a schema, you can set different vectorizers and vectorize instructions on a class level. First, because our use case is semantic search over Wikipedia, we will be dividing the dataset into paragraphs and use Weaviate's graph schema to link them back to the articles. Therefore we need two classes; *Article* and *Paragraph*. ```javascript\n{\n  classes: [\n    {\n      class: \"Article\",\n      description: \"A wikipedia article with a title\",\n      properties: {...},\n      vectorIndexType: \"hnsw\",\n      vectorizer: \"none\"\n    },\n    {\n      class: \"Paragraph\",\n      description: \"A wiki paragraph\",\n      properties: {...},\n      vectorIndexType: \"hnsw\",\n      vectorizer: \"text2vec-transformers\"\n    },\n  ]\n}\n```\n\n*Weaviate class structure*\n\nNext, we want to make sure that the content of the paragraphs gets vectorized properly, the vector representations that the SentenceBERT transformers will generate are used for all our semantic search queries. ```javascript\n{\n  name: \"content\",\n  datatype: [\n    \"text\"\n  ],\n  description: \"The content of the paragraph\",\n  invertedIndex: false,\n  moduleConfig: {\n    text2vec-transformers: {\n      skip: false,\n      vectorizePropertyName: false\n    }\n  }\n}\n```\n\n*A single data type that gets vectorized*\n\nLast, we want to make graph relations, in the dataset from step one we will distill all the graph relations between articles that we can reference like this:\n\n```javascript\n{\n  name: \"hasParagraphs\"\n  dataType: [\n    \"Paragraph\"\n  ],\n  description: \"List of paragraphs this article has\",\n  invertedIndex: true\n}\n```\n*Paragraph cross-references*\n\nThe complete schema we import using the [Python client](/developers/weaviate/client-libraries/python) can be found [here](https://github.com/weaviate/semantic-search-through-wikipedia-with-weaviate/blob/main/step-2/import.py#L19-L120)."], "query": "How do you set up a Weaviate schema for semantic search on Wikipedia data with vectorized content and graph relations??"}
{"relevant_passages": ["---\ntitle: Support for Hugging Face Inference API in Weaviate\nslug: hugging-face-inference-api-in-weaviate\nauthors: [sebastian]\ndate: 2022-09-27\ntags: ['integrations']\nimage: ./img/hero.png\ndescription: \"Running ML Model Inference in production is hard. You can use Weaviate \u2013 a vector database \u2013 with Hugging Face Inference module to delegate the heavy lifting.\"\n---\n![Support for Hugging Face Inference API in Weaviate](./img/hero.png)\n\n<!-- truncate -->\n\nVector databases use Machine Learning models to offer incredible functionality to operate on your data. We are looking at anything from **summarizers** (that can summarize any text into a short) sentence), through **auto-labelers** (that can classify your data tokens), to **transformers** and **vectorizers** (that can convert any data \u2013 text, image, audio, etc. \u2013 into vectors and use that for context-based queries) and many more use cases. All of these use cases require `Machine Learning model inference` \u2013 a process of running data through an ML model and calculating an output (e.g. take a paragraph, and summarize into to a short sentence) \u2013 which is a compute-heavy process."], "query": "What functionalities does the integration of Hugging Face Inference API with Weaviate offer??"}
{"relevant_passages": ["Determining whether a generated response is good or bad is dependent on a few metrics. You can have answers that are factual, but not relevant to the given query. Additionally, answers can be vague and miss out key contextual information to support the response. We will now go one step back through the pipeline and cover retrieval metrics. ### Retrieval Metrics\nThe next layer in the evaluation stack is information retrieval."], "query": "What factors are important in evaluating the quality of generated responses and what role do retrieval metrics play?"}
