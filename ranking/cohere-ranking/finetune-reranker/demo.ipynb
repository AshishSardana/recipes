{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Reranker using Cohere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This demo will show you how to:\n",
    "1. Generate synthetic data using DSPy\n",
    "2. Export all your data from your Weaviate instance\n",
    "3. Steps to fine-tune a reranker using Cohere\n",
    "4. Query in Weaviate using your fine-tuned reranker model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: To fine-tune a model in Cohere, you need to have a minimum of 256 unique queries with at least 1 relevant passage per query. If you already have a dataset with query + relevant passages, you can skip to the end of the notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/weaviate/__init__.py:128: DeprecationWarning: Dep010: Importing AuthApiKey from weaviate is deprecated. Please import it from its specific module: weaviate.auth\n",
      "  _Warnings.root_module_import(name, map_[name])\n",
      "/usr/local/lib/python3.11/site-packages/weaviate/warnings.py:158: DeprecationWarning: Dep016: You are using the Weaviate v3 client, which is deprecated.\n",
      "            Consider upgrading to the new and improved v4 client instead!\n",
      "            See here for usage: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import json\n",
    "import os \n",
    "\n",
    "client = weaviate.Client(\n",
    "    url = \"WEAVIATE_URL\",  # Replace with your cluster url\n",
    "    auth_client_secret=weaviate.AuthApiKey(api_key=\"AUTH_KEY\"),  # Replace w/ your Weaviate instance API key\n",
    "    additional_headers = {\n",
    "        \"X-Cohere-Api-Key\": \"API_KEY\" # Replace with your inference API key\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import uuid\n",
    "import tqdm\n",
    "from typing import List, Any\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import asyncio\n",
    "import weaviate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Break a list into chunks of the specified size.\"\"\"\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentences using regular expressions.\"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def read_and_chunk_index_files(main_folder_path):\n",
    "    \"\"\"Read index.md files from subfolders, split into sentences, and chunk every 5 sentences.\"\"\"\n",
    "    blog_chunks = []\n",
    "    for folder_name in os.listdir(main_folder_path):\n",
    "        subfolder_path = os.path.join(main_folder_path, folder_name)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            index_file_path = os.path.join(subfolder_path, 'index.mdx')\n",
    "            if os.path.isfile(index_file_path):\n",
    "                with open(index_file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    sentences = split_into_sentences(content)\n",
    "                    sentence_chunks = chunk_list(sentences, 5)\n",
    "                    sentence_chunks = [' '.join(chunk) for chunk in sentence_chunks]\n",
    "                    blog_chunks.extend(sentence_chunks)\n",
    "    return blog_chunks\n",
    "\n",
    "# Example usage\n",
    "main_folder_path = './blog'\n",
    "blogs = read_and_chunk_index_files(main_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1182\n"
     ]
    }
   ],
   "source": [
    "print(len(blogs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to reset your schema and delete objects in a collection, run:\n",
    "`client.schema.delete_all()` or `client.schema.delete_class(\"Blogs\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "   \"classes\": [\n",
    "       {\n",
    "           \"class\": \"Blogs\",\n",
    "           \"description\": \"Weaviate blogs\",\n",
    "           \"vectorizer\": \"text2vec-cohere\",\n",
    "           \"properties\": [\n",
    "               {\n",
    "                   \"name\": \"content\",\n",
    "                   \"dataType\": [\"text\"],\n",
    "                   \"description\": \"Content from the blogs.\",\n",
    "               },\n",
    "               {\n",
    "                   \"name\": \"synthetic_query\",\n",
    "                   \"dataType\": [\"text\"],\n",
    "                   \"description\": \"Synthetic query generated from a LM.\"\n",
    "               }\n",
    "           ]\n",
    "       }      \n",
    "   ]\n",
    "}\n",
    "    \n",
    "client.schema.create(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install dspy-ai > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To generate the synthetic queries, we will use DSPy's signature and chain-of-thought module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to understand the relationship between recall, heap usage, and parameter sets as depicted in the charts. The document seems to describe the trade-offs between memory usage and the accuracy of a search algorithm, as influenced by the choice of parameter sets. The charts on the left illustrate how recall (the ability to retrieve relevant items) is affected by the amount of heap memory used. The charts on the right seem to compare heap usage with different parameter sets, suggesting that certain sets require more memory but may result in a larger and more accurate graph. The document also hints at the possibility of adjusting the level of data compression, which would further affect these relationships. Therefore, the query should aim to extract information about how these variables interact.\",\n",
      "    query='How does heap usage affect recall and the accuracy of search results based on different parameter sets?'\n",
      ")\n",
      "{'content': \"Charts to the left show Recall (vertical axis) Vs Heap usage (horizontal axis). Charts to the right show Heap usage (horizontal axis) Vs the different parameter sets. Parameter sets to achieve a larger graph (also producing a more accurate search) are charted from top down.*\\n\\nLet's sumamrize what we see in the above charts. We could index our data using high or low parameters set. Additionally, we could aim for different levels of compression.\", 'synthetic_query': 'How does heap usage affect recall and the accuracy of search results based on different parameter sets?'}\n",
      "\n",
      "1\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify the unique information provided in the document that can be asked about. The document mentions specific details about the memory requirements, recall, and latency for indexing data using uncompressed HNSW with the Sift1M dataset. Therefore, we can ask about the memory requirements, recall range, or latency range for this specific scenario.\",\n",
      "    query='What is the range of memory required to index the Sift1M dataset using uncompressed HNSW, and what recall and latency can be expected?'\n",
      ")\n",
      "{'content': 'The fact that the graph is still in memory makes it hard to see the difference between those different levels of compression. The more we compress the lower the recall we would expect. Let us discuss the lowest level of compression along with some expectations. For Sift1M we would require roughly 1277 MB to 1674 MB of memory to index our data using uncompressed HNSW. This version would give us recall ranging from 0.96811 to 0.99974 and latencies ranging from 293 to 1772 microseconds.', 'synthetic_query': 'What is the range of memory required to index the Sift1M dataset using uncompressed HNSW, and what recall and latency can be expected?'}\n",
      "\n",
      "2\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: If we compress the vectors then the memory requirements goes down to 610 MB to 1478 MB range. After compression recall, drops to values ranging from 0.9136 to 0.9965. Latency rises up to the 401 to 1937 microsends range. For DeepImage96 we would require roughly 9420 MB to 15226 MB of memory to index our data using uncompressed HNSW. This version would give us recall ranging from 0.8644 to 0.99757 and latencies ranging from 827 to 2601 microseconds.\\n\\nReasoning: Let's think step by step in order to produce the query. We need to identify a unique piece of information that the document provides. The document compares two scenarios: one with compressed vectors and one with uncompressed vectors for a system called DeepImage96. It provides specific metrics for memory requirements, recall, and latency for both scenarios. A query that this document would answer should ask for a comparison between the two scenarios in terms of one of these metrics.\",\n",
      "    query='What is the range of memory requirements for indexing data with compressed vectors compared to using uncompressed HNSW for DeepImage96?'\n",
      ")\n",
      "{'content': 'If we compress the vectors then the memory requirements goes down to 610 MB to 1478 MB range. After compression recall, drops to values ranging from 0.9136 to 0.9965. Latency rises up to the 401 to 1937 microsends range. For DeepImage96 we would require roughly 9420 MB to 15226 MB of memory to index our data using uncompressed HNSW. This version would give us recall ranging from 0.8644 to 0.99757 and latencies ranging from 827 to 2601 microseconds.', 'synthetic_query': 'What is the range of memory requirements for indexing data with compressed vectors compared to using uncompressed HNSW for DeepImage96?'}\n",
      "\n",
      "3\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify the unique pieces of information provided in the document. The document mentions specific ranges for memory requirements, recall, and latency associated with compressed vectors, as well as details for an uncompressed HNSW index for Gist. By focusing on the unique data points such as the specific memory range for compressed vectors or the recall and latency for the uncompressed HNSW, we can formulate a query that the document can uniquely answer.\",\n",
      "    query='What is the range of memory requirements for compressed vectors according to the document?'\n",
      ")\n",
      "{'content': 'If we compress the vectors then the memory requirements goes down to the 4730 MB to 12367 MB range. After compression, recall drops to values ranging from 0.8566 to 0.9702. Latency rises up to the 1039 to 2708 microsends range. For Gist we would require roughly 4218 MB to 5103 MB of memory to index our data using uncompressed HNSW. This version would give us recall ranging from 0.7446 to 0.9962 and latencies ranging from 2133 to 15539 microseconds.', 'synthetic_query': 'What is the range of memory requirements for compressed vectors according to the document?'}\n",
      "\n",
      "4\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify a question that can be uniquely answered by the information provided in the document. The document contains specific data about the effects of compression on recall, latency, and memory requirements for different parameters and datasets. The table provides a clear comparison between uncompressed and compressed states for each dataset and parameter setting. Therefore, a query that asks for the impact of compression on one of these metrics for a specific dataset and parameter setting would be uniquely answerable by this document.',\n",
      "    query='What is the percentage reduction in memory requirements for the Gist dataset with low parameters after compression?'\n",
      ")\n",
      "{'content': 'If we compress the vectors then the memory requirements goes down to the 1572 MB to 2129 MB range. After compression, recall drops to values ranging from 0.7337 to 0.9545. Latency rises up to the 7521 to 37402 microsends range. A summary is shown in Table 3 below. |                       |              | Recall100@100 | Latency ($\\\\mu s$)         | Memory required (MB)         |\\n|-----------------------|--------------|---------------|---------------------------|------------------------------|\\n| Sift1M Low params     | Uncompressed | 0.91561       | 293                       | 1277                         |\\n|                       | Compressed   | 0.91361       | 401               (x1.36) | 610                 (47.76%) |\\n| Sift1M High params    | Uncompressed | 0.99974       | 1772                      | 1674                         |\\n|                       | Compressed   | 0.99658       | 1937             (x1.09)  | 1478               (88.29%)  |\\n| DeepImage Low params  | Uncompressed | 0.8644        | 827                       | 9420                         |\\n|                       | Compressed   | 0.85666       | 1039             (x1.25)  | 4730               (50.21%)  |\\n| DeepImage High params | Uncompressed | 0.99757       | 2601                      | 15226                        |\\n|                       | Compressed   | 0.97023       | 2708             (x1.04)  | 12367             (81.22%)   |\\n| Gist Low params       | Uncompressed | 0.74461       | 2133                      | 4218                         |\\n|                       | Compressed   | 0.73376       | 7521             (x3.52)  | 1572              (37.26%)   |\\n| Gist High params      | Uncompressed | 0.99628       | 15539                     | 5103                         |\\n|                       | Compressed   | 0.95455       | 37402           (x2.40)   | 2129               (41.72%)  |\\n\\n**Tab.', 'synthetic_query': 'What is the percentage reduction in memory requirements for the Gist dataset with low parameters after compression?'}\n",
      "\n",
      "5\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify the unique aspects of the document that a query could be asking about. The document mentions a summary of results for parameter sets in both uncompressed and compressed versions, a speed down rate in latency, and the percentage of memory needed for compressed options. Additionally, it references an open-source dataset called Sphere, released by Meta, with specific details about its dimensions and the number of objects it contains. Therefore, the query should be about the content of the summary, the comparison between uncompressed and compressed versions, the performance metrics mentioned, or details about the Sphere dataset.\",\n",
      "    query='What does the document say about the performance comparison between uncompressed and compressed parameter sets?'\n",
      ")\n",
      "{'content': '3**: *Summary of the previously presented results. We show results for the shortest and largest parameter sets for uncompressed and compressed versions. Additionally we show the speed down rate in latency and the percentage of memory, compared to uncompressed, needed to operate under compressed options.*\\n\\nWe would like to give you an extra bonus row from the above table. [Sphere](https://github.com/facebookresearch/sphere) is an open-source dataset recently released by Meta. It collects 768 dimensions and nearly a billion objects.', 'synthetic_query': 'What does the document say about the performance comparison between uncompressed and compressed parameter sets?'}\n",
      "\n",
      "6\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information provided in the document that a query could be asking for. The document mentions the size of memory required to host vectors, the implications of working with large data sets, the trade-offs between cost and performance, and a reference to a test that was conducted. The document also hints at the topic of data compression and the use of disk storage for graphs. Therefore, the query should be about the memory requirements for hosting vectors, the consequences of handling large data, or the results of a specific test mentioned in the document.',\n",
      "    query='How much memory is required to host vectors in memory for 10 million objects, and what are the trade-offs of working with such large data sets?'\n",
      ")\n",
      "{'content': 'Only hosting the vectors in memory would take 3.1 TB. To work with data of this size, either you need to spend more to provision expensive resources, or you can sacrifice a bit on recall and latency and save drastically on resources. We show a simple test below to drive home the importance of compressing data and eventually moving graphs also to disk. The test was run over 10 million objects only. A final note on the numbers.', 'synthetic_query': 'How much memory is required to host vectors in memory for 10 million objects, and what are the trade-offs of working with such large data sets?'}\n",
      "\n",
      "7\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify a unique piece of information that the document provides. The document seems to be summarizing the results of experiments comparing the performance of an algorithm on a dataset with 10 million vectors from Sphere, specifically focusing on latency and memory requirements in both uncompressed and compressed states. The document also mentions the version of Weaviate where a feature was released. Therefore, a query that this document would answer should be related to the performance metrics of the algorithm or the version of Weaviate where a certain feature was introduced.',\n",
      "    query='What are the latency and memory savings when using the compressed option for the 10M vectors from Sphere in Weaviate v1.18?'\n",
      ")\n",
      "{'content': 'While all the experiments above have been carried out directly on the algorithm, the table below is constructed using data collected from interactions with the Weaviate server. This means the latency is calculated end to end (with all the overhead on communication from clients). |                         |                         | Latency (ms) | Memory required (GB) | Memory to host the vectors (GB) |\\n|-------------------------|-------------------------|--------------|----------------------|---------------------------------|\\n| 10M vectors from Sphere | Uncompressed            | 119          | 32.54                | 28.66                           |\\n|                         | Compressed<br/>(4-to-1) | 273 (x2.29)  | 10.57 (32.48%)       | 7.16 (24.98%)                   |\\n\\n**Tab. 4**: *Summary of latency and memory requirements on the 10M Sphere subset. Additionally we show the speed down rate in latency and the percentage of memory, compared to uncompressed, needed to operate under compressed options.*\\n\\n## Conclusions\\n\\nIn this post we explore the journey and details behind the HNSW+PQ feature released in Weaviate v1.18.', 'synthetic_query': 'What are the latency and memory savings when using the compressed option for the 10M vectors from Sphere in Weaviate v1.18?'}\n",
      "\n",
      "8\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key points of the document that could be the basis for a question. The document mentions a journey and accomplishments, which are related to Weaviate users. It talks about a solution that allows for significant memory savings when compressing vectors of certain dimensions. It also mentions a trade-off in recall or latency and an increase in indexing time. Finally, it refers to a second encoder developed by Weaviate to reduce indexing time. Based on these points, we can formulate a question that the document would answer.',\n",
      "    query='What are the benefits and trade-offs of the new solution for compressing vectors in Weaviate?'\n",
      ")\n",
      "{'content': \"Though we still have a ways to go on this journey, as noted earlier, but what we've accomplished thus far can add loads of value to Weaviate users. This solution already allows for significant savings in terms of memory. When compressing vectors ranging from 100 to 1000 dimensions you can save half to three fourths of the memory you would normally need to run HNSW. This saving comes with a little cost in recall or latency. The indexing time is also larger but to address this we've cooked up a second encoder, developed specially by Weaviate, based on the distribution of the data that will reduce the indexing time significantly.\", 'synthetic_query': 'What are the benefits and trade-offs of the new solution for compressing vectors in Weaviate?'}\n",
      "\n",
      "9\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify the unique information provided in the document that can be asked about. The document mentions that more details will be shared soon on a blog, and it includes a smiley face emoji, which suggests a positive tone. Additionally, there is a mention of an import statement for a component called WhatNext from a specific directory. The query should be about the content that is promised to be detailed on the blog or about the WhatNext component that is being imported.\",\n",
      "    query='Where can I find upcoming details and insights that are mentioned in the document?'\n",
      ")\n",
      "{'content': \"Keep your eyes peeled for more details on this soon on [our blog](/blog)! We will share our insights as we go. 😀\\n\\n\\nimport WhatNext from '/_includes/what-next.mdx'\\n\\n<WhatNext />\", 'synthetic_query': 'Where can I find upcoming details and insights that are mentioned in the document?'}\n",
      "\n",
      "10\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify a question that is specific to the content provided in the document. The document seems to be about a concept called Retrieval Augmented Generation (RAG) and its evaluation. It mentions that RAG is a popular application involving Large Language Models and Vector Databases, and it is used in chatbots and question-answering systems. The document also hints at the importance of evaluating the performance of RAG applications and mentions that the RAG pipeline can be broken down into components, although the components are not listed in the excerpt provided. Therefore, a query that this document would answer should be related to the concept of RAG, its applications, or its evaluation process.\",\n",
      "    query='What is Retrieval Augmented Generation (RAG) and how is it evaluated?'\n",
      ")\n",
      "{'content': '---\\ntitle: An Overview on RAG Evaluation\\nslug: rag-evaluation\\nauthors: [erika, connor]\\ndate: 2023-11-21\\ntags: [concepts]\\nimage: ./img/hero.png\\ndescription: \"Learn about new trends in RAG evaluation and the current state of the art.\"\\n---\\n![hero](img/hero.png)\\n\\n<!-- truncate -->\\n\\nRetrieval Augmented Generation (RAG) is picking up steam as one of the most popular applications of Large Language Models and Vector Databases. RAG is the process of augmenting inputs to a Large Language Model (LLM) with context retrieved from a vector database, like [Weaviate](https://weaviate.io/). RAG applications are commonly used for chatbots and question-answering systems. Like any engineering system, evaluating performance is crucial to the development of RAG applications. The RAG pipeline is broken down into three components: 1.', 'synthetic_query': 'What is Retrieval Augmented Generation (RAG) and how is it evaluated?'}\n",
      "\n",
      "11\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key elements of the document that are unique and informative. The document mentions \"RAG Evaluation,\" which is likely a specific process or methodology, and it also refers to \"using LLMs to produce evaluations\" and the \"state of RAG components.\" These elements suggest that the document provides insights into the evaluation process of RAG using LLMs and the current status of RAG components. Therefore, the query should be focused on seeking information that is specific to the RAG Evaluation process and its interaction with LLMs.',\n",
      "    query='What are the challenges and developments in evaluating RAG using LLMs?'\n",
      ")\n",
      "{'content': 'Indexing, 2. Retrieval, and 3. Generation. RAG Evaluation is tricky because of the series of interacting components and the strain of collecting test data. This article will present an exciting development in using LLMs to produce evaluations and the state of RAG components.', 'synthetic_query': 'What are the challenges and developments in evaluating RAG using LLMs?'}\n",
      "\n",
      "12\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspects of the document that can be used to form a question. The document mentions a conversation with the creators of Ragas, Jithin James and Shauhul Es, on a specific podcast episode. It also talks about the use of LLMs to evaluate RAG systems, the distinction between RAG and Agent systems, and the structure of the blog post which includes 5 major sections with one being LLM Evaluations. Therefore, a question that would be uniquely answered by this document could pertain to the inspiration behind the blog post, the specific podcast mentioned, the evaluation of RAG systems, or the content structure of the blog post.',\n",
      "    query='What inspired the authors to write the blog post about evaluating RAG systems and what podcast episode did they reference?'\n",
      ")\n",
      "{'content': '**TL;DR**: We were inspired to write this blog post from our conversation with the creators of [Ragas](https://docs.ragas.io/en/latest/), Jithin James and Shauhul Es on the [77th Weaviate podcast](https://www.youtube.com/watch?v=C-UQwvO8Koc). These new advances in using LLMs to evaluate RAG systems, pioneered by Ragas and ARES, motivated us to reflect on previous metrics and take inventory of the RAG knobs to tune. Our investigation led us to think further about what RAG experiment tracking software may look like. We also further clarify how we distinguish RAG systems from Agent systems and how to evaluate each. Our blog post has 5 major sections:\\n* [**LLM Evaluations**](#llm-evaluations): New trends in using LLMs to score RAG performance and scales of Zero-Shot, Few-Shot, and Fine-Tuned LLM Evaluators.', 'synthetic_query': 'What inspired the authors to write the blog post about evaluating RAG systems and what podcast episode did they reference?'}\n",
      "\n",
      "13\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information that the document provides. The document seems to be about RAG (Retrieval-Augmented Generation) systems, which are a type of AI model used for tasks like question answering. The document contains sections on metrics for evaluating these systems, the tuning parameters that affect their performance, how to manage experimental configurations, and the transition from RAG systems to Agent systems, including differences in evaluation. Therefore, the query should be about one of these specific aspects.',\n",
      "    query='What are the common metrics used to evaluate the performance of RAG systems in AI?'\n",
      ")\n",
      "{'content': '* [**RAG Metrics**](#rag-metrics): Common metrics used to evaluate Generation, Search, and Indexing and how they interact with each other. * [**RAG: Knobs to Tune**](#rag-knobs-to-tune): What decision makes RAG systems perform significantly different from one another? * [**Orchestrating Tuning**](#tuning-orchestration): How do we manage tracking experimental configurations of RAG systems? * [**From RAG to Agent Evaluation**](#from-rag-to-agent-evaluation): We define RAG as a three step procss of index, retrieve, and generate. This section describes when a RAG system becomes an Agent system and how Agent Evaluation differs.', 'synthetic_query': 'What are the common metrics used to evaluate the performance of RAG systems in AI?'}\n",
      "\n",
      "14\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspect of the document that a question could be based on. The document discusses a specific advancement in machine learning, particularly in the context of LLM evaluations. It mentions the historical context of machine learning being driven by manual data labeling and then introduces a new development that reduces manual effort. The document also references a specific technique called Zero-Shot LLM Evaluations and a framework named Ragas. Therefore, the query should be about this new development in LLM evaluations that is highlighted as \"what\\'s new\" and should also be specific enough to be uniquely answered by the document.',\n",
      "    query='What is the new development in LLM evaluations that is reducing the need for manual data labeling?'\n",
      ")\n",
      "{'content': '## LLM Evaluations\\nLet’s start with the newest and most exciting component of all this, LLM evaluations! The history of machine learning has been heavily driven by the manual labor of labeling data, such as whether a Yelp review is positive or negative, or whether an article about nutritional supplements is related to the query, “Who is the head coach of the Boston Celtics?”. LLMs are becoming highly effective at data annotation with less manual effort. This is the key **“what’s new”** development accelerating the development of RAG applications. The most common technique pioneered by frameworks, like [Ragas](https://docs.ragas.io/en/latest/), are Zero-Shot LLM Evaluations. Zero-Shot LLM Evaluation describes prompting a Large Language Model with a prompt template such as: “Please provide a rating on a scale of 1 to 10 of whether these search results are relevant to the query.', 'synthetic_query': 'What is the new development in LLM evaluations that is reducing the need for manual data labeling?'}\n",
      "\n",
      "15\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify a question that is specific enough to be uniquely answered by the provided document snippet. The document mentions a visualization related to the use of LLMs for evaluating RAG systems and points out three major opportunities for tuning Zero-Shot LLM Evaluation. Therefore, the query should be related to the content of the visualization or the tuning opportunities for Zero-Shot LLM Evaluation.\",\n",
      "    query='What are the three major opportunities for tuning Zero-Shot LLM Evaluation?'\n",
      ")\n",
      "{'content': 'The query is {query}, the search results are {search_results}”. The visualization below shows how an LLM can be used to evaluate the performance of RAG systems. ![RAG-evaluation](img/rag-eval.png)\\n\\nThere are three major opportunities for tuning Zero-Shot LLM Evaluation: 1. the design of the metrics such as precision, recall, or nDCG, 2. the exact language of these prompts, and 3.', 'synthetic_query': 'What are the three major opportunities for tuning Zero-Shot LLM Evaluation?'}\n",
      "\n",
      "16\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information provided in the document that can be asked about. The document gives specific details about the cost of evaluating search results using a language model like GPT-4. It mentions the number of tokens per search result and query, the total number of tokens per call, and the rate per 1K tokens. It also mentions the cost to evaluate a certain number of queries and references the adoption of Zero-Shot LLM Evaluation from frameworks like Ragas. Therefore, a question that would be uniquely answered by this document would pertain to the cost calculation for using GPT-4 to evaluate search results, including the specific token count and rate mentioned.',\n",
      "    query='How much does it cost to evaluate 100 queries using GPT-4, assuming 500 tokens per search result and a rate of $0.005 per 1K tokens?'\n",
      ")\n",
      "{'content': 'the language model used for evaluation, such as GPT-4, Coral, Llama-2, Mistral, and many others. At the time of writing this blog post, people are mainly curious on the cost of evaluation using an LLM. Let’s use GPT-4 as an example to see the cost of evaluating 10 search results, assuming 500 tokens per result and 100 tokens for the query and instructions, totaling roughly 6,000 tokens per LLM call to make the napkin math easier. Then assuming a rate of $0.005 per 1K tokens, this would cost $3 to evaluate 100 queries. The adoption of Zero-Shot LLM Evaluation from frameworks like Ragas is widely spreading.', 'synthetic_query': 'How much does it cost to evaluate 100 queries using GPT-4, assuming 500 tokens per search result and a rate of $0.005 per 1K tokens?'}\n",
      "\n",
      "17\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information provided in the document that a query could be asking for. The document discusses the evaluation of language model systems, specifically mentioning Zero-Shot and Few-Shot LLM Evaluation in the context of RAG (Retrieval-Augmented Generation) systems. It also introduces a scoring system called RAGAS, which includes four metrics: Faithfulness, Answer Relevancy, Context Precision, and Context Recall. The document also briefly describes the transition from Zero-Shot to Few-Shot evaluation by adding labeled examples. Therefore, a query that this document would answer should be about the RAGAS score, the metrics it includes, or the difference between Zero-Shot and Few-Shot LLM Evaluation in the context of RAG systems.',\n",
      "    query='What are the four metrics included in the RAGAS score for evaluating Zero-Shot LLMs in RAG systems?'\n",
      ")\n",
      "{'content': 'This has led to people questioning whether a Few-Shot LLM Evaluation is necessary. Due to its “good enough” status on the tipping scale, Zero-Shot LLM Evaluation may be all that is needed to be a north star for RAG system tuning. Shown below, the RAGAS score is made up of 4 prompts for Zero-Shot LLMs that evaluate the 2 metrics for generation, **Faithfulness** and **Answer Relevancy**, as well as 2 metrics for retrieval, **Context Precision** and **Context Recall**. ![Ragas-score](img/ragas-score.png)\\n[Source](https://docs.ragas.io/en/latest/concepts/metrics/index.html)\\n\\nThe transition from Zero-Shot to Few-Shot LLM Evaluation is quite straightforward. Instead of using just an instruction template, we add a few labeled examples of the relevancy of linked search results to a query.', 'synthetic_query': 'What are the four metrics included in the RAGAS score for evaluating Zero-Shot LLMs in RAG systems?'}\n",
      "\n",
      "18\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique concept or technique that the document is describing. The document mentions \"In-Context Learning\" as a key breakthrough in GPT-3 and discusses the implications of adding examples to the prompt, which increases the cost of evaluation. It also touches on the concept of Knowledge Distillation as a method to reduce costs. Therefore, the query should ask about the technique that involves adding examples to the prompt to improve the performance of a language model like GPT-3, and the potential cost implications of doing so.',\n",
      "    query=\"What is the technique that improves GPT-3's performance by adding examples to the prompt, and how does it affect the cost of evaluation?\"\n",
      ")\n",
      "{'content': 'This is also known as In-Context Learning, and the discovery of this technique was one of the key breakthroughs in GPT-3. For example, adding 5 examples of human relevance ratings, we add 30,000 tokens to the prompt. Assuming the same cost as above, we 5x our cost to evaluate 100 queries from $3 to $15. Note this is a toy example and not based on the real pricing models of LLMs. A key consideration here is that adding few-shot examples may require longer context models, which are currently priced higher than smaller input LLMs.\\n\\nThis is already a very attractive price for producing an LLM Evaluation with Zero-Shot or Few-Shot inference, but further research suggests that the price of LLM evaluation can be further reduced with Knowledge Distillation training algorithms. This describes taking an LLM, using it to generate training data for the task of evaluation and then fine-tuning it into a smaller model.', 'synthetic_query': \"What is the technique that improves GPT-3's performance by adding examples to the prompt, and how does it affect the cost of evaluation?\"}\n",
      "\n",
      "19\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspects of the ARES framework as described in the document. The document mentions specific inputs required for ARES, the process of generating synthetic queries, and the principle of roundtrip consistency used for filtering. It also describes the method for adding weak and strong negatives to the dataset. Therefore, the query should focus on these unique elements of the ARES framework.',\n",
      "    query='What are the inputs and steps involved in the ARES framework for evaluating retrieval-augmented generation systems?'\n",
      ")\n",
      "{'content': 'In [ARES](https://arxiv.org/abs/2311.09476): An Automated Evaluation Framework for Retrieval-Augmented Generation Systems, Saad-Falcon et al. found that training your own LLM evaluator can have a better performance than zero-shot prompting. To begin, “ARES requires three inputs for the pipeline: a set of passages from the target corpus, a human preference validation set of 150 annotated datapoints or more, and five few-shot examples of in-domain queries.” ARES then uses the few-shot examples of queries to generate a large dataset of synthetic queries. These queries are then filtered using the roundtrip consistency principle: Can we retrieve the document that produced the synthetic query when searching with the synthetic query? In addition to the positive chunk that was used to create the synthetic query, ARES adds weak negatives by randomly sampling other chunks from other documents in the corpus and strong negatives by either looking for a chunk in the same document as the one used to produce the query, or if unavailable, using one of the top-10 results from a BM25 query.', 'synthetic_query': 'What are the inputs and steps involved in the ARES framework for evaluating retrieval-augmented generation systems?'}\n",
      "\n",
      "20\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspects of the document that would not be commonly found in other documents. The document discusses a specific system called ARES, which fine-tunes lightweight classifiers for context relevance, answer faithfulness, and answer relevance. It also mentions the use of a specific model, DeBERTa-v3-large, and compares its performance with GPT-3.5-turbo-16k judges. Additionally, it references a paper by Saad-Falcon et al. for more details. Therefore, the query should be about the ARES system, the specific model used for fine-tuning, the comparison with GPT-3.5, or the paper by Saad-Falcon et al.',\n",
      "    query='What system uses DeBERTa-v3-large to fine-tune classifiers for context relevance, answer faithfulness, and answer relevance, and is compared with GPT-3.5-turbo-16k judges?'\n",
      ")\n",
      "{'content': 'Now armed with queries, answers, gold documents, and negatives, ARES fine-tunes lightweight classifiers for **context relevance**, **answer faithfulness**, and **answer relevance**. The authors experiment with fine-tuning [DeBERTa-v3-large](https://huggingface.co/microsoft/deberta-v3-large), which contains a more economical 437 million parameters, with each classifier head sharing the base language model, adding 3 total classification heads. The ARES system is then evaluated by dividing the synthetic data into a train-test split and comparing the fine-tuned judges with zero-shot and few-shot GPT-3.5-turbo-16k judges, finding that the fine-tuned models perform significantly better. For further details, such as a novel use of confidence intervals with prediction powered inference (PPI) and more experimental details, please see the paper from [Saad-Falcon et al](https://arxiv.org/abs/2311.09476). To better understand the potential impact of LLMs for evaluation, we will continue with a tour of the existing methods for benchmarking RAG systems and how they are particularly changed with LLM Evaluation.', 'synthetic_query': 'What system uses DeBERTa-v3-large to fine-tune classifiers for context relevance, answer faithfulness, and answer relevance, and is compared with GPT-3.5-turbo-16k judges?'}\n",
      "\n",
      "21\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspects of the document that would lead to a specific question it can answer. The document discusses RAG metrics, which refers to a process involving generation, retrieval, and indexing. It also mentions the directionality of error propagation within the RAG stack and the typical evaluation methods for RAG, including the use of oracle context or controlled distractors. Additionally, it touches on the evaluation of embeddings and the common practice of using brute force indexing. Therefore, the query should be about the RAG metrics, the error propagation within the RAG stack, or the evaluation methods mentioned.',\n",
      "    query='What is the reason for presenting RAG metrics from a top-down perspective, and how does error propagation affect the different layers of the RAG stack?'\n",
      ")\n",
      "{'content': '## RAG Metrics\\nWe are presenting RAG metrics from a top-down view from generation, to retrieval, and then indexing. We then present the RAG knobs to tune from a bottom-up perspective of building an index, tuning how to retrieve, and then options for generation. Another reason to present RAG Metrics from a top-down view is because errors from Indexing will bubble up to Search and then Generation, but errors in Generation (as we have defined the stack) have no impact on errors in Indexing. In the current state of RAG evaluation, it is uncommon to evaluate the RAG stack end-to-end, rather **oracle context**, or **controlled distractors** (such as the Lost in the Middle experiments) are assumed when determining faithfulness and answer relevancy in generation. Similarly, embeddings are typically evaluated with brute force indexing that doesn’t account for approximate nearest neighbor errors.', 'synthetic_query': 'What is the reason for presenting RAG metrics from a top-down perspective, and how does error propagation affect the different layers of the RAG stack?'}\n",
      "\n",
      "22\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: Approximate nearest neighbor errors are typically measured by finding pareto-optimal points that trade off accuracy with queries per second and recall, ANN recall being the ground truth nearest neighbors to a query, rather than documents labeled as “relevant” to the query. ### Generation Metrics The overall goal of a RAG application is to have a helpful output that uses the retrieved context for support. The evaluation must consider that the output has used the context without directly taking it from the source, avoiding redundant information, as well as preventing incomplete answers. To score the output, there needs to be a metric that covers each criteria. [Ragas](https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics) introduced two scores to measure the performance of an LLM output: faithfulness and answer relevancy.\\n\\nReasoning: Let's think step by step in order to produce the query. We need to identify the unique information provided by the document. The document discusses the measurement of errors in approximate nearest neighbor (ANN) searches and introduces specific metrics used in the evaluation of outputs from a RAG application. The unique aspects are the mention of pareto-optimal points in ANN error measurement, the definition of ANN recall, and the introduction of two specific scores for evaluating LLM outputs in RAG applications: faithfulness and answer relevancy. Therefore, the query should ask about the evaluation of ANN errors or the specific metrics used in RAG applications.\",\n",
      "    query='What metrics are introduced by Ragas to measure the performance of an LLM output in RAG applications?'\n",
      ")\n",
      "{'content': 'Approximate nearest neighbor errors are typically measured by finding pareto-optimal points that trade off accuracy with queries per second and recall, ANN recall being the ground truth nearest neighbors to a query, rather than documents labeled as “relevant” to the query. ### Generation Metrics\\nThe overall goal of a RAG application is to have a helpful output that uses the retrieved context for support. The evaluation must consider that the output has used the context without directly taking it from the source, avoiding redundant information, as well as preventing incomplete answers. To score the output, there needs to be a metric that covers each criteria. [Ragas](https://docs.ragas.io/en/latest/concepts/metrics/index.html#ragas-metrics) introduced two scores to measure the performance of an LLM output: faithfulness and answer relevancy.', 'synthetic_query': 'What metrics are introduced by Ragas to measure the performance of an LLM output in RAG applications?'}\n",
      "\n",
      "23\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information provided in the document that can be formulated into a question. The document discusses two concepts: \"faithfulness\" and \"answer relevance\" in the context of evaluating answers. It explains that faithfulness is about factual correctness based on the retrieved context, while answer relevance is about the relevance of the answer to the question asked. It also provides an example where an answer can be faithful but not relevant. Therefore, the query should focus on distinguishing these two concepts and their relationship.',\n",
      "    query='What is the difference between faithfulness and answer relevance in the context of evaluating answers?'\n",
      ")\n",
      "{'content': '[Faithfulness](https://docs.ragas.io/en/latest/concepts/metrics/faithfulness.html) observes how factually correct the answer is based on the retrieved context. [Answer relevance](https://docs.ragas.io/en/latest/concepts/metrics/answer_relevance.html) determines how relevant the answer is given the question. An answer can have a high faithfulness score, but a low answer relevance score. For example, a faithful response is one that copies the context verbatim, however, that would result in a low answer relevance. The answer relevance score is penalized when an answer lacks completeness or has duplicate information.', 'synthetic_query': 'What is the difference between faithfulness and answer relevance in the context of evaluating answers?'}\n",
      "\n",
      "24\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key elements of the document that are unique and informative. The document mentions a conversational agent named Meena, released by Google in 2020. It also introduces a specific evaluation metric called Sensibleness and Specificity Average (SSA). The purpose of Meena and the SSA metric is to evaluate the performance of open-domain chatbots based on their ability to have sensible and specific conversations. Therefore, the query should focus on asking about the conversational agent Meena, its release year, its goal, or the evaluation metric introduced by Google.',\n",
      "    query='What is the Sensibleness and Specificity Average (SSA) introduced by Google for?'\n",
      ")\n",
      "{'content': 'In 2020, Google released [Meena](https://blog.research.google/2020/01/towards-conversational-agent-that-can.html), a conversational agent. The goal of Meena was to show that it can have **sensible** and **specific** conversations. To measure the performance of the open-domain chatbots, they introduced the Sensibleness and Specificity Average (SSA) evaluation metrics. The bot’s response was measured by its sensibleness, meaning it needed to make sense in context and be specific (specificity average). This ensures the output is comprehensive without being too vague.', 'synthetic_query': 'What is the Sensibleness and Specificity Average (SSA) introduced by Google for?'}\n",
      "\n",
      "25\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspect of the document that a query could be asking about. The document discusses a specific issue related to language model responses, namely \"hallucination,\" and mentions a tool called LlamaIndex that uses a `FaithfulnessEvaluator` metric to measure this. Therefore, the query should be about the concept of hallucination in the context of language models and the specific metric used to evaluate it.',\n",
      "    query='What is hallucination in language models and how is it measured by LlamaIndex?'\n",
      ")\n",
      "{'content': 'Back in 2020, this required humans to have conversations with the chatbot and manually assign these ratings. While it is good to avoid vague responses, it is equally important to avoid the LLM from **hallucinating**. Hallucination refers to the LLM generating a response that is not grounded in actual facts or the provided context. [LlamaIndex](https://docs.llamaindex.ai/en/latest/examples/evaluation/faithfulness_eval.html) measures this with a `FaithfulnessEvaluator` metric. The score is based on whether the response matches the retrieved context.', 'synthetic_query': 'What is hallucination in language models and how is it measured by LlamaIndex?'}\n",
      "\n",
      "26\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the main topic of the document, which is the evaluation of generated responses and the importance of retrieval metrics in that process. The document seems to discuss the criteria for determining the quality of a response, such as factual accuracy and relevance, and then transitions to a specific focus on retrieval metrics. Therefore, the query should be about the evaluation of responses and the role of retrieval metrics in that evaluation.',\n",
      "    query='What factors are important in evaluating the quality of generated responses and what role do retrieval metrics play?'\n",
      ")\n",
      "{'content': 'Determining whether a generated response is good or bad is dependent on a few metrics. You can have answers that are factual, but not relevant to the given query. Additionally, answers can be vague and miss out key contextual information to support the response. We will now go one step back through the pipeline and cover retrieval metrics. ### Retrieval Metrics\\nThe next layer in the evaluation stack is information retrieval.', 'synthetic_query': 'What factors are important in evaluating the quality of generated responses and what role do retrieval metrics play?'}\n",
      "\n",
      "27\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the main topic of the document, which is the process of evaluating retrieval and the challenges associated with annotating documents for relevance. The document also mentions the use of heuristics to reduce the cost of labeling, specifically mentioning click logs. Therefore, the query should be about the evaluation of retrieval, the annotation process, or the use of heuristics like click logs in determining search relevancy.',\n",
      "    query='What are the challenges in annotating documents for search engine relevance, and what heuristic is commonly used to reduce labeling costs?'\n",
      ")\n",
      "{'content': 'The history of evaluating retrieval has required humans to annotate which documents are relevant for a query. So thus, to create 1 query annotation, we may need to annotate the relevance of 100 documents. This is already an immensely difficult task for general search queries, but becomes additionally challenging when building search engines for specific domains such as legal contracts, medical patient history, to give a few examples. To lighten the costs of labeling, heuristics are often used for search relevancy. The most common of which being the click log where: given a query, the title that was clicked on is likely relevant and the others are not.', 'synthetic_query': 'What are the challenges in annotating documents for search engine relevance, and what heuristic is commonly used to reduce labeling costs?'}\n",
      "\n",
      "28\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information provided in the document that can be used to form a question. The document mentions a specific term, \"weak supervision,\" and then goes on to describe three common metrics used for evaluation in machine learning, which are nDCG, Recall, and Precision. It also provides a detailed explanation of what nDCG is and gives an example of how it works with relevance labels. Therefore, the query should be about one of these metrics, preferably nDCG since it is explained in detail.',\n",
      "    query='What is nDCG and how is it used in evaluating machine learning models?'\n",
      ")\n",
      "{'content': 'This is also known as weak supervision in machine learning. Once a dataset has been prepared, there are three common metrics used for evaluation: **nDCG**, **Recall**, and **Precision**. NDCG (Normalized Discounted Cumulative Gain) measures ranking with multiple relevance labels. For example, a document about Vitamin B12 may not be the most relevant result to a query about Vitamin D, but it is more relevant than a document about the Boston Celtics. Due to the additional difficulty of relative ranking, binary relevance labels are often used (1 for relevant, 0 for irrelevant).', 'synthetic_query': 'What is nDCG and how is it used in evaluating machine learning models?'}\n",
      "\n",
      "29\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify a question that directly relates to the definitions and applications of recall and precision in the context of search results. The document provides a clear explanation of what recall and precision are and how they can be calculated using prompts for a language model. Therefore, the query should be about the evaluation of search results using these metrics.',\n",
      "    query='What are recall and precision in the context of search results, and how can they be measured using LLM prompts?'\n",
      ")\n",
      "{'content': 'Recall measures how many of the positives were captured in the search results. Precision then measures how many of the search results are labeled as relevant. LLMs can thus calculate precision with the prompt: “How many of the following search results are relevant to the query {query}? {search_results}”. A proxy measure for recall can also be achieved with an LLM prompt: “Do these search results contain all the needed information to answer the query {query}?', 'synthetic_query': 'What are recall and precision in the context of search results, and how can they be measured using LLM prompts?'}\n",
      "\n",
      "30\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='Document: A document containing information.\\nReasoning: Let\\'s think step by step in order to produce the query. We need to identify a question that is directly answered by the information provided in the document. The document mentions a metric called \"LLM Wins\" used to compare the relevance of two sets of search results based on a query. Therefore, the query should be about how to determine which set of search results is more relevant using the LLM Wins metric.',\n",
      "    query='How can you determine which set of search results is more relevant using the LLM Wins metric?'\n",
      ")\n",
      "{'content': '{search_results}”. We similarly encourage readers to check out some of the prompts available in Ragas [here](https://github.com/explodinggradients/ragas/tree/main/src/ragas/metrics). Another metric worth exploring is LLM Wins, where an LLM is prompted with: “Based on the query {query}, which set of search results are more relevant? Set A {Set_A} or Set B {Set_B}. VERY IMPORTANT! Please restrict your output to “Set A” or “Set B”.', 'synthetic_query': 'How can you determine which set of search results is more relevant using the LLM Wins metric?'}\n",
      "\n",
      "31\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information provided in the document that can be asked about. The document discusses the comparison of vector indexes, mentioning a specific benchmark used in the industry (ANN Benchmarks), a particular version of Weaviate that was inspired by this benchmark (Weaviate 1.19), and the introduction of a gRPC API. It also touches on the evaluation criteria for databases, such as latency and storage cost, and the additional emphasis on accuracy for stochastic vector indexes. Finally, it makes a comparison to SQL select statements and the concept of approximation. Based on this information, we can formulate a query that is uniquely answered by the document.',\n",
      "    query='What inspired the development of the gRPC API in Weaviate 1.19?'\n",
      ")\n",
      "{'content': 'Let’s now dive one layer deeper to understand how vector indexes are compared with one another. ### Indexing Metrics\\nTenured Weaviate users are likely familiar with the [ANN Benchmarks](https://github.com/erikbern/ann-benchmarks/tree/main), which for example inspired the development of the [gRPC API in Weaviate 1.19](https://weaviate.io/blog/weaviate-1-19-release#grpc-api-support-experimental). The ANN Benchmarks measure Queries Per Second versus Recall, with additional nuances on single-threaded restrictions and so on. Databases are typically evaluated based on latency and storage cost, stochastic vector indexes place additional emphasis on accuracy measurement. There is some analog with approximation in [SQL select statements](https://learn.microsoft.com/en-us/sql/t-sql/functions/approx-count-distinct-transact-sql?view=sql-server-ver16), but we predict that error caused by approximation will have an even larger emphasis with the rising popularity of vector indexes.', 'synthetic_query': 'What inspired the development of the gRPC API in Weaviate 1.19?'}\n",
      "\n",
      "32\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspect of the document that a query could be asking about. The document discusses a specific use of \"Recall\" in the context of vector indexing and its impact on search metrics. It contrasts this with the typical use of \"Recall\" in Information Retrieval. The document also poses a question about the relationship between ANN accuracy errors and IR errors, which is specific to the context of a RAG stack. Therefore, the query should be about the impact of ANN recall rates on search metrics within a RAG stack.',\n",
      "    query='What is the impact of different ANN recall rates on search metrics in a RAG stack?'\n",
      ")\n",
      "{'content': 'Accuracy is measured based on Recall. Recall in vector indexing measures how many of the ground truth nearest neighbors determined by brute force are returned from the approximate indexing algorithm. This is distinct from how “Recall” is typically used in Information Retrieval to reference how many of the relevant documents are returned from the search. Both are typically measured with an associated @K parameter. The interesting question in the full context of a RAG stack is then: **When do ANN accuracy errors manifest in IR errors?** For example, we may be able to get 1,000 QPS at 80% recall versus 500 QPS at 95% recall, what is the impact of this on the search metrics presented above such as Search nDCG or an LLM Recall score?', 'synthetic_query': 'What is the impact of different ANN recall rates on search metrics in a RAG stack?'}\n",
      "\n",
      "33\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspects of the document that would not be commonly found in other documents. The document discusses specific metrics for evaluating different aspects of information retrieval systems, such as generation, retrieval, and indexing. It also mentions specific terms like \"Faithfulness,\" \"answer relevance,\" \"hallucinations,\" \"Sensibleness and Specificity Average (SSA),\" \"LLM rated context precision,\" \"context recall,\" \"human labeling,\" \"recall,\" \"precision,\" \"nDCG,\" and \"ANN errors.\" The document poses a key question regarding the relationship between ANN errors and IR errors. Therefore, the query should be focused on these unique elements.',\n",
      "    query='What metrics are used to evaluate the generation, retrieval, and indexing aspects of information retrieval systems, and what is the key question regarding ANN errors?'\n",
      ")\n",
      "{'content': '### Concluding thoughts on RAG Metrics\\nIn conclusion, we have presented metrics used to evaluate indexing, retrieval, and generation:\\n* Generation: Faithfulness and answer relevance, and the evolution from a massive focus on detecting hallucinations and other metrics such as Sensibleness and Specificity Average (SSA). * Retrieval: New opportunities with LLM rated context precision and context recall, as well as an overview of how human labeling has been used to measure recall, precision, and nDCG. * Indexing: Measuring recall as the number of ground truth nearest neighbors returned from the vector search algorithm. We believe the key question here is: *When do ANN errors seep into IR errors*? All components generally have an option to trade-off performance for latency or cost.', 'synthetic_query': 'What metrics are used to evaluate the generation, retrieval, and indexing aspects of information retrieval systems, and what is the key question regarding ANN errors?'}\n",
      "\n",
      "34\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information provided in the document that a query could be asking for. The document discusses the trade-offs between different components of a RAG (Retrieval-Augmented Generation) system, such as language model quality, retrieval filtering, and indexing memory usage. It also mentions the perspective of presenting metrics and the approach to tuning the system. Therefore, the query should be about the trade-offs in RAG system performance, the rationale behind the order of presenting metrics, or the approach to tuning the system from the perspective of an application developer.',\n",
      "    query='What are the trade-offs in improving the performance of a RAG system, and how does the perspective of metrics presentation differ for users and developers?'\n",
      ")\n",
      "{'content': 'We can get higher quality generations with a more expensive language model, we can get higher quality retrieval by filtering results with re-rankers, and we can get higher recall indexing by using more memory. How to manage these trade-offs to improve performance will hopefully become more clear as we continue our investigation into “RAG: Knobs to Tune”. As a quick reminder, we chose to present metrics from a top-down view from Generation to Search and Indexing because evaluation time is closer to the user experience. We will alternatively present the knobs to tune from the bottom-up of Indexing to Retrieval and Generation because this is similar to the experience of the RAG application developer. ## RAG Knobs to Tune\\nNow that we’ve covered the metrics to compare RAG systems, let’s dive further into significant decisions that can alter the performance.', 'synthetic_query': 'What are the trade-offs in improving the performance of a RAG system, and how does the perspective of metrics presentation differ for users and developers?'}\n",
      "\n",
      "35\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key elements of the document that could be the basis for a question. The document mentions \"indexing knobs\" in the context of designing RAG systems, with a focus on \"vector compression settings.\" It specifically talks about the introduction of Product Quantization (PQ) in Weaviate 1.18, which was launched in March 2023. PQ is described as a vector compression algorithm that can significantly reduce memory usage. The document also mentions a specific example of memory reduction and the concept of PQ Rescoring to address recall loss. Based on these elements, we can formulate a question that the document would answer.',\n",
      "    query='What vector compression algorithm was introduced in Weaviate 1.18 to reduce memory usage for RAG systems, and what is its impact on recall?'\n",
      ")\n",
      "{'content': '### Indexing Knobs\\nFor the sake of designing RAG systems, the most important indexing knob looks like vector compression settings. Launched in March 2023, Weaviate 1.18 introduced Product Quantization (PQ). PQ is a vector compression algorithm that groups contiguous segments of a vector, clusters their values across the collection, and then reduces the precision with centroids. For example, a contiguous segment of 4 32-bit floats requires 16 bytes to represent, a segment length of 4 with 8 centroids results in only needing 1 byte, a 16:1 memory reduction. Recent advances in PQ Rescoring help significantly with recall loss from this compression, but is still an important consideration with very high levels of compression.', 'synthetic_query': 'What vector compression algorithm was introduced in Weaviate 1.18 to reduce memory usage for RAG systems, and what is its impact on recall?'}\n",
      "\n",
      "36\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique piece of information that the document provides. The document discusses the use of different indexing methods for RAG (Retriever-Augmented Generation) applications based on the size of the corpora. It mentions that for small corpora (less than 10K vectors), a brute force index may be sufficient, but for larger corpora, Proximity Graph algorithms like HNSW are preferred due to latency issues with brute force. It also touches on how HNSW performance is measured, specifically mentioning the trade-off between queries per second and recall, and the role of the ef parameter. Therefore, the query should ask about the appropriate indexing method for a certain size of corpora in RAG applications and how the performance of a specific Proximity Graph algorithm is measured.',\n",
      "    query='What indexing method is recommended for RAG applications with large corpora, and how is the performance of this method measured?'\n",
      ")\n",
      "{'content': 'The next step is the routing index used. For corpora of less than 10K vectors, RAG applications may be satisfied with a brute force index. However, with increased vectors brute force latency becomes far slower than Proximity Graph algorithms such as HNSW. As mentioned under RAG Metrics, HNSW performance is typically measured as a pareto-optimal point trading off queries per second with recall. This is done by varying the ef, or size of the search queue, used at inference time.', 'synthetic_query': 'What indexing method is recommended for RAG applications with large corpora, and how is the performance of this method measured?'}\n",
      "\n",
      "37\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key elements and concepts mentioned in the document that could be the basis for a question. The document discusses the effect of the parameter `ef` on search performance, mentioning that a larger `ef` results in more distance comparisons and slower searches but with more accurate results. It also mentions parameters related to index building, such as `efConstruction` and `maxConnections`. Furthermore, the document touches on the impact of distribution shift on PQ centroids and the intersection with certain algorithms, suggesting that the Recall metric could be used to determine when to re-fit centroids. Finally, it discusses the potential risk of overfitting if only the most recent vectors are used for re-fitting. Based on this information, we can formulate a query that is uniquely answered by the document.',\n",
      "    query='What is the effect of increasing the `ef` parameter on the performance of a search algorithm?'\n",
      ")\n",
      "{'content': 'A larger ef results in more distance comparisons done during the search, slowing it down significantly although producing a more accurate result. The next parameters to look at are the ones used in index building, efConstruction, the size of the queue when inserting data into the graph, and maxConnections, the number of edges per node, which also must be stored with each vector. Another new direction we are exploring is the impact of distribution shift on PQ centroids and the intersection with hybrid clustering and graph index algorithms such as [DiskANN](https://suhasjs.github.io/files/diskann_neurips19.pdf) or [IVFOADC+G+P](https://openaccess.thecvf.com/content_ECCV_2018/papers/Dmitry_Baranchuk_Revisiting_the_Inverted_ECCV_2018_paper.pdf). Using the Recall metric may be a good enough measure of this to trigger re-fitting the centroids, with the question then being: which subset of vectors to use in re-fitting. If we use the last 100K that may have caused the recall drop, we could risk overfitting to the new distribution, thus we likely want some hybrid sampling of the timeline of our data distribution when inserted into Weaviate.', 'synthetic_query': 'What is the effect of increasing the `ef` parameter on the performance of a search algorithm?'}\n",
      "\n",
      "38\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspect of the document that can be inquired about. The document discusses a specific process related to Deep Learning model optimization, which is the chunking of data before inserting it into Weaviate. It mentions that chunking is done to enhance retrieval and to stay within the token limit of LLMs (Large Language Models). The document also hints at the existence of various strategies to parse documents, which is likely detailed in the document. Therefore, the query should focus on the process of chunking data for Weaviate and its benefits.',\n",
      "    query='What is the purpose of chunking data before inserting it into Weaviate, and how does it improve retrieval?'\n",
      ")\n",
      "{'content': 'This topic is heavily related to our perspectives on continual optimization of Deep Learning models, discussed further in “Orchestrating Tuning”. Chunking your data is an important step before inserting your data into Weaviate. Chunking takes long documents and converts it into smaller sections. This enhances the retrieval since each chunk has an important nugget of information and this helps to stay within the LLMs token limit. There are quite a few strategies to parse documents.', 'synthetic_query': 'What is the purpose of chunking data before inserting it into Weaviate, and how does it improve retrieval?'}\n",
      "\n",
      "39\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify the unique aspect of the document which is the process of breaking down a research paper into chunks and the method of overlapping these chunks to improve search functionality. The document specifically mentions the use of a rolling window to create overlap between chunks for better context understanding. Therefore, the query should be focused on this unique chunking and overlapping process described in the document.\",\n",
      "    query='How does the rolling window method improve search functionality in the context of chunking a research paper?'\n",
      ")\n",
      "{'content': 'The above visual illustrates converting a research paper into chunks based on the heading. For example, chunk 1 is the abstract, chunk 2 is the introduction, and so on. Additionally, there are methods to combine chunks and have an overlap. Including a rolling window takes tokens from the previous chunk and begins the next chunk with it. The slight overlap of chunks can improve the search since the retriever will understand the previous context/chunk.', 'synthetic_query': 'How does the rolling window method improve search functionality in the context of chunking a research paper?'}\n",
      "\n",
      "40\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify a unique piece of information that the document provides. The document discusses the process of Retrieval in the context of RAG (Retrieval-Augmented Generation) development, mentioning specific aspects such as embedding models, hybrid search weighting, AutoCut, and re-ranker models. It also lists examples of embedding models and introduces the concept of the `alpha` parameter in hybrid search weighting. Therefore, a query that this document would answer should be related to one of these unique points.',\n",
      "    query='What are the four major knobs to tune in Retrieval for RAG development?'\n",
      ")\n",
      "{'content': 'The following image presents a high-level illustration of chunking text. ![chunking](img/chunk.png)\\n\\n### Retrieval \\nThere are four major knobs to tune in Retrieval: Embedding models, Hybrid search weighting, whether to use AutoCut, and Re-ranker models. Most RAG developers may instantly jump to tuning the embedding model used, such as OpenAI, Cohere, Voyager, Jina AI, Sentence Transformers, and many others! Developers also need to consider the dimensionality of the models and how it affects the PQ compression. The next key decision is how to weight the aggregation of sparse and dense retrieval methods in Hybrid Search. The weighting is based on the `alpha` parameter.', 'synthetic_query': 'What are the four major knobs to tune in Retrieval for RAG development?'}\n",
      "\n",
      "41\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key pieces of information that the document provides. The document mentions a parameter `alpha` that adjusts the balance between bm25 and vector search, and it also references two specific re-ranking models offered by Weaviate from Cohere. The document seems to be discussing search algorithms and their customization, as well as providing information about available re-ranking models that can be used with Weaviate. Therefore, a query that this document would answer should be related to the customization of search algorithms in Weaviate or the specific re-ranking models available.',\n",
      "    query='What are the two re-ranking models from Cohere that Weaviate offers for search customization?'\n",
      ")\n",
      "{'content': 'An `alpha` of 0 is pure bm25 and an alpha of 1 is pure vector search. Therefore, the set `alpha` is dependent on your data and application. Another emerging development is the effectiveness of zero-shot re-ranking models. Weaviate currently offers 2 [re-ranking models from Cohere](https://weaviate.io/developers/weaviate/modules/retriever-vectorizer-modules/reranker-cohere): `rerank-english-v2.0` and `rerank-multilingual-v2.0`. As evidenced from the name, these models mostly differ because of the training data used and the resulting multilingual capabilities.', 'synthetic_query': 'What are the two re-ranking models from Cohere that Weaviate offers for search customization?'}\n",
      "\n",
      "42\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the main topics discussed in the document. The document mentions the future expectations regarding the optionality of a model, the trade-offs between performance and latency, the challenge of determining the capacity re-ranker and the number of results to re-rank, the opportunity for fine-tuning custom models in the RAG stack, and the concept of Multi-Index Search. The document seems to be focused on the challenges and opportunities in fine-tuning retrieval systems, specifically in the context of a RAG (Retrieval-Augmented Generation) stack. Therefore, the query should be about one of these topics, and it should be specific enough to be uniquely answered by the document.',\n",
      "    query='What are some of the challenges and opportunities in fine-tuning retrieval systems in the RAG stack?'\n",
      ")\n",
      "{'content': 'In the future, we expect further optionality ablating the capacity of the model due to inherent trade-offs of performance and latency that may make sense for some applications but not others. Discovering jointly which capacity re-ranker is needed and how many retrieved results to re-rank is another challenge for tuning the knobs in retrieval. This is also one of the lowest hanging fruit opportunities for fine-tuning custom models in the RAG stack, which we will discuss further in “Tuning Orchestration”. Another interesting knob to tune is Multi-Index Search. Similar to our discussion on chunking, this is a tricky one that may involve structural changes to the database.', 'synthetic_query': 'What are some of the challenges and opportunities in fine-tuning retrieval systems in the RAG stack?'}\n",
      "\n",
      "43\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspect of the document that a query would need to address. The document discusses the decision-making process regarding the organization of data into separate collections or using filters within a single collection. It specifically mentions the use of tags for filtering and the potential for context fusion in LLMs (Large Language Models) as they process longer inputs. The document also hints at a consideration for how many documents to retrieve from each index or filter. Therefore, the query should focus on the decision criteria for data organization in the context of LLMs and context fusion.',\n",
      "    query='When should separate collections be used instead of filters for organizing data in the context of LLMs and context fusion?'\n",
      ")\n",
      "{'content': 'Broadly there is the question of: **When does it make sense to use separate collections instead of filters?** Should `blogs` and `documentation` be separated into two collections or jointly housed in a `Document` class with a `source` property? ![multi-index](img/multi-index.png)\\n\\nUsing filters gives us a quick way to test the utility of these labels, because we can add more than one tag to each chunk and then ablate how well the classifiers use the labels. There are many interesting ideas here such as explicitly annotating where the context came from in the input to the LLM, such as “Here are search results from blogs {search_results}. Here are search results from documentation {documentation}”. As LLMs are able to process longer inputs, we expect that context fusion between multiple data sources will become more popular and thus, an associated hyperparameter emerges of how many documents to retrieve from each index or filter.', 'synthetic_query': 'When should separate collections be used instead of filters for organizing data in the context of LLMs and context fusion?'}\n",
      "\n",
      "44\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify the key elements of the document that could be the basis for a question. The document mentions different providers of language learning models (LLMs) such as OpenAI, Cohere, Facebook, and open-source options. It also references frameworks that offer easy integrations with these models, such as LangChain, LlamaIndex, and Weaviate’s generate module. Additionally, the document touches on factors that might influence the choice of a model, like data privacy, cost, and resources, and mentions a specific feature of LLMs, which is the temperature setting. Therefore, a query that this document would answer should be related to the selection of LLMs, their integration frameworks, or the factors influencing the choice of a model.\",\n",
      "    query='What are some frameworks that provide easy integration with various language learning models?'\n",
      ")\n",
      "{'content': '### Generation\\nWhen it comes to Generation, the obvious first place to look is the choice of LLM. For example, you have options from OpenAI, Cohere, Facebook, and many open-source options. It is also helpful that many LLM frameworks like [LangChain](https://www.langchain.com/) and [LlamaIndex](https://www.llamaindex.ai/), and [Weaviate’s generate module](/developers/weaviate/modules/reader-generator-modules/generative-openai) offer easy integrations into various models. The model that you choose can be dependent on whether you want to keep your data private, the cost, resources, and more. A common LLM specific knob that you can tune is the temperature.', 'synthetic_query': 'What are some frameworks that provide easy integration with various language learning models?'}\n",
      "\n",
      "45\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"produce the query. We need to identify the unique piece of information that the document provides. The document explains the function of the temperature setting in a generative model, specifically how it affects the randomness and predictability of the model's responses. It also mentions that a temperature of 1 allows for more variability in the responses upon multiple runs of the model. Additionally, it briefly touches on the relevance of long context models. Therefore, the query should be focused on the impact of the temperature setting on the model's output.\",\n",
      "    query=\"How does the temperature setting affect the variability of a generative model's responses?\"\n",
      ")\n",
      "{'content': 'The temperature setting controls the amount of randomness in the output. A temperature of 0 means that the response is more predictable and will vary less. A temperature of 1 gives the model the ability to introduce randomness and creativity into its responses. Therefore, if you’re running the generative model more than once and it has a temperature of 1, the responses can vary after each rerun. Long context models are an emerging direction for choosing the LLM for your application.', 'synthetic_query': \"How does the temperature setting affect the variability of a generative model's responses?\"}\n",
      "\n",
      "46\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the main topic of the document, which is the impact of search result placement on the quality of answers generated by language models. The document references two studies that provide evidence on how the positioning of relevant information in search results affects language model performance. The first study, \"Lost in the Middle,\" suggests that placing relevant information in the middle of search results hinders the model\\'s ability to integrate it into the response. The second study highlights that language models can be distracted by irrelevant context. The document also mentions the Ragas score, which is a metric for testing new systems, and refers to stages of tuning for generation. Based on this information, we can formulate a query that the document would answer.',\n",
      "    query='Do language models perform worse when relevant information is placed in the middle of search results?'\n",
      ")\n",
      "{'content': 'Does adding more search results to the input improve answer quality? The Lost in the Middle experiments have tempered expectations here a bit. In [Lost in the Middle](https://arxiv.org/abs/2307.03172), researchers from Stanford University, UC Berkeley, and Samaya AI presented controlled experiments showing that if relevant information was placed in the middle of search results, rather than the beginning or end, the language model would be unable to integrate it in the generated response. Another paper from researchers at Google DeepMind, Toyota, and Purdue University showed that [“Large Language Models Can Be Easily Distracted by Irrelevant Contex.”](https://arxiv.org/abs/2302.00093) Although the potential is captivating, at the time of this writing, it seems early on for Long-Context RAG. Luckily, metrics such as the Ragas score are here to help us quickly test the new systems!\\n\\nSimilar to our earlier discussion on recent breakthroughs in LLM Evaluation, there are 3 stages of tuning for generation: 1.', 'synthetic_query': 'Do language models perform worse when relevant information is placed in the middle of search results?'}\n",
      "\n",
      "47\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspect of the document, which is the explanation of different methods to guide AI behavior in language tasks. The document specifically mentions \"prompt tuning\" and contrasts it with \"few-shot examples\" and \"fine-tuning.\" It also provides an example of how the language can be tweaked in prompt tuning to achieve different results. Therefore, the query should focus on the concept of prompt tuning and how it differs from other methods in guiding AI responses.',\n",
      "    query='What is prompt tuning in AI language tasks and how does it differ from few-shot examples and fine-tuning?'\n",
      ")\n",
      "{'content': 'Prompt tuning, 2. Few-Shot Examples, and 3. Fine-Tuning. Prompt tuning entails tweaking the particular language used such as: “Please answer the question based on the provided search results.” versus “Please answer the question. IMPORTANT, please follow these instructions closely.', 'synthetic_query': 'What is prompt tuning in AI language tasks and how does it differ from few-shot examples and fine-tuning?'}\n",
      "\n",
      "48\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='Document: A document containing information.\\nReasoning: Let\\'s think step by step in order to produce the query. We need to identify the key elements of the document that would be relevant to a query. The document mentions \"Few-Shot Examples,\" \"In-Context Vectors,\" \"GPT-3.5-turbo,\" \"Weaviate queries,\" \"Weaviate Gorilla project,\" and \"fine-tuning LLMs for RAG applications.\" These elements suggest that the document is discussing the use of language models, specifically GPT-3.5-turbo, in generating queries for a project named Weaviate Gorilla, and how the performance improved with the use of few-shot examples. It also references a research paper on \"In-Context Vectors\" and talks about fine-tuning language models for Retrieval-Augmented Generation (RAG) applications. Therefore, a query that this document would answer should be related to these topics.',\n",
      "    query='What impact did few-shot examples have on the performance of GPT-3.5-turbo in the Weaviate Gorilla project?'\n",
      ")\n",
      "{'content': 'Your answer to the question must be grounded in the provided search results and nothing else!!”. As described earlier, Few-Shot Examples describes collecting a few manually written examples of question, context, answer pairs to guide the language model’s generation. Recent research such as [“In-Context Vectors”](https://arxiv.org/abs/2311.06668) are further pointing to the importance of guiding latent space like this. We were using GPT-3.5-turbo to generate Weaviate queries in the Weaviate Gorilla project and performance skyrocketed once we added few-shot examples of natural language to query translations. Lastly, there is increasing interest in fine-tuning LLMs for RAG applications.', 'synthetic_query': 'What impact did few-shot examples have on the performance of GPT-3.5-turbo in the Weaviate Gorilla project?'}\n",
      "\n",
      "49\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify the unique aspects of the document that would lead to a specific question. The document mentions the use of a more powerful LLM to generate training data for a smaller model, the possibility of human annotations for fine-tuning an LLM, a tutorial from Brev on using the HuggingFace PEFT library, and the main knobs to tune in RAG systems, specifically indexing. The query should be focused on one of these unique aspects to ensure that the document provides a clear answer.\",\n",
      "    query='What are some strategies for fine-tuning language models mentioned in the document?'\n",
      ")\n",
      "{'content': 'There are a couple of flavors to consider with this. Again reminiscent of our discussion of LLM Evaluation, we may want to use a more powerful LLM to generate the training data to produce a smaller, more economical model owned by you. Another idea could be to provide human annotations of response quality to fine-tune an LLM with instruction following. If you’re interested in fine-tuning models, check out this [tutorial](https://brev.dev/blog/fine-tuning-mistral) from Brev on how to use the HuggingFace PEFT library. ### Concluding thoughts on RAG Knobs to Tune\\nIn conclusion, we have presented the main knobs to tune in RAG systems:\\n\\n* Indexing: At the highest level, we consider when to just use brute force and when to bring in an ANN index.', 'synthetic_query': 'What are some strategies for fine-tuning language models mentioned in the document?'}\n",
      "\n",
      "50\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: This is especially interesting when tuning a multi-tenant use case with new versus power users. Within ANN indexing, we have PQ’s hyperparameters of (segments, centroids, and the training limit). HNSW comes with (ef, efConstruction, and maxConnections). * Retrieval: Choosing an embedding model, weighting hybrid search, choosing a re-ranker, and dividing collections into multiple indexes. * Generation: Choosing an LLM and when to make the transition from Prompt Tuning to Few-Shot Examples, or Fine-Tuning.\\nReasoning: Let's think step by step in order to identify the unique aspects of the document that can be queried. The document mentions specific hyperparameters for ANN indexing, such as PQ and HNSW, and also discusses aspects of retrieval and generation in the context of multi-tenant use cases. It specifically mentions the decision-making process in choosing an embedding model, a re-ranker, and when to transition from Prompt Tuning to Few-Shot Examples or Fine-Tuning in the context of using a Large Language Model (LLM). Therefore, a query that this document would answer should be related to these specific topics.\",\n",
      "    query='What are the hyperparameters associated with PQ and HNSW in ANN indexing, and what considerations are involved in retrieval and generation for multi-tenant use cases?'\n",
      ")\n",
      "{'content': 'This is especially interesting when tuning a multi-tenant use case with new versus power users. Within ANN indexing, we have PQ’s hyperparameters of (segments, centroids, and the training limit). HNSW comes with (ef, efConstruction, and maxConnections). * Retrieval: Choosing an embedding model, weighting hybrid search, choosing a re-ranker, and dividing collections into multiple indexes. * Generation: Choosing an LLM and when to make the transition from Prompt Tuning to Few-Shot Examples, or Fine-Tuning.', 'synthetic_query': 'What are the hyperparameters associated with PQ and HNSW in ANN indexing, and what considerations are involved in retrieval and generation for multi-tenant use cases?'}\n",
      "\n",
      "51\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information that the document provides. The document mentions RAG metrics, tuning orchestration, LLM Evaluation, and experiment tracking frameworks. It also talks about a specific example of an orchestrator with an intuitive API that allows a human user to request tests of various models and configurations. The query should be about something that only this document can answer, which seems to be related to the capabilities of the orchestrator in the context of LLM evaluation and experiment tracking.',\n",
      "    query='What is an example of how an orchestrator with an intuitive API can be used in LLM evaluation and experiment tracking?'\n",
      ")\n",
      "{'content': 'Armed with an understanding of RAG metrics and what we can tune to improve them. Let’s discuss what experiment tracking may look like. ## Tuning Orchestration\\nGiven the recent advances in LLM Evaluation and an overview of some of the knobs to tune, one of the most exciting opportunities is to tie all this together with experiment tracking frameworks. For example, a simple orchestrator that has an intuitive API for a human user to 1. request an exhaustive test of say: 5 LLMs, 2 embedding models, and 5 index configurations, 2.', 'synthetic_query': 'What is an example of how an orchestrator with an intuitive API can be used in LLM evaluation and experiment tracking?'}\n",
      "\n",
      "52\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key elements of the document that could lead to a specific question. The document mentions \"Weights & Biases,\" which is a tool for experiment tracking in the context of training deep learning models. It also refers to \"RAG experimentation\" and mentions \"knobs and metrics\" outlined in an article. Additionally, it talks about the expectation of increased interest in this kind of support and mentions that there are a couple of directions being watched for evolution. Therefore, the query should be about the specific tool or concept discussed in the document that is related to deep learning experiment tracking and RAG experimentation.',\n",
      "    query='What tool is mentioned for tracking deep learning model training experiments, and what type of experimentation does it support?'\n",
      ")\n",
      "{'content': 'run the experiments, and 3. return a high quality report to the human user. Weights & Biases has paved an incredible experiment tracking path for training deep learning models. We expect interest to accelerate in this kind of support for RAG experimentation with the knobs and metrics we have outlined in this article. There are a couple of directions we are watching this evolve in.', 'synthetic_query': 'What tool is mentioned for tracking deep learning model training experiments, and what type of experimentation does it support?'}\n",
      "\n",
      "53\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information provided in the document that a query could be asking for. The document discusses the performance of Zero-Shot LLMs with oracle context and the opportunity to focus on the search aspect of these models. It mentions specific technologies and features such as ANN error trade-offs, embedding models, hybrid search weightings, re-ranking, Async Indexing, node status APIs, RAG evaluation, and tuning orchestration. It also references Weaviate 1.22 and its new features. The query should be about something specific that only this document can answer, such as the new feature introduced in Weaviate 1.22 or the focus of partnerships in relation to RAG evaluation and tuning orchestration.',\n",
      "    query='What new feature does Weaviate 1.22 introduce to assist with RAG evaluation and tuning orchestration?'\n",
      ")\n",
      "{'content': 'One on hand, the Zero-Shot LLMs out there such as GPT-4, Command, Claude, and open-source options such as Llama-2 and Mistral perform fairly well when they have **oracle context**. Thus, there is a massive opportunity to **focus solely on the search half**. This requires finding the needle in the haystack of configurations that trade-off ANN error from PQ or HNSW trade-offs, with embedding models, hybrid search weightings, and re-ranking as described earlier in the article. Weaviate 1.22 introduces Async Indexing with corresponding node status APIs. Our hope with partnerships focusing on RAG evaluation and tuning orchestration, is that they can use these APIs to see when an index is finished building and then run the test. This is particularly exciting when considering interfaces between these node statuses and tuning orchestration per tenant where some tenants may get away with brute force, and others need to find the right embedding model and HNSW configuration for their data.', 'synthetic_query': 'What new feature does Weaviate 1.22 introduce to assist with RAG evaluation and tuning orchestration?'}\n",
      "\n",
      "54\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify the unique aspects of the document that would prompt a specific question. The document mentions a process of speeding up testing by parallelizing resource allocation and specifically talks about evaluating embedding models. It also discusses tuning chunking and symbolic metadata from a data importer, using the Weaviate Verba dataset as an example. The dataset includes `Blogs`, `Documentation`, and `Video` transcripts, and there is a mention of ablating chunk sizes of 100 versus 300 without re-invoking the web scraper. Therefore, the query should be about one of these unique aspects.\",\n",
      "    query='What dataset contains `Blogs`, `Documentation`, and `Video` transcripts used for evaluating embedding models and tuning chunk sizes?'\n",
      ")\n",
      "{'content': 'Further, we may want to speed up testing by parallelizing resource allocation. For example, evaluating 4 embedding models at the same time. As discussed earlier, another interesting component to this is tuning chunking or other symbolic metadata that may come from our data importer. To paint the picture with an example, the Weaviate Verba dataset contains 3 folders of Weaviate `Blogs`, `Documentation`, and `Video` transcripts. If we want to ablate chunk sizes of 100 versus 300, it probably doesn’t make sense to re-invoke the web scraper.', 'synthetic_query': 'What dataset contains `Blogs`, `Documentation`, and `Video` transcripts used for evaluating embedding models and tuning chunk sizes?'}\n",
      "\n",
      "55\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key points in the document that could lead to a specific question. The document mentions a more economical way to experiment with data storage and associated metadata, possibly in an S3 bucket. It also talks about model fine-tuning and continual learning with gradients, rather than data inserts or updates, and mentions the use of embeddings, re-rankers, and LLMs (Large Language Models) in RAG (Retrieval-Augmented Generation) systems. The focus on keeping machine learning models fresh with new data through continual learning frameworks and MLops orchestration is highlighted. Finally, the document raises the question of whether LLMs can directly extend their knowledge base cut-off date to stay up-to-date with new data. Based on this, we can formulate a query that the document would answer.',\n",
      "    query='Can Large Language Models directly extend their knowledge base to stay current with new data in RAG systems?'\n",
      ")\n",
      "{'content': 'We instead may want to have another format, whether that be data stored in an S3 bucket or something else, that has the associated metadata with it, but provides a more economical way to experiment with this. On the other hand, we have model fine-tuning and continual learning with gradients, rather than data inserts or updates. The most common models used in RAG are embeddings, re-rankers, and of course, LLMs. Keeping machine learning models fresh with new data has been a longstanding focus of continual learning frameworks and MLops orchestration that manage the re-training and testing and deployment of new models. Starting with continual learning of LLMs, one of the biggest selling points of RAG systems is the ability to extend the “cut-off” date of the LLM’s knowledge base, keeping it up to date with your data. Can the LLM do this directly?', 'synthetic_query': 'Can Large Language Models directly extend their knowledge base to stay current with new data in RAG systems?'}\n",
      "\n",
      "56\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspects of the document that a query could be asking about. The document discusses the uncertainty of interaction effects between continued training and information freshness in the context of RAG (Retrieval-Augmented Generation). It also mentions a specific research method, MEMIT, and its use of causal mediation analysis for updating facts. Additionally, it talks about an alternative approach involving tagging and re-training with updated information. Finally, it references a potential integration with Weaviate using PQ centroids. Considering these points, we can formulate a query that asks about the specific research and methods mentioned in the document for updating information in a machine learning context.',\n",
      "    query='What are the methods being considered for updating factual information in retrieval-augmented generation models, and how might they be integrated with Weaviate?'\n",
      ")\n",
      "{'content': 'We don’t believe it is clear what the interaction effect will be between continued training and keeping information fresh purely with RAG. Some research, such as MEMIT, experiments with updating facts such as “LeBron James plays basketball” to “LeBron James plays soccer” purely using causal mediation analysis of weight attribution. This is quite an advanced technique, and another opportunity could be simply tagging the chunks used in training such as “LeBron James plays basketball” and re-training with retrieval-augmented training data with the new information. This is a major area we are keeping an eye on. As mentioned earlier, we are also thinking of how to interface this kind of continual tuning directly into Weaviate with PQ centroids.', 'synthetic_query': 'What are the methods being considered for updating factual information in retrieval-augmented generation models, and how might they be integrated with Weaviate?'}\n",
      "\n",
      "57\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key concepts and issues discussed in the document. The document mentions the impact of data distribution shifts on PQ centroids in the context of Weaviate, a vector search engine. It also discusses the problem of catastrophic forgetting in machine learning models and the consideration of this issue in the design of re-fitting PQ centroids. Additionally, the document differentiates between RAG (Retrieval-Augmented Generation) and Agent Evaluation, defining RAG by a specific flow and suggesting that Agent Evaluation has a broader scope. Therefore, the query should be about one of these specific topics.',\n",
      "    query='What is the impact of data distribution shifts on the fitting of PQ centroids in Weaviate, and how does it relate to catastrophic forgetting in machine learning?'\n",
      ")\n",
      "{'content': 'The PQ centroids fit with the first K vectors that enter Weaviate may be impacted by a significant shift in the data distribution. The continual training of machine learning models has a notorious “catastrophic forgetting” problem where training on the newest batch of data harms performance on earlier batches of data. This is also something that we are considering with the design of re-fitting PQ centroids. ## From RAG to Agent Evaluation\\nThroughout the article, we have been concerned with **RAG**, rather than **Agent** Evaluation. In our view **RAG** is defined by the flow of Index, Retrieve, and Generate, whereas **Agents** have a more open-ended scope.', 'synthetic_query': 'What is the impact of data distribution shifts on the fitting of PQ centroids in Weaviate, and how does it relate to catastrophic forgetting in machine learning?'}\n",
      "\n",
      "58\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify a question that is specific to the content of the document provided. The document mentions several components (Planning, Memory, Tools) that add power to a system, the use of advanced query engines in RAG applications, and a reference to a resource for beginners (Episode 3 of the LlamaIndex and Weaviate series). It also lists examples of advanced query engines and hints at future considerations for Weaviate’s modules. Therefore, the query should be related to these topics.',\n",
      "    query='What are some examples of advanced query engines mentioned for RAG applications?'\n",
      ")\n",
      "{'content': 'The illustration below denotes how we see major components such as Planning, Memory, and Tools that jointly add significant power to your system, but also make it more difficult to evaluate. <img\\n  src={require(\\'./img/agents.png\\').default}\\n  alt=\"Agents meme\"\\n  style={{ maxWidth: \"60%\" }}\\n/>\\n\\n\\nA common next step for RAG applications is to add advanced query engines. For interested readers new to the concept, please check out [Episode 3](https://www.youtube.com/watch?v=Su-ROQMaiaw) of our LlamaIndex and Weaviate series that provides python code examples of how to get started. There are many different advanced query engines, such as the Sub-Question Query Engine, SQL Router, Self-Correcting Query Engine, and more. We are also considering how a promptToQuery API or Search Query Extractor could look like in Weaviate’s modules.', 'synthetic_query': 'What are some examples of advanced query engines mentioned for RAG applications?'}\n",
      "\n",
      "59\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspect of the document that a query would need to address. The document discusses the strengths of multi-hop query engines, specifically in breaking down complex questions into sub-questions. It also provides an example of how a multi-hop query engine would handle a specific query about \"Ref2Vec in Weaviate\" by making two separate calls to a database. Therefore, the query should be about the functionality of multi-hop query engines and their approach to handling complex questions.',\n",
      "    query='How do multi-hop query engines handle complex questions?'\n",
      ")\n",
      "{'content': 'Each query engine has its own strengths in the information retrieval process, let’s dive into a couple of them and how we might think about evaluation. <img\\n  src={require(\\'./img/sub-question.png\\').default}\\n  alt=\"Sub Question Query Engine\"\\n  style={{ maxWidth: \"60%\" }}\\n/>\\n\\nMulti-hop query engines (otherwise known as [sub question query engine](https://gpt-index.readthedocs.io/en/latest/examples/query_engine/sub_question_query_engine.html)) are great at breaking down complex questions into sub-questions. In the visual above, we have the query “What is Ref2Vec in Weaviate?” To answer this question, you need to know what Ref2Vec and Weaviate are separately. Therefore, two calls will need to be made to your database to retrieve relevant context based on the two questions. The two answers are then combined to generate one output.', 'synthetic_query': 'How do multi-hop query engines handle complex questions?'}\n",
      "\n",
      "60\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify a question that would require the evaluation of a multi-hop query engine's performance. This involves considering a scenario where the engine must create relevant sub-questions, answer each accurately, and combine the answers to provide a factual and relevant output. The document suggests that complex questions are best suited for a multi-hop query engine, implying that the query we produce should be complex enough to necessitate breaking it down into sub-questions. Therefore, we should formulate a complex question that would require multiple pieces of information to answer.\",\n",
      "    query='What is a multi-hop query engine and how is its performance evaluated?'\n",
      ")\n",
      "{'content': 'Evaluating the performance of multi-hop query engines can be done by observing the sub questions. It is important that the LLM is creating relevant sub questions, answering each accurately, and combining the two answers to provide a factual and relevant output. Additionally, if you’re asking complex questions, it is probably best to utilize the multi-hop query engine. A multi-hop question is firstly dependent on the accuracy of the sub-questions. We can imagine a similar LLM Evaluation used here with the prompt: “Given the question: {query}.', 'synthetic_query': 'What is a multi-hop query engine and how is its performance evaluated?'}\n",
      "\n",
      "61\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to understand how a system processes complex queries. The document describes a system that breaks down a complex query into sub-questions, evaluates them separately, and then combines the answers to address the original query. It also mentions the use of agents to route queries to the appropriate database type, such as SQL or Vector Database, based on the nature of the query. To create a query that this document would answer, we need to focus on the system's method of query decomposition and routing.\",\n",
      "    query='How does a system decompose complex queries for evaluation and route them to the appropriate database?'\n",
      ")\n",
      "{'content': 'A system decided to break it into the sub questions {sub_question_1} and {sub_question_2}. Does this decomposition of the question make sense?”. We then have two separate RAG evaluations for each of the sub questions, and then an evaluation of whether the LLM was able to combine the answers from each question to answer the original question. As another example of evolving complexity from RAG to Agents, let’s consider Routing Query Engines. The following visual illustrates an agent routing a query to either an SQL or Vector Database query.', 'synthetic_query': 'How does a system decompose complex queries for evaluation and route them to the appropriate database?'}\n",
      "\n",
      "62\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspects of the document that a query could target. The document mentions specific concepts such as Multi-Index Routing, SQL and Vector Databases, RAGAS Context Relevance score, and references to \"From RAG to Agent Evaluation\". It also discusses the complexity of evaluating agents, particularly with the introduction of open-ended planning loops, tool use, API request formatting, and memory management prompts like those in MemGPT. Therefore, the query should focus on these unique elements to ensure that the document provides a specific answer.',\n",
      "    query='What are the considerations for evaluating agents in the context of Multi-Index Routing and SQL and Vector Databases?'\n",
      ")\n",
      "{'content': 'This case is quite similar to our discussion of Multi-Index Routing and we can similarly evaluate generations with a prompt that explains the needs for SQL and Vector Databases and then asks the LLM whether the router made the right decision. We can also use the RAGAS Context Relevance score for the results of the SQL query. <img\\n  src={require(\\'./img/sql-router.png\\').default}\\n  alt=\"SQL Router Query Engine\"\\n  style={{ maxWidth: \"60%\" }}\\n/>\\n\\nConcluding our discussion of “From RAG to Agent Evaluation”, we believe that it is still too early to tell what the common patterns will be for agent use. We have intentionally shown the multi-hop query engine and query router because these are relatively straightforward to understand. Once we add more open-ended planning loops, tool use and the associated evaluation of how well the model can format API requests to the tool, and more meta internal memory management prompts such as the ideas in MemGPT, it is very difficult to provide a general abstraction around how Agents will be evaluated.', 'synthetic_query': 'What are the considerations for evaluating agents in the context of Multi-Index Routing and SQL and Vector Databases?'}\n",
      "\n",
      "63\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspects of the document that can be used to formulate a question. The document mentions a specific topic, which is RAG Evaluation, and it discusses trends in using LLMs (Large Language Models) for this purpose. It also talks about traditional metrics for evaluating the RAG stack, which includes Generation, Search, and Indexing. Additionally, the document mentions tuning knobs for improving performance and the challenge of experiment tracking. Finally, it differentiates between RAG evaluation and Agent evaluation. With these points in mind, we can create a query that asks about the content of the document.',\n",
      "    query='What does the document say about using LLMs for RAG Evaluation and the traditional metrics involved?'\n",
      ")\n",
      "{'content': \"## Conclusion\\nThank you so much for reading our overview of RAG Evaluation! As a quick recap, we began by covering new trends in using LLMs for Evaluation, providing enormous cost and time savings for iterating on RAG systems. We then provided some more background on the traditional metrics used to evaluate the RAG stack from Generation, to Search, and then Indexing. For builders looking to improve their performance on these metrics, we then presented some knobs to tune in Indexing, then Search, and Generation. We presented the incoming challenge of experiment tracking for these systems, and our view on what differentiates RAG evaluation from Agent evaluation. We hope you found this article useful! We would love to connect on **X** at [@ecardenas300](https://twitter.com/ecardenas300) and [@CShorten30](https://twitter.com/CShorten30)!\\n\\nimport WhatNext from '/_includes/what-next.mdx'\\n\\n<WhatNext />\", 'synthetic_query': 'What does the document say about using LLMs for RAG Evaluation and the traditional metrics involved?'}\n",
      "\n",
      "64\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspects of the document that a query could be asking about. The document is about \"Multimodal Embedding Models\" and discusses the multisensory nature of human learning, indicating that these models are designed to mimic the way humans integrate multiple sensory inputs. The document also provides a date, which could be relevant to the query. Additionally, the document mentions that the learning process through multi-sensory inputs begins from early stages of human development, which is a specific detail that could be the focus of a query.',\n",
      "    query='What are ML models that mimic the human ability to integrate multiple sensory inputs called?'\n",
      ")\n",
      "{'content': '---\\ntitle: Multimodal Embedding Models\\nslug: multimodal-models\\nauthors: zain\\ndate: 2023-06-27\\nimage: ./img/hero.png\\ntags: [\\'concepts\\']\\ndescription: \"ML Models that can see, read, hear and more!\"\\n\\n---\\n\\n![Multimodal Models](./img/hero.png)\\n\\n<!-- truncate -->\\n\\n## The Multisensory Nature of Human Learning\\n\\nHumans have a remarkable ability to learn and build world models through the integration of multiple sensory inputs. Our combination of senses work synergistically to provide us with rich and diverse information about our environment. By combining and interpreting these sensory inputs, we are able to form a coherent understanding of the world, make predictions, and acquire new knowledge very efficiently. The process of learning through multi-sensory inputs begins from the early stages of human development. Infants explore the world through their senses, touching, tasting, listening, and observing objects and people around them.', 'synthetic_query': 'What are ML models that mimic the human ability to integrate multiple sensory inputs called?'}\n",
      "\n",
      "65\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the main topic of the document, which is about how humans, especially children, learn new concepts through sensory exploration and the ability to understand concepts with few data points. The document compares this human learning process to a machine learning concept known as few-shot learning. The example given is teaching a child what a dog is with just one observation. Therefore, the query should be about the process of human learning in relation to sensory experiences and few-shot learning.',\n",
      "    query='How do humans, particularly children, utilize sensory exploration to learn new concepts with limited data points?'\n",
      ")\n",
      "{'content': \"This sensory exploration helps them link different perspectives of the same experience to create a holistic understanding of their environment. This fusion of multisensory data when learning new concepts is also partially responsible for why humans can learn with very few data points - making us great few-shot learners. Let's imagine you are trying to teach the concept of a dog to a child. The next time you see a dog at the park you point it out and say “This is a dog!”. Let's say that this is a single observation/data point - in the supervised machine-learning sense.\", 'synthetic_query': 'How do humans, particularly children, utilize sensory exploration to learn new concepts with limited data points?'}\n",
      "\n",
      "66\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information provided by the document that can be used to formulate a question. The document discusses the process by which a child learns about dogs through multimodal sensory experiences. It also mentions an anecdote about watching a movie without sound on an airplane. Therefore, the query should be related to the learning process of children using multiple senses or the experience of understanding a movie without sound.',\n",
      "    query='How do children develop a rich understanding of the concept of a dog through sensory experiences?'\n",
      ")\n",
      "{'content': \"From this single data point, the child receives an abundance of information: they see how the dog moves, interacts, and reacts to the world around it, hear how the dog barks, see how the fur on the dog blows in the wind, can touch the dog to see how it feels and even smell the dog. So, from this single “data point” the child extracts a very rich representation of multiple interdependent modalities of data that very distinctly define the concept of a dog. ![dogs](./img/dogs.jpg)\\n\\nOver time and with age, this fusion of sensory inputs gets more refined and nuanced and allows infants to develop higher levels of abstraction and understanding of objects' properties, such as shape, texture, and weight. Humans are such good multimodal reasoning engines that we do it without noticing - let's consider a practical scenario. Imagine you’re on an airplane and only have your wireless headset that cannot be plugged into the in-flight entertainment system - a predicament I find myself in more often than not these days!😅 So you start watching a movie with no sound and for the most part, you notice that you can get a pretty good, although imperfect, understanding of what’s going on.\", 'synthetic_query': 'How do children develop a rich understanding of the concept of a dog through sensory experiences?'}\n",
      "\n",
      "67\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the main topic of the document, which is about machine learning models that can interact with multiple data modalities, such as image, video, text, audio, and tactile representations. The document discusses the importance of these models being able to understand and preserve the meaning of data across these modalities. It also mentions the challenges in creating such multimodal models. Therefore, the query should be focused on the concept of multimodal machine learning models and their capabilities or challenges.',\n",
      "    query='What are the challenges in developing machine learning models that can understand multiple data modalities?'\n",
      ")\n",
      "{'content': \"Now imagine you turn on the captions, and now you can understand pretty much everything that’s going on and you can even fill in sound effects with your imagination supported by the modalities of data you have available. In order for our machine learning models to interact more naturally with data, the way we do, and ultimately be more general and powerful reasoning engines we need them to understand a data point through its corresponding image, video, text, audio, tactile and other representations - it needs to be able to preserve the meaning behind all of these data modalities once embedded in high dimensional vector space as demonstrated in Figure1 below. For example, in order to understand what a train is, we need our ML models to “see” and “hear” the train move, “feel” the motion of the ground near the train, and “read” about it. This is easier said and understood than done. Having a model that jointly comprehends all of these data modalities is very difficult, let's discuss some of the challenges that stand between us and truly multimodal models.\", 'synthetic_query': 'What are the challenges in developing machine learning models that can understand multiple data modalities?'}\n",
      "\n",
      "68\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"produce the query. We need to identify the unique information provided by the document. The document describes a multimodal model's embedding space and discusses the challenges of learning multimodal embeddings, specifically the lack of rich and aligned multimodal datasets. The query should be about the content that is specific to this document, such as the characteristics of the embedding space or the challenges mentioned.\",\n",
      "    query='What is a challenge in collecting and preparing multimodal datasets for learning embeddings?'\n",
      ")\n",
      "{'content': '<figure>\\n\\n![vectorspace](./img/vectorspace.png)\\n<figcaption>Figure 1. Shows a joint embedding space of a multimodal model that understands both text and images. Notice how objects that are similar are closer together and dissimilar objects are farther apart, this means that the model preserves semantic similarity within and across modalities.</figcaption>\\n\\n</figure>\\n\\n## Addressing Challenges of Learning Multimodal Embeddings\\n\\n### 1. Lack of Rich & Aligned Multimodal Datasets\\n\\nCollecting and preparing multimodal datasets is very challenging. Each modality requires specific data collection techniques and preprocessing steps.', 'synthetic_query': 'What is a challenge in collecting and preparing multimodal datasets for learning embeddings?'}\n",
      "\n",
      "69\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify the key points addressed in the document. The document discusses the importance of aligning and normalizing data across different modalities to ensure compatibility. It highlights the challenges such as differences in data formats, temporal alignment, and semantic alignment. It also mentions the difficulty machine-learning models face when data from different datasets is not properly aligned, particularly in learning interdependencies between modalities. Finally, it describes current approaches to data fusion across modalities, using the example of combining image/video data with audio data, but notes the limitation when certain data (like motion data for a lion) is not available. Based on this, we can formulate a query that the document would answer.\",\n",
      "    query='Why is it important to align and normalize data across different modalities in machine learning?'\n",
      ")\n",
      "{'content': 'Furthermore aligning and normalizing the data across modalities is crucial to ensure compatibility. This is quite challenging due to differences in data formats, temporal alignment, and semantic alignment. If the data is stitched together from different datasets and is not aligned properly then it becomes very difficult for the machine-learning model to extract and learn interdependencies between the modalities. Current approaches address this by taking multiple rich datasets and fusing them across data modalities where possible. So for example you might be able to combine the image/video of a lion from a computer vision dataset with the roar of a lion from an audio dataset but perhaps not with motion since you might not have motion data for a lion.', 'synthetic_query': 'Why is it important to align and normalize data across different modalities in machine learning?'}\n",
      "\n",
      "70\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information provided in the document. The document mentions a specific multimodal dataset called EGO4D and describes the type of data it captures, which includes motion/tactile data through accelerometers and gyroscopes, along with video captures. It also touches on the challenges of creating such datasets and the difficulty in designing machine learning models for multimodal data. Therefore, a query that this document would answer should be related to the EGO4D dataset, the type of data it captures, or the challenges associated with multimodal datasets and model architecture design.',\n",
      "    query='What is the EGO4D dataset, and what types of data does it capture?'\n",
      ")\n",
      "{'content': 'There are also attempts to collect richer multimodal datasets such as the [EGO4D](https://ego4d-data.org/docs/) which collects multiple data modalities for a given scenario. EGO4D actually captures motion/tactile data by capturing accelerometer and gyroscopic data temporally aligned with video captures of activities! The problem with this is that generating a rich multimodal dataset is very costly and even impractical in some cases. ### 2. Model Architecture\\n\\nDesigning the architecture for a single model that can process multimodal data is difficult. Usually, machine learning models are specialists in one domain of data - for example computer vision or natural language.', 'synthetic_query': 'What is the EGO4D dataset, and what types of data does it capture?'}\n",
      "\n",
      "71\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the main points of the document. The document discusses a multimodal approach called ImageBind from FAIR, Meta AI, which involves combining different types of data (image, speech, text, video, motion, and depth/thermal) by using pre-trained models for each modality and finetuning them with a contrastive loss function. It also mentions a key insight about the sufficiency of image-paired data for training joint embeddings and the scalability issue due to the cost of adding and finetuning separate models for each modality. Therefore, the query should ask about the approach used by ImageBind to train a multimodal model and the associated limitation.',\n",
      "    query='What approach does ImageBind use to train a multimodal model, and what is its scalability limitation?'\n",
      ")\n",
      "{'content': 'Training a single, jack-of-all-modalities model is very difficult. Current multimodal approaches like [ImageBind](https://arxiv.org/abs/2305.05665) from FAIR, Meta AI(which combines image, speech, text, video, motion and depth/thermal data) approach this problem by taking separate specialist pre-trained models for each modality and then finetuning them to bind their latent space representations using a contrastive loss function; which essentially pulls together representations of similar examples across modalities closer together and pushes apart distinct examples in a joint vector space. The key insight is that all combinations of paired modalities are not necessary to train such a joint embedding, and only image-paired data is sufficient to align and bind the modalities together. The limitation however is that adding and finetuning a separate pretrained model for every modality or task becomes prohibitively expensive and is not scalable. ### 3.', 'synthetic_query': 'What approach does ImageBind use to train a multimodal model, and what is its scalability limitation?'}\n",
      "\n",
      "72\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique aspects of the document that a query could be asking about. The document discusses the complexity of interpreting multimodal models, specifically the challenge of fusing different modalities and interpreting their learned representations. It also mentions a specific example involving accelerometer motion data and speech about globalization, highlighting the difficulty of pairing certain modalities. Furthermore, it introduces the ImageBind model as a solution to align different modalities by using image representation as ground truth. Therefore, the query should be about the challenges of multimodal model interpretability, the example provided, or the approach used by the ImageBind model to align modalities.',\n",
      "    query='What is the challenge of interpreting decisions made by multimodal models, and how does the ImageBind model address the alignment of different modalities?'\n",
      ")\n",
      "{'content': 'Model Interpretability\\n\\nUnderstanding and interpreting the decisions made by multimodal models can be challenging. The fusion of different modalities may introduce complexities in interpreting the learned representations and attributing importance to each modality. Consider, for example, searching in the learned joint embedding space of a multimodal model what accelerometer motion data is closest to a person giving a speech about globalization. The lesson is that some modalities or data objects just don’t pair together naturally. The ImageBind model handles this by using the image representation as ground truth and pulling all other concepts closer to image representations which then establishes a naturally learned alignment between other modalities.', 'synthetic_query': 'What is the challenge of interpreting decisions made by multimodal models, and how does the ImageBind model address the alignment of different modalities?'}\n",
      "\n",
      "73\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key points in the document that could lead to a specific question. The document discusses the initialization of image representations with vision-language models like CLIP, the explanation of alignments between different modalities, and the challenges of modality imbalance in training optimization. The query should be focused on one of these aspects to ensure that the document provides a unique answer.',\n",
      "    query='What challenges are associated with training optimization in multimodal learning systems that handle different types of data such as images, text, motion, and tactile data?'\n",
      ")\n",
      "{'content': 'These image representations are initialized with large-scale vision-language models such as CLIP, thereby leveraging the rich image and text representations of these models. Some of these alignments are explained directly (comparing a video of a person delivering a speech to audio of the speech) while other pairs of modalities are explained through their relation to an intermediate modality(understanding the relation between motion data and text data by seeing how they both relate to video/image data). With that said in other cases a logical explanation of why two examples are close together might not be possible without a close examination of the training sets. ### 4. Heterogeneity and Training Optimization\\n\\nHandling modality imbalance(we have a lot of image and text data and significantly less motion and tactile data) makes it very difficult to learn all modalities equally well; this takes careful tuning of the contribution of each modality during training and optimizing the fusion mechanism to prevent dominance by a single modality.', 'synthetic_query': 'What challenges are associated with training optimization in multimodal learning systems that handle different types of data such as images, text, motion, and tactile data?'}\n",
      "\n",
      "74\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key points mentioned in the document that could form the basis of a question. The document discusses the importance of heterogeneity in data modalities and introduces HighMMT as a solution for handling high-modality scenarios. It mentions that HighMMT uses two new information-theoretic metrics for heterogeneity quantification and that it can scale up to handle multiple modalities and tasks. The document also compares HighMMT to ImageBind, highlighting a specific scaling behavior. Based on these points, we can formulate a question that the document would answer.',\n",
      "    query='What is the advantage of HighMMT over ImageBind in terms of scaling behavior with added modalities?'\n",
      ")\n",
      "{'content': 'Data imbalance is one consideration but even more important to consider is how much unique information a modality brings - this is known as heterogeneity. Assessing heterogeneity accurately allows you to decide which modalities are different enough to be separately processed and which modality pairs interact differently and thus should be differently fused. [HighMMT](https://arxiv.org/abs/2203.01311) has been proposed to handle high-modality scenarios involving a large set of diverse modalities. HighMMT uses two new information-theoretic metrics for heterogeneity quantification, enabling it to automatically prioritize the fusion of modalities that contain unique information or unique interactions. This results in a **single model** that scales up to 10 modalities and 15 tasks from 5 different research areas, demonstrating a crucial scaling behavior not found in ImageBind: performance continues to improve with each modality added, and it transfers to entirely new modalities and tasks during fine-tuning.', 'synthetic_query': 'What is the advantage of HighMMT over ImageBind in terms of scaling behavior with added modalities?'}\n",
      "\n",
      "75\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information provided in the document that can be the basis of a question. The document discusses the concept of multimodal models, their purpose, and their implementation in Weaviate, specifically mentioning the `multi2vec-clip` module. It also mentions the limitations of Weaviate in terms of using multimodal models that are not provided out-of-the-box, such as those hosted on Huggingface or proprietary models. Therefore, a question that this document would answer is one that asks about the capabilities of Weaviate regarding multimodal models and the specific module it supports.',\n",
      "    query='What is the only out-of-the-box multimodal module available in Weaviate for joint embedding of images and text?'\n",
      ")\n",
      "{'content': 'In conclusion, efforts on developing multimodal models attempt to mimic human learning by combining different inputs, such as images, text, and audio, to improve the performance and robustness of machine learning systems. By leveraging multi-sensory inputs, these models can learn to recognize complex multimodal patterns, understand context across modes, and generate more comprehensive and accurate outputs even in the absence of some modalities. The main goal is to give these models the ability to interact with data in a more natural way thus enabling them to be more powerful and general reasoning engines. ## Multimodal Models in Weaviate\\n\\nCurrently, the only out-of-the-box multimodal module that can be configured and used with Weaviate is `multi2vec-clip` which can be used to project images and text into a joint embedding space and then perform a `nearVector` or `nearImage` search over these two modalities. Outside of this, you can only use multimodal models if they are hosted on Huggingface or if you have your own proprietary multimodal models.', 'synthetic_query': 'What is the only out-of-the-box multimodal module available in Weaviate for joint embedding of images and text?'}\n",
      "\n",
      "76\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='Document: A document containing information.\\nReasoning: Let\\'s think step by step in order to produce the query. We need to identify the key elements of the document that would lead to a specific question. The document mentions \"multimodal models,\" \"OOTB integrations,\" and \"vector search pipelines,\" indicating that it is related to some form of search technology that uses multiple types of data inputs. The phrase \"be on the lookout for new multimodal search capabilities soon\" suggests that there is an upcoming feature or product release. The word \"hear\" is in quotes, which might imply that the new capabilities involve audio data. The document also contains a hint of an import statement from a code snippet, which suggests that it is related to software development or a programming context. Based on these elements, we can formulate a query that the document would answer.',\n",
      "    query='What new capabilities involving multimodal models are being added to vector search pipelines?'\n",
      ")\n",
      "{'content': \"We know that our users love multimodal models and we’ve been cooking up more OOTB integrations with cutting-edge multimodal models so that it’s easy for you to start using them in your vector search pipelines. Be on the lookout for new multimodal search capabilities soon, I “hear” it's going to be amazing.😉\\n\\nimport WhatNext from '/_includes/what-next.mdx'\\n\\n<WhatNext />\", 'synthetic_query': 'What new capabilities involving multimodal models are being added to vector search pipelines?'}\n",
      "\n",
      "77\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"produce the query. We need to identify the unique aspects of the document that can be used to formulate a question. The document provides specific information about a UK-based startup called Moonsift, its use of AI and Weaviate, the purpose of its ecommerce browser extension, and the background of its co-founders. Therefore, we can ask about the startup's location, the technology they are using, the functionality of their product, or the expertise of the co-founders.\",\n",
      "    query=\"What is the UK-based startup that uses Weaviate's vector database to enhance online shopping discoverability?\"\n",
      ")\n",
      "{'content': '---\\ntitle: Building an AI-Powered Shopping Copilot with Weaviate\\nslug: moonsift-story\\nauthors: [alea, zain]\\ndate: 2023-11-15\\ntags: []\\nimage: ./img/hero.png\\ndescription: \"UK-based startup Moonsift is harnessing the power of AI with Weaviate.\"\\n---\\n![hero](img/hero.png)\\n\\nUK-based startup Moonsift is harnessing the power of AI—using machine learning models and Weaviate’s vector database—to help online shoppers discover the products they love. <!-- truncate -->\\n\\n[Moonsift](https://www.moonsift.com/) offers an ecommerce browser extension for users to curate shoppable boards with products from across the internet. Stylists and curators use Moonsift to create collections, registries, and wish lists that can be shared and shopped with a simple link. While thousands of customers add products from tens of thousands of retailers per month to Moonsift, co-founders David Wood and Alex Reed have a bigger vision for improving product discoverability for online shoppers. With combined experience in natural language processing (NLP), data science, and consulting for retail brands, Wood and Reed saw how retailers unknowingly restrict their own discoverability by tailoring keywords for search engines rather than users.', 'synthetic_query': \"What is the UK-based startup that uses Weaviate's vector database to enhance online shopping discoverability?\"}\n",
      "\n",
      "78\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"produce the query. We need to identify the key elements of the document that could lead to a specific question. The document mentions a problem with retail discoverability and introduces Moonsift as a solution built by Wood and Reed using AI to improve product discovery for shoppers. It also mentions the necessity of collecting data from retailers and cross-retailer shopping data to train machine learning models. Therefore, a question that this document would answer is related to the purpose of Moonsift, the founders' vision, and the type of data required for its functionality.\",\n",
      "    query='What is the purpose of Moonsift, and what kind of data did its founders collect to train its AI?'\n",
      ")\n",
      "{'content': 'How even with the most illustrious descriptions, shoppers struggle to find the products they’re looking for. They knew that retail discoverability wasn’t what it could be, and built Moonsift to improve it. > From the beginning, Wood and Reed’s vision was to use the power of AI to help shoppers more easily discover the products they love. ## Collecting data to enable understanding \\nThe first generation of Moonsift has been a stepping stone toward this vision. In order to create a comprehensive AI shopping copilot, they needed product data from retailers along with cross-retailer shopping data to enable understanding of the discovery process and train machine learning models.', 'synthetic_query': 'What is the purpose of Moonsift, and what kind of data did its founders collect to train its AI?'}\n",
      "\n",
      "79\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique pieces of information provided in the document that could be the basis for a specific question. The document mentions the company Moonsift, the use of their browser extension, the amount of data collected (products, interactions, retailers), and the hiring of Marcel Marais as the lead machine learning engineer. It also mentions the initial approach to improving product discovery using a keyword-based search system with BM25 and re-ranking. With these details in mind, we can formulate a query that would be uniquely answered by the document.',\n",
      "    query=\"How many products, interactions, and retailers has Moonsift's browser extension collected data on, and who did they hire as the lead machine learning engineer to improve their discovery engine?\"\n",
      ")\n",
      "{'content': 'And that’s exactly what they’ve been collecting. Usage of their browser extension over the last few years has enabled Moonsift to explore discovery and train models on over 60M products, 250M interactions, and 40K retailers across the internet. Now, Moonsift has the data they need (which is growing every day) to improve product discovery with AI. ## Building the discovery engine \\nThe Moonsift team brought on Marcel Marais as lead machine learning engineer to build a system that could harness the data they’ve gathered to take their product to the next level. At first, Marais looked at improving discovery through a keyword-based search system using BM25 and re-ranking, but he quickly assessed that would not be sufficient to power the type of recommendation engine they needed.', 'synthetic_query': \"How many products, interactions, and retailers has Moonsift's browser extension collected data on, and who did they hire as the lead machine learning engineer to improve their discovery engine?\"}\n",
      "\n",
      "80\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"produce the query. We know that Marais was looking for a specific type of database to handle a particular problem. The document outlines the problem as the need for a semantic search system capable of interpreting user intent and indexing multimodal data. It also mentions that Marais had a set of priorities for selecting this database, with ease of use being a critical factor. Therefore, we are looking for a question that directly relates to Marais's requirements for a database system that can handle semantic search and multimodal data, as well as the importance of ease of use in the selection process.\",\n",
      "    query=\"What were Marais's priorities when selecting a vector database for semantic search and multimodal data indexing?\"\n",
      ")\n",
      "{'content': 'He needed to implement semantic search, a system that could interpret a user’s intent rather than rely on exact keyword matches. And they needed a system that could index and search multimodal (text and image) data across millions of objects. Based on prior experience with semantic search and vector embeddings, Marais decided a vector database was the tool for the job. Marais quickly put together a checklist of priorities for their vector database selection. Moving fast was important, so ease of use was at the top of the list.', 'synthetic_query': \"What were Marais's priorities when selecting a vector database for semantic search and multimodal data indexing?\"}\n",
      "\n",
      "81\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"produce the query. We need to identify the key elements of the document that would be the basis for a unique question. The document mentions a company's preference for an open source platform due to the need to index millions of products and the requirement for high performance and cost efficiency at scale. It also states that Weaviate was chosen as the vector database and provides specific reasons for this choice, such as being open source, having an active community, managed cloud offering, comprehensive documentation, support for LLMs and multi-modal models, and the ability to integrate and experiment with machine learning models. Therefore, the query should be focused on why Weaviate was selected as the vector database by the company.\",\n",
      "    query='Why did the company choose Weaviate as their vector database solution?'\n",
      ")\n",
      "{'content': 'The company preferred an open source platform, and since they would be indexing millions of products, they needed a solution that was both high-performing and cost-efficient at scale. ## Selecting Weaviate as the vector database of choice \\nAfter a thorough evaluation of a handful of open and closed-source vector databases, the team decided that Weaviate was the best-fit solution for their needs. They cited the following reasons for choosing Weaviate:  \\n\\n* Open source, with an active community and managed cloud offering. * Comprehensive documentation and strong support for popular LLMs and multi-modal models. * Direct integration of machine learning models using a module system, with the ability to easily swap out and experiment with different models.', 'synthetic_query': 'Why did the company choose Weaviate as their vector database solution?'}\n",
      "\n",
      "82\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='Document: A document containing information.\\nReasoning: Let\\'s think step by step in order to produce the query. We need to identify the key aspects of the document that could be the basis for a question. The document mentions \"advanced monitoring and replication capabilities,\" \"high query throughput at scale,\" and \"unique search features\" that combine keyword and vector search. It also includes a testimonial about \"Weaviate\" being a solution that enabled the creation of a \"production-ready AI-powered search engine\" within weeks. Therefore, the query should be about a database or search engine solution that offers these specific features and benefits.',\n",
      "    query='What database technology offers advanced monitoring, replication, high query throughput, and unique search features for AI-powered search engines?'\n",
      ")\n",
      "{'content': '* Advanced monitoring and replication capabilities. * High query throughput at scale. * Unique search features to drive performance and efficiency (using important aspects of both keyword and vector search). > “Weaviate was exactly what we needed. Within a couple of weeks we had a production-ready AI-powered search engine.', 'synthetic_query': 'What database technology offers advanced monitoring, replication, high query throughput, and unique search features for AI-powered search engines?'}\n",
      "\n",
      "83\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"produce the query. We need to identify a unique piece of information that is provided by the document. The document mentions a specific technology or feature that Moonsift is considering to optimize their system. It also mentions the name of the CTO and co-founder of Moonsift, as well as a product they are preparing to launch. The document provides a unique example of what the product can do, which is not likely to be found in other documents. Therefore, we can form a query around the product launch, the technology being considered for optimization, or the unique examples of the product's capabilities.\",\n",
      "    query=\"What examples of user intent can Moonsift's AI Copilot understand according to their CTO, David Wood?\"\n",
      ")\n",
      "{'content': 'We’re confident in knowing we don’t have to trade performance for scale.”\\n\\n– David Wood, CTO and co-founder, Moonsift\\n\\n## Next steps \\nMoonsift is now getting ready to launch their [AI Copilot](https://www.moonsift.com/copilot) to the world. They’re seeing early results of the power of its ability to understand user intent and serve intelligent results. Some fun examples include “shirt that looks like a Caipirinha” or “skirt with a pattern inspired by ocean waves”. ![image](img/image1.png)\\n\\nAs Moonsift prepares for the public launch of its shopping Copilot, the team is continuing to explore ways to optimize the cost, performance, and scale of their system. They are looking into Weaviate’s new feature, [Product Quantization (PQ)](/blog/pq-rescoring), which helps reduce the memory footprint of their system, by compressing vectors and performing rescoring, while retaining search relevance.', 'synthetic_query': \"What examples of user intent can Moonsift's AI Copilot understand according to their CTO, David Wood?\"}\n",
      "\n",
      "84\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify a unique piece of information that the document provides. The document mentions a specific feature (multi-tenancy) related to a product (Weaviate) and an application (Moonsift’s shopping Copilot) that demonstrates this feature. It also offers ways to engage with the product, such as signing up for early access, trying a managed cloud offering, and reaching out through various channels. The query should be focused on one of these unique aspects.',\n",
      "    query='What feature does Weaviate offer to scale personalized vector search for millions of customers?'\n",
      ")\n",
      "{'content': \"Along with PQ they are also exploring [multi-tenancy](/blog/multi-tenancy-vector-search) that will allow them to scale and perform personalized vector search for millions of customers. [See a demo](https://www.youtube.com/watch?v=hOsBxvV9rvI) of Moonsift’s shopping Copilot, and [sign up for early access today](https://www.moonsift.com/copilot)!\\n\\n\\n## What's next\\nInterested in using Weaviate to power your search and AI applications? [Give our fully managed cloud offering a spin](https://console.weaviate.cloud/) for free and check out our [Quickstart guide](/developers/weaviate/quickstart). You can reach out to us on [Slack](https://weaviate.io/slack) or [Twitter](https://twitter.com/weaviate_io), or [join the community forum](https://forum.weaviate.io/).\", 'synthetic_query': 'What feature does Weaviate offer to scale personalized vector search for millions of customers?'}\n",
      "\n",
      "85\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify key pieces of information that are unique to this document. The document title, \"Weaviate 1.18 release,\" the release date, \"2023-03-07,\" and specific features introduced in this release, such as \"Faster Filtering through Bitmap Indexing, HNSW-PQ, Cursor API,\" are all unique identifiers for this document. Using these details, we can formulate a query that would be answered by the content of this document.',\n",
      "    query='What features were introduced in the Weaviate 1.18 release on March 7, 2023?'\n",
      ")\n",
      "{'content': '---\\ntitle: Weaviate 1.18 release\\nslug: weaviate-1-18-release\\nauthors: [jp, erika, zain, dan]\\ndate: 2023-03-07\\ntags: [\\'release\\']\\nimage: ./img/hero.png\\ndescription: \"Weaviate 1.18 introduces Faster Filtering through Bitmap Indexing, HNSW-PQ, Cursor API, and more! Learn all about it.\"\\n---\\n\\nimport Core118 from \\'./_core-1-18-include.mdx\\' ;\\n\\n<Core118 />\\n\\nimport WhatsNext from \\'/_includes/what-next.mdx\\'\\n\\n<WhatsNext />\\n\\nimport Ending from \\'/_includes/blog-end-oss-comment.md\\' ;\\n\\n<Ending />', 'synthetic_query': 'What features were introduced in the Weaviate 1.18 release on March 7, 2023?'}\n",
      "\n",
      "86\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information that the document provides. The document talks about Hacktoberfest 2023, specifically the celebration organized by Weaviate. It mentions the date range for the event, the requirement for participants to receive a digital reward, and the task related to onboarding engineers and machine learning practitioners. The query should be something that this document can uniquely answer, based on the details provided.',\n",
      "    query=\"What are the requirements to receive a digital reward during Weaviate's Hacktoberfest 2023 celebration?\"\n",
      ")\n",
      "{'content': '---\\ntitle: Hacktoberfest 2023 - Celebrating Open Source with Weaviate\\nslug: hacktoberfest-2023\\nauthors: [leonie]\\ndate: 2023-10-02\\ntags: []\\nimage: ./img/weaviate-hacktoberfest-2023.png\\ndescription: \"Join us in celebrating Hacktoberfest, a month-long celebration of open source!\"\\n---\\nimport hacktober_demo from \\'./img/hacktoberfest_2023_demo.mp4\\';\\n\\n![Celebrating Hacktoberfest 2023 with Weaviate](./img/weaviate-hacktoberfest-2023.png)\\n\\nAt [Weaviate](https://weaviate.io/), we love open source! Join us in celebrating [Hacktoberfest](https://hacktoberfest.com/), a month-long celebration of open source!\\nParticipants with four pull requests (PR) accepted between **October 1 - 31, 2023** will receive a unique digital reward [from Hacktoberfest](https://hacktoberfest.com/participation/). ## The Task\\n\\nWelcome to our demo sprint!\\n\\nIt is aimed at onboarding engineers and machine learning practitioners to open-source. In our [example use cases and demos](https://weaviate.io/developers/weaviate/more-resources/example-use-cases) page, we showcase what you can do with a [vector database](https://weaviate.io/blog/what-is-a-vector-database). Some of these demos have received more love than others recently, and we want to give them a little polish. ![Untitled](./img/weaviate-demos.png)\\n\\nThe gallery contains demos using Weaviate in different states.', 'synthetic_query': \"What are the requirements to receive a digital reward during Weaviate's Hacktoberfest 2023 celebration?\"}\n",
      "\n",
      "87\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information provided in the document that can be asked about. The document mentions a specific process related to Hacktoberfest contributions, including finding a project marked with a pumpkin (🎃) sign and looking at its issues. Therefore, the query should be about how to contribute to a Hacktoberfest project as described in the document.',\n",
      "    query='How can I contribute to a Hacktoberfest project according to the instructions provided in the linked document?'\n",
      ")\n",
      "{'content': \"[Here](https://github.com/databyjp/distyll) is an example of a demo project in a good state. While some may only need a little polish of the description (README.md file), others are e.g., a little older or can use a little makeover for the user interface. The steps to your first Hacktoberfest PR are simple:\\n\\n1. Find an issue you're interested in: Go to the [example use cases and demos](https://weaviate.io/developers/weaviate/more-resources/example-use-cases) page and select a project marked for Hacktober with a 🎃\\xa0sign. Click on “code” to get to the repository and have a look at its issues.\", 'synthetic_query': 'How can I contribute to a Hacktoberfest project according to the instructions provided in the linked document?'}\n",
      "\n",
      "88\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify the unique aspects of the document that can be queried. The document appears to be a set of instructions for contributing to a project during Hacktoberfest, specifically for a project related to Weaviate. It includes a video element with specific attributes and mentions a process for expressing interest in contributing and a guide to follow. Therefore, the query should focus on these unique elements.\",\n",
      "    query='How do I contribute to the Weaviate project during Hacktoberfest?'\n",
      ")\n",
      "{'content': '<video width=\"100%\" autoplay loop controls>\\n  <source src={hacktober_demo} type=\"video/mp4\" />\\nYour browser does not support the video tag. </video>\\n\\n\\n2. Ping us on the project’s issue, saying you\\'re interested and which parts of the issue you would like to contribute to. 3. Open the PR as instructed in the [Weaviate Contributor Guide](https://weaviate.io/developers/contributor-guide).', 'synthetic_query': 'How do I contribute to the Weaviate project during Hacktoberfest?'}\n",
      "\n",
      "89\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information that the document provides. The document seems to be an invitation to contribute to Weaviate projects during Hacktoberfest, with a focus on helping newcomers get started with open-source contributions and Weaviate. It includes resources for learning about open source, contributing to projects, and specific guides for using Weaviate. The query should be something that this document can answer, which is not too broad and relates directly to the content provided.',\n",
      "    query='Where can I find resources to help me make my first open-source contribution to Weaviate?'\n",
      ")\n",
      "{'content': \"We will review and help you out in the process. 💚\\n\\nYou can also contribute by adding your own Weaviate examples. If you have other great ideas for contributions, let us know on our [Discourse](https://forum.weaviate.io/) and [Slack](https://weaviate.slack.com/) channels, and we will figure out how to highlight it in Hacktoberfest. You don't need to be an expert to contribute to these demo projects!\\n\\n\\n## Resources to Get Started\\n\\nWe're thrilled to help you make your first open-source contribution! Here are some helpful resources to kickstart your journey:\\n\\nWhat is Open Source, and how do you contribute to it? - 🎯 [What is Open Source](https://www.digitalocean.com/community/tutorials/what-is-open-source)\\n- 🎯 [Introduction to GitHub and Open-Source Projects](https://www.digitalocean.com/community/tutorial_series/an-introduction-to-open-source)\\n- 🎯 [How to Contribute to Open Source](https://opensource.guide/how-to-contribute/)\\n- 🎯\\xa0[GitHub Contribution Guide by Hugging Face](https://www.notion.so/Contribution-Guide-19411c29298644df8e9656af45a7686d?pvs=21)\\n- 🎯 [How to Use Git](https://www.digitalocean.com/community/cheatsheets/how-to-use-git-a-reference-guide)\\n- 🎯 [Weaviate Contributor Guide](https://weaviate.io/developers/contributor-guide)\\n\\nIf you're new to Weaviate, get up and running quickly with these beginner-friendly guides:\\n\\n- [Quickstart Guide](https://weaviate.io/developers/weaviate/quickstart) 🚀\\n- [Weaviate Academy](https://weaviate.io/developers/academy) 🎓\\n\\nDive deeper into specific topics with these detailed guides:\\n\\n- [How-to Search Guides](https://weaviate.io/developers/weaviate/search) 🔍\\n- [Keyword, Vector, Hybrid, and Generative Search](https://github.com/weaviate-tutorials/generative-search/blob/main/GenerativeSearchDemo.ipynb) 🔍\\n- [How-to Manage Data (CRUD Operations)](https://weaviate.io/developers/weaviate/manage-data) 💾\\n- [Tutorial: Importing Data with Your Own Vectors](https://weaviate.io/developers/weaviate/tutorials/wikipedia) 📊\\n- [Weaviate Architecture Concepts](https://weaviate.io/developers/weaviate/concepts#weaviate-architecture) 🏛️\\n\\nJoin one of our [workshops](https://weaviate.io/learn/workshops) for an introduction to Weaviate.\", 'synthetic_query': 'Where can I find resources to help me make my first open-source contribution to Weaviate?'}\n",
      "\n",
      "90\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify a unique piece of information that the document provides. The document lists specific events with dates, times, and presenters, as well as additional resources related to Weaviate. It also mentions Hacktoberfest and the potential for swag. A query that this document would answer should be specific to the information provided, such as the details of an event or the resources mentioned.',\n",
      "    query=\"Who is hosting the 'Introduction to Weaviate' webinar on 4th October 2023?\"\n",
      ")\n",
      "{'content': \"- 4th October 2023 (20:00-21:00 UTC+2): [Introduction to Weaviate](https://form.jotform.com/232574048361254) with Zain Hasan, Senior Dev Advocate @ Weaviate\\n- 5th October 2023 (17:00-18:00 UTC+2): [NEW Python `Collections` Client API Preview](https://form.jotform.com/232683153137859) with JP Hwang - Educator @ Weaviate\\n- 18th October 2023 (15:00-16:00 UTC+2): [Introduction to Weaviate](https://form.jotform.com/232602295283859) with JP Hwang - Educator @ Weaviate\\n\\nExpand your knowledge with these supplementary resources:\\n\\n- [Weaviate YouTube Channel](https://www.youtube.com/@Weaviate) 📺\\n- [Weaviate Blog](https://weaviate.io/blog) 📰\\n- [Weaviate Recipes](https://github.com/weaviate/recipes) 🍳\\n\\n\\n## What's in it for you? The repositories you contribute to participate in [Hacktoberfest](https://hacktoberfest.com/) and are a great opportunity for your first-ever Hacktoberfest PR. We will also assist you throughout the whole process. You might even receive some swag in the end. ## Connect with the Weaviate Community!\\n\\nTo make your Hacktoberfest experience successful, connect with the Weaviate community for collaboration and assistance.\", 'synthetic_query': \"Who is hosting the 'Introduction to Weaviate' webinar on 4th October 2023?\"}\n",
      "\n",
      "91\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information that the document provides. The document lists various ways to reach out to a community for a specific event called Hacktoberfest. It includes forums, social media, newsletters, and a GitHub repository. The query should ask for a specific piece of information that can only be answered by this document.',\n",
      "    query=\"How can I participate in the Weaviate community's Hacktoberfest event and stay updated on their developments?\"\n",
      ")\n",
      "{'content': \"You can reach us through:\\n\\n- Join our dedicated Hacktoberfest channel in our [Discourse community forum](https://forum.weaviate.io/), where we're ready to answer your questions. - Join our dedicated #hacktoberfest channel in our [Weaviate Community Slack](https://weaviate.slack.com/) channel to stay connected and receive real-time support. - Join our [newsletter](https://newsletter.weaviate.io/) and follow us on [Twitter](https://twitter.com/weaviate_io) and [Linkedin](https://www.linkedin.com/company/weaviate-io/mycompany/verification/) to receive updates. - Stay up to date with Weaviate's development by exploring the [Weaviate GitHub Repository](https://github.com/weaviate/weaviate). Don’t forget to give us a ⭐️ while you are there!\\n\\n:::info Pro Tip\\nShare your process online and tag us on [Twitter](https://twitter.com/weaviate_io) and [LinkedIn](https://nl.linkedin.com/company/weaviate-io).\", 'synthetic_query': \"How can I participate in the Weaviate community's Hacktoberfest event and stay updated on their developments?\"}\n",
      "\n",
      "92\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify a piece of information that is unique and specific to the document provided. The document mentions a specific event, Hacktoberfest 2023, and provides details about participation, including the timeframe for valid contributions and where to seek help or discuss ideas. The query should be focused on extracting one of these unique details.',\n",
      "    query='Does a contribution count towards Hacktoberfest 2023 if it is submitted on October 1st?'\n",
      ")\n",
      "{'content': 'Use the hashtag #hacktoberfest2023 for increased visibility. :::\\n\\n\\n## FAQ\\n\\n- **Will this count towards Hacktoberfest?** Yes, it definitely does! If your PR/MR is created between **October 1** and **October 31** (in any time zone, UTC-12 thru UTC+14), we will add the \"HACKTOBERFEST-ACCEPTED\" label to it. - **Where do I get help?** For any questions or assistance, contact us on our [Discourse](https://forum.weaviate.io/) and [Slack](https://weaviate.slack.com/) channels. - **I have a cool contribution idea. Can I still participate?** Awesome! Connect with us on our [Discourse](https://forum.weaviate.io/) or [Slack](https://weaviate.slack.com/) channels and we will figure it out.', 'synthetic_query': 'Does a contribution count towards Hacktoberfest 2023 if it is submitted on October 1st?'}\n",
      "\n",
      "93\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the main points of the document. The document addresses a concern about the necessity of coding skills for contributing to a project, specifically during Hacktoberfest 2023. It provides reassurance that non-coding contributions are welcome and offers help for those who want to learn to code. It also mentions where to find help and where to start with example use cases and demos. Based on this, we can formulate a query that the document answers.',\n",
      "    query=\"Can I contribute to Hacktoberfest 2023 if I don't know how to code?\"\n",
      ")\n",
      "{'content': \"- **I don’t know how to write code. Can I still contribute?** Yes, of course! You can make no-code contributions, e.g., by updating the README.md files. If you want to learn how to write code with a concrete example, we can help you find a good issue. Just ping us on our [Discourse](https://forum.weaviate.io/) or [Slack](https://weaviate.slack.com/) channels. ---\\n\\nHappy hacking, and let's make Hacktoberfest 2023 a memorable one together! 🚀\\n\\nJump right in and have a look at our [example use cases and demos](https://weaviate.io/developers/weaviate/more-resources/example-use-cases) page.\", 'synthetic_query': \"Can I contribute to Hacktoberfest 2023 if I don't know how to code?\"}\n",
      "\n",
      "94\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify a question that can be uniquely answered by the information provided in the document. The document gives us specific details about the Weaviate 1.14 release, including the release date, the authors, and the key features of the release. Therefore, a query that asks for the release date, the authors involved, or the main features of this specific version would be uniquely answered by this document.',\n",
      "    query='What are the main features of the Weaviate 1.14 release?'\n",
      ")\n",
      "{'content': '---\\ntitle: Weaviate 1.14 release\\nslug: weaviate-1-14-release\\nauthors: [connor, etienne, laura, sebastian]\\ndate: 2022-07-06\\ntags: [\\'release\\']\\nimage: ./img/hero.png\\ndescription: \"Learn what is new in Weaviate 1.14, the most reliable and observable Weaviate release yet!\"\\n---\\n![Weaviate 1.14 release](./img/hero.png)\\n\\n<!-- truncate -->\\n\\n## What is new\\nWe are excited to announce the release of Weaviate 1.14, the most reliable and observable Weaviate release yet. > Later this week, we will release Weaviate v1.14, possibly the most boring release so far.😱 Yet I\\'m incredibly excited about it and so should you. Why? <br/>\\n> (1/9)<i className=\"fas fa-reel\"></i><br/>\\n> [See <i className=\"fab fa-twitter\"></i> by @etiennedi](https://twitter.com/etiennedi/status/1544689150217027584?ref_src=twsrc%5Etfw)\\n\\nBesides many bug fixes and reliability improvements, we have a few neat features that you might find interesting. In short, this release covers:\\n\\n* Reliability fixes and improvements\\n* Monitoring and Observability\\n* Support for non-cosine distances\\n* API changes\\n\\n## Reliability fixes and improvements\\n![Reliability and fixes](./img/reliability.png)\\n\\nAt Weaviate, Reliability is one of our core values, which means that we will always strive to make our software dependable, bug-free, and behave as expected.', 'synthetic_query': 'What are the main features of the Weaviate 1.14 release?'}\n",
      "\n",
      "95\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the unique information provided in the document that can be asked about. The document mentions a critical bug fix in the compaction logic of Weaviate version 1.14.0, which could result in data loss in environments with frequent updates and deletes. It also refers to a changelog for a complete list of features and bug fixes. Therefore, the query should be about the specific bug fix or the release version that includes it.',\n",
      "    query='What critical bug was fixed in Weaviate version 1.14.0?'\n",
      ")\n",
      "{'content': 'And yes, bug fixing is not always the most exciting topic, as we often get more excited about shiny new features. But for you to truly enjoy working with Weaviate, we need to make sure that no bugs are getting in the way. Check out [the changelog](https://github.com/weaviate/weaviate/releases/tag/v1.14.0) to see the complete list of features and over 25 bug fixes. ### Critical bug fix in compaction logic\\nIn this release we fixed a critical bug, which in rare situations could result in data loss.<br/>\\nThe bug affected environments with frequent updates and deletes. > This bug fix alone, makes it worth upgrading to Weaviate 1.14.', 'synthetic_query': 'What critical bug was fixed in Weaviate version 1.14.0?'}\n",
      "\n",
      "96\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify a question that is directly related to the specific error described in the document. The document details an error in the compaction logic that affects data retrieval and integrity. It provides symptoms of the error, an example of how the error manifests in both GraphQL and REST API calls, and the potential impact on data manipulation operations. Therefore, the query should be about the nature of the error, how it can be observed, or its consequences on data operations.',\n",
      "    query='What error in the compaction logic could cause data retrieval issues and affect update and delete operations?'\n",
      ")\n",
      "{'content': '#### Background\\nWe found a critical error in the compactioniong logic that could lead to the compaction operation either corrupting or completely losing data elements. This could be obsereved through a variety of symptoms:\\n  * Retrieving an object by it\\'s ID would lead to a different result than retrieving the object using a filter on the id property\\n  * Filters that should match a specific number of objects matched fewer objects than expected\\n  * Objects missing completely\\n  * Filters with `limit=1` would not return any results when there should be exactly one element, but increasing the limit would then include the object\\n  * Filters would return results with `null` ids\\n\\n#### Example\\nIn the first case, if you had an object with id: **my-id-123456**. Calling the following GraphQL API with a filter on id would return the expected object. ```graphql\\n{\\n  Get {\\n    Article(where: {\\n        path: [\"id\"],\\n        operator: Equal,\\n        valueText: \"my-id-123456\"\\n      }) {\\n      title\\n    }\\n  }\\n}\\n```\\n\\nHowever, calling the following REST API with the same id wouldn\\'t get the object back. ```\\nGET /v1/objects/{my-id-123456}\\n```\\n\\n#### The problem\\nSo, if your data manipulation logic depended on the above operations to perform as expected, you update and delete operations might have been issued incorrectly.', 'synthetic_query': 'What error in the compaction logic could cause data retrieval issues and affect update and delete operations?'}\n",
      "\n",
      "97\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify the key information provided in the document that is unique and specific. The document mentions a specific version of Weaviate (`1.14`) and talks about performance improvements in data imports for large datasets. It also provides specific numbers regarding the performance improvements, stating that they are noticeable for imports of over 10 million objects and that the update enables imports of over 200 million objects. Additionally, it mentions a problem with the previous version where the HNSW index would grow in constant intervals of 25,000 objects, which was an issue for datasets under 25 million objects. Based on this information, we can formulate a query that asks about the improvements in data import performance in a specific version of Weaviate and for which scale of data these improvements are significant.\",\n",
      "    query='What improvements does Weaviate version 1.14 offer for large scale data imports?'\n",
      ")\n",
      "{'content': '### Improved performance for large scale data imports\\nWeaviate `1.14` significantly improves the performance of data imports for large datasets. Note that the performance improvements should be noticeable for imports of over 10 million objects. Furthermore, this update enables you to import over 200 million objects into the database. #### Problem\\nBefore, the HNSW index would grow in constant intervals of 25,000 objects. This was fine for datasets under 25 million objects.', 'synthetic_query': 'What improvements does Weaviate version 1.14 offer for large scale data imports?'}\n",
      "\n",
      "98\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale=\"Document: A document containing information.\\nReasoning: Let's think step by step in order to produce the query. We need to identify a unique piece of information that the document provides. The document discusses a specific problem with database performance related to the number of objects it contains. It then outlines a solution involving changes to the HNSW index growth pattern. The unique aspect here is the relative growth pattern implemented to address the performance issue. Therefore, the query should be about the solution to the database performance problem when the number of objects reaches a certain threshold.\",\n",
      "    query='What solution was implemented to improve the performance of adding new objects to a database with over 25 million objects?'\n",
      ")\n",
      "{'content': \"But once the database got to around 25 million objects, adding new objects would be significantly slower. Then from 50–100m, the import process would slow down to a walking pace. #### Solution\\nTo address this problem, we changed how the HNSW index grows. We implemented a relative growth pattern, where the HNSW index size increases by either 25% or 25'000 objects (whichever is bigger). ![HNSW index growth chart](./img/hnsw-index-growth.jpg)\\n\\n#### Test\\nAfter introducing the relative growth patterns, we've run a few tests.\", 'synthetic_query': 'What solution was implemented to improve the performance of adding new objects to a database with over 25 million objects?'}\n",
      "\n",
      "99\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key points of information that the document provides. The document mentions a successful import of a large number of objects and the performance consistency during the process. It also refers to an improvement in the Mean-Time-To-Recovery (MTTR) in Weaviate version 1.14, which reduced recovery time from minutes or hours to seconds after a crash. Therefore, the query should be about either the import performance of a large number of objects or the MTTR improvement in a specific version of Weaviate.',\n",
      "    query=\"What improvements were made to Weaviate's Mean-Time-To-Recovery in version 1.14?\"\n",
      ")\n",
      "{'content': 'We were able to import 200 million objects and more, while the import performance remained constant throughout the process. [See more on github](https://github.com/weaviate/weaviate/pull/1976). ### Drastically improved Mean-Time-To-Recovery (MTTR)\\nWeaviate `1.14` fixes an issue where a crash-recovery could take multiple minutes, or even hours in some extreme cases. It is now a matter of just seconds. So even in the rare event that your instance crashes, it will come back up very quickly.', 'synthetic_query': \"What improvements were made to Weaviate's Mean-Time-To-Recovery in version 1.14?\"}\n",
      "\n",
      "100\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key points in the document that could be the basis for a question. The document mentions a specific technology, Weaviate, and a feature it has, the Write-Ahead Log (WAL). It explains the purpose of the WAL, which is to prevent data loss in the event of an unexpected crash. It also describes what happens when Weaviate starts up after a crash, specifically the parsing of the WAL and the recovery of unfinished operations. Finally, it points out potential issues with the recovery process, such as the size of unflushed memtables and the fact that the recovery process was single-threaded. Based on this information, we can formulate a question that the document would answer.',\n",
      "    query='How does Weaviate ensure data recovery after an unexpected crash?'\n",
      ")\n",
      "{'content': '#### Problem\\nIf Weaviate encounters an unexpected crash, no data will be lost. To provide this guarantee, a Write-Ahead Log (WAL) is in place. If a crash had occurred, the WAL is parsed at startup, and all previously unfinished operations are recovered, even if they were part of in-memory structures that had not yet been flushed. While this system is very safe, the recovery could be slow for several reasons:\\n\\n- Unflushed memtables could become very large. This would lead to a lot of data that needs to be recovered after a crash\\n- The recovery process was single-threaded.', 'synthetic_query': 'How does Weaviate ensure data recovery after an unexpected crash?'}\n",
      "\n",
      "101\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key points in the document that could lead to a specific question. The document mentions the sequential nature of recoveries, the impact on startup time, the underutilization of CPU cores during this process, and the performance degradation due to the data structure not being designed to hold many items. The document also mentions a WAL (Write-Ahead Logging), which is a concept in database systems. Therefore, the query should be about the performance implications of the recovery process on a system with a large WAL.',\n",
      "    query='Why does the recovery process from a large WAL lead to slow startup times on a machine with multiple CPU cores?'\n",
      ")\n",
      "{'content': 'If multiple recoveries were required, they would happen in sequence. On a large machine, this could mean that startup would be slow, yet only one of many CPU cores was utilized. - The data structure used to hold the recovered items was never intended to hold many items. Each additional insertion would degrade its performance. As a result, the larger the WAL to recover, the slower the recovery would become.', 'synthetic_query': 'Why does the recovery process from a large WAL lead to slow startup times on a machine with multiple CPU cores?'}\n",
      "\n",
      "102\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "Prediction(\n",
      "    rationale='produce the query. We need to identify the key points addressed in the document that are related to improving MTTR (Mean Time To Recovery). The document mentions three specific improvements: deduplication of large WALs, parallel recovery processes, and a mechanism for flushing idle memtables. By focusing on these improvements, we can formulate a query that asks about the strategies used to enhance the MTTR in a specific context, which in this case seems to be a database or data storage system.',\n",
      "    query='What strategies were implemented to improve the Mean Time To Recovery (MTTR) in the described system?'\n",
      ")\n",
      "{'content': '#### Solution\\nWe addressed each of the points above individually and improved the overall MTTR substantially:\\n\\n- A deduplication process was added, so that large WALs with a lot of updates (i.e. redundant data) could be reduced to only the necessary information. - The recovery process now runs in parallel. If there are multiple places that require recovery, they can each recover independently, without one recovery having to wait for the other. - A mechanism was added that flushes any memtable that has been idle (no writes) for 60s or more. In addition to speeding up the recovery, this change also ensures that no recovery is needed at all in many cases.', 'synthetic_query': 'What strategies were implemented to improve the Mean Time To Recovery (MTTR) in the described system?'}\n",
      "\n",
      "103\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(idx)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dspy\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39mgpt4T):\n\u001b[0;32m---> 18\u001b[0m     llm_query \u001b[38;5;241m=\u001b[39m \u001b[43mdspy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChainOfThought\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWriteQuery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblog_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(llm_query)\n\u001b[1;32m     20\u001b[0m data_properties \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: blog_chunk,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msynthetic_query\u001b[39m\u001b[38;5;124m\"\u001b[39m: llm_query\u001b[38;5;241m.\u001b[39mquery\n\u001b[1;32m     23\u001b[0m }\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dspy/predict/predict.py:60\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dspy/predict/chain_of_thought.py:66\u001b[0m, in \u001b[0;36mChainOfThought.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     signature \u001b[38;5;241m=\u001b[39m dsp\u001b[38;5;241m.\u001b[39mTemplate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39minstructions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_signature)\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dspy/predict/predict.py:92\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     signature \u001b[38;5;241m=\u001b[39m dsp\u001b[38;5;241m.\u001b[39mTemplate(signature\u001b[38;5;241m.\u001b[39minstructions, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_signature)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     x, C \u001b[38;5;241m=\u001b[39m \u001b[43mdsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcontext(lm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm, query_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;66;03m# print(f\"using lm = {self.lm} !\")\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dsp/primitives/predict.py:78\u001b[0m, in \u001b[0;36m_generate.<locals>.do_generate\u001b[0;34m(example, stage, max_depth, original_example)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Generate and extract the fields.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m prompt \u001b[38;5;241m=\u001b[39m template(example)\n\u001b[0;32m---> 78\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m completions: \u001b[38;5;28mlist\u001b[39m[Example] \u001b[38;5;241m=\u001b[39m [template\u001b[38;5;241m.\u001b[39mextract(example, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m completions]\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Find the completions that are most complete.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dsp/modules/gpt3.py:190\u001b[0m, in \u001b[0;36mGPT3.__call__\u001b[0;34m(self, prompt, only_completed, return_sorted, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m return_sorted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# if kwargs.get(\"n\", 1) > 1:\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m#     if self.model_type == \"chat\":\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m#         kwargs = {**kwargs}\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m#     else:\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m#         kwargs = {**kwargs, \"logprobs\": 5}\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dsp\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mlog_openai_usage:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_usage(response)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/backoff/_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    102\u001b[0m }\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     max_tries_exceeded \u001b[38;5;241m=\u001b[39m (tries \u001b[38;5;241m==\u001b[39m max_tries_value)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dsp/modules/gpt3.py:156\u001b[0m, in \u001b[0;36mGPT3.request\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dsp/modules/gpt3.py:129\u001b[0m, in \u001b[0;36mGPT3.basic_request\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}]\n\u001b[1;32m    128\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(kwargs)}\n\u001b[0;32m--> 129\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_request\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    132\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dsp/modules/gpt3.py:272\u001b[0m, in \u001b[0;36mchat_request\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m OPENAI_LEGACY:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cached_gpt3_turbo_request_v2_wrapped(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv1_cached_gpt3_turbo_request_v2_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dsp/modules/cache_utils.py:17\u001b[0m, in \u001b[0;36mnoop_decorator.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dsp/modules/gpt3.py:264\u001b[0m, in \u001b[0;36mv1_cached_gpt3_turbo_request_v2_wrapped\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m cache_turn_on \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;129m@NotebookCacheMemory\u001b[39m\u001b[38;5;241m.\u001b[39mcache\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mv1_cached_gpt3_turbo_request_v2_wrapped\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv1_cached_gpt3_turbo_request_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/memory.py:655\u001b[0m, in \u001b[0;36mMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/memory.py:598\u001b[0m, in \u001b[0;36mMemorizedFunc._cached_call\u001b[0;34m(self, args, kwargs, shelving)\u001b[0m\n\u001b[1;32m    595\u001b[0m     must_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m must_call:\n\u001b[0;32m--> 598\u001b[0m     out, metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmmap_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    600\u001b[0m         \u001b[38;5;66;03m# Memmap the output at the first call to be consistent with\u001b[39;00m\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;66;03m# later calls\u001b[39;00m\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/joblib/memory.py:856\u001b[0m, in \u001b[0;36mMemorizedFunc.call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28mprint\u001b[39m(format_call(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, args, kwargs))\n\u001b[0;32m--> 856\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_backend\u001b[38;5;241m.\u001b[39mdump_item(\n\u001b[1;32m    858\u001b[0m     [func_id, args_id], output, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose)\n\u001b[1;32m    860\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/dsp/modules/gpt3.py:259\u001b[0m, in \u001b[0;36mv1_cached_gpt3_turbo_request_v2\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    258\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstringify_request\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py:271\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions.py:659\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    658\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1200\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1188\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1196\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1197\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1198\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1199\u001b[0m     )\n\u001b[0;32m-> 1200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:889\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    882\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    888\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:918\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    915\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 918\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    924\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:915\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    907\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    911\u001b[0m )\n\u001b[1;32m    913\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 915\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:943\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 943\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:980\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    978\u001b[0m     hook(request)\n\u001b[0;32m--> 980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:1016\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1013\u001b[0m     )\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1016\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1020\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:231\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    218\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    219\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    220\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    229\u001b[0m )\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 231\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    236\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    237\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    238\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    239\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    240\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    106\u001b[0m     (\n\u001b[1;32m    107\u001b[0m         http_version,\n\u001b[1;32m    108\u001b[0m         status,\n\u001b[1;32m    109\u001b[0m         reason_phrase,\n\u001b[1;32m    110\u001b[0m         headers,\n\u001b[0;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    113\u001b[0m         http_version,\n\u001b[1;32m    114\u001b[0m         status,\n\u001b[1;32m    115\u001b[0m         reason_phrase,\n\u001b[1;32m    116\u001b[0m         headers,\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     },\n\u001b[1;32m    128\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1296\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1169\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "import openai\n",
    "openai.api_key = \"sk-key\"\n",
    "\n",
    "class WriteQuery(dspy.Signature):\n",
    "    \"\"\"Write a query that this document would have the answer to.\"\"\"\n",
    "\n",
    "    document = dspy.InputField(desc=\"A document containing information.\") \n",
    "    query = dspy.OutputField(desc=\"A short question uniquely answered by the document.\")\n",
    "\n",
    "gpt4T = dspy.OpenAI(model='gpt-4-1106-preview', max_tokens=1000, model_type='chat')\n",
    "\n",
    "for idx, blog_chunk in enumerate(blogs[300:]):\n",
    "    if idx > 400: # only need 400\n",
    "        break\n",
    "    print(idx)\n",
    "    with dspy.context(lm=gpt4T):\n",
    "        llm_query = dspy.ChainOfThought(WriteQuery)(document=blog_chunk)\n",
    "    print(llm_query)\n",
    "    data_properties = {\n",
    "        \"content\": blog_chunk,\n",
    "        \"synthetic_query\": llm_query.query\n",
    "    }\n",
    "    print(f\"{data_properties}\\n\")\n",
    "    client.data_object.create(data_properties, \"Blogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is one example of the chain-of-thought module in DSPy. It is taking my initial signature (prompt) and putting the first blog chunk into the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Write a query that this document would have the answer to.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Document: A document containing information.\n",
      "Reasoning: Let's think step by step in order to ${produce the query}. We ...\n",
      "Query: A short question uniquely answered by the document.\n",
      "\n",
      "---\n",
      "\n",
      "Document: --- title: Combining LangChain and Weaviate slug: combining-langchain-and-weaviate authors: [erika] date: 2023-02-21 tags: ['integrations'] image: ./img/hero.png description: \"LangChain is one of the most exciting new tools in AI. It helps overcome many limitations of LLMs, such as hallucination and limited input lengths.\" --- ![Combining LangChain and Weaviate](./img/hero.png) Large Language Models (LLMs) have revolutionized the way we interact and communicate with computers. These machines can understand and generate human-like language on a massive scale. LLMs are a versatile tool that is seen in many applications like chatbots, content creation, and much more. Despite being a powerful tool, LLMs have the drawback of being too general.\n",
      "Reasoning: Let's think step by step in order to\u001b[32m produce the query. We need to identify the unique aspects of the document that would allow us to formulate a question that this document can answer. The document seems to focus on the combination of LangChain and Weaviate, mentioning the benefits of LangChain in overcoming limitations of LLMs such as hallucination and limited input lengths. It also provides a date, author, and tags related to integrations. Given this information, we can create a query that asks about the purpose of combining LangChain with Weaviate, as this is a specific topic that the document addresses.\n",
      "\n",
      "Query: What are the benefits of combining LangChain with Weaviate in the context of LLMs?\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with dspy.context(lm=gpt4T):\n",
    "    dspy.ChainOfThought(WriteQuery)(document=blogs[0]).query\n",
    "gpt4T.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the data from your Weaviate instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fine-tune the model, we need to export our data and upload it to Cohere's reranker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your data is out of Weaviate!\n",
      "Extracted 302 in 0.6464710235595703 seconds.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This example will show you how to get all of your data\n",
    "out of Weaviate and into a JSON file using the Cursor API!\n",
    "'''\n",
    "import json\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Step 1 - Get the UUID of the first object inserted into Weaviate\n",
    "\n",
    "get_first_object_weaviate_query = \"\"\"\n",
    "{\n",
    "  Get {\n",
    "    Blogs {\n",
    "      _additional {\n",
    "        id\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "results = client.query.raw(get_first_object_weaviate_query)\n",
    "uuid_cursor = results[\"data\"][\"Get\"][\"Blogs\"][0][\"_additional\"][\"id\"]\n",
    "\n",
    "# Step 2 - Get the Total Objects in Weaviate\n",
    "\n",
    "total_objs_query = \"\"\"\n",
    "{\n",
    "    Aggregate {\n",
    "        Blogs {\n",
    "            meta {\n",
    "                count\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "results = client.query.raw(total_objs_query)\n",
    "total_objects = results[\"data\"][\"Aggregate\"][\"Blogs\"][0][\"meta\"][\"count\"]\n",
    "\n",
    "# Step 3 - Iterate through Weaviate with the Cursor\n",
    "increment = 50\n",
    "data = []\n",
    "for i in range(0, total_objects, increment):\n",
    "    results = (\n",
    "        client.query.get(\"Blogs\", [\"content\", \"synthetic_query\"])\n",
    "        .with_additional([\"id\"])\n",
    "        .with_limit(50)\n",
    "        .with_after(uuid_cursor)\n",
    "        .do()\n",
    "    )[\"data\"][\"Get\"][\"Blogs\"]\n",
    "    # extract data from result into JSON\n",
    "    for result in results:\n",
    "        if len(result[\"synthetic_query\"]) < 5:\n",
    "            continue\n",
    "        new_obj = {}\n",
    "        for key in result.keys():\n",
    "            if key == \"_additional\":\n",
    "                continue\n",
    "            if key == \"synthetic_query\":\n",
    "                new_obj[\"query\"] = result[key]\n",
    "            if key == \"content\":\n",
    "                new_obj[\"relevant_passages\"] = [result[key]]\n",
    "        data.append(new_obj)\n",
    "    # update uuid cursor to continue the loop\n",
    "    # we have just exited a loop where result holds the last obj\n",
    "    uuid_cursor = result[\"_additional\"][\"id\"]\n",
    "\n",
    "# save JSON\n",
    "file_path = \"my_data.jsonl\"\n",
    "with open(file_path, 'w') as jsonl_file:\n",
    "    for item in data:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"Your data is out of Weaviate!\")\n",
    "print(f\"Extracted {total_objects} in {time.time() - start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your data is out of Weaviate!\n",
      "Extracted objects from 303 to 405 in 0.21405792236328125 seconds.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Grab objects 303 to 405 for the validation set\n",
    "'''\n",
    "import weaviate\n",
    "import json\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# Step 1 - Get the UUID of the first object inserted into Weaviate\n",
    "\n",
    "get_first_object_weaviate_query = \"\"\"\n",
    "{\n",
    "  Get {\n",
    "    Blogs {\n",
    "      _additional {\n",
    "        id\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "results = client.query.raw(get_first_object_weaviate_query)\n",
    "uuid_cursor = results[\"data\"][\"Get\"][\"Blogs\"][0][\"_additional\"][\"id\"]\n",
    "\n",
    "# Step 2 - Get the Total Objects in Weaviate\n",
    "\n",
    "total_objs_query = \"\"\"\n",
    "{\n",
    "    Aggregate {\n",
    "        Blogs {\n",
    "            meta {\n",
    "                count\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "results = client.query.raw(total_objs_query)\n",
    "total_objects = results[\"data\"][\"Aggregate\"][\"Blogs\"][0][\"meta\"][\"count\"]\n",
    "\n",
    "# Step 3 - Iterate through Weaviate with the Cursor\n",
    "start_object_index = 302\n",
    "end_object_index = 405    \n",
    "increment = 50            \n",
    "data = []\n",
    "\n",
    "first_batch_size = min(increment, end_object_index - start_object_index + 1)\n",
    "\n",
    "for i in range(start_object_index, end_object_index, increment):\n",
    "    # Adjust the limit based on the range still needed to cover\n",
    "    current_batch_size = min(increment, end_object_index - i + 1)\n",
    "    \n",
    "    results = (\n",
    "        client.query.get(\"Blogs\", [\"content\", \"synthetic_query\"])\n",
    "        .with_additional([\"id\"])\n",
    "        .with_limit(current_batch_size)\n",
    "        .with_after(uuid_cursor)\n",
    "        .do()\n",
    "    )[\"data\"][\"Get\"][\"Blogs\"]\n",
    "    # Extract data from result into JSON\n",
    "    for result in results:\n",
    "        if len(result[\"synthetic_query\"]) < 5:\n",
    "            continue\n",
    "        new_obj = {}\n",
    "        for key in result.keys():\n",
    "            if key == \"_additional\":\n",
    "                continue\n",
    "            if key == \"synthetic_query\":\n",
    "                new_obj[\"query\"] = result[key]\n",
    "            if key == \"content\":\n",
    "                new_obj[\"relevant_passages\"] = [result[key]]\n",
    "        data.append(new_obj)\n",
    "        uuid_cursor = result[\"_additional\"][\"id\"]\n",
    "\n",
    "# save JSON\n",
    "file_path = \"validation.jsonl\"\n",
    "with open(file_path, 'w') as jsonl_file:\n",
    "    for item in data:\n",
    "        jsonl_file.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"Your data is out of Weaviate!\")\n",
    "print(f\"Extracted objects from 303 to 405 in {time.time() - start} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Index Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use our fine-tuned reranker, we will need to upload our data again to a new collection and add the `model_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New collection with the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "   \"classes\": [\n",
    "       {\n",
    "           \"class\": \"BlogsFineTuned\",\n",
    "           \"description\": \"Weaviate blogs\",\n",
    "           \"vectorizer\": \"text2vec-cohere\",\n",
    "           \"moduleConfig\": {\n",
    "                \"reranker-cohere\": {\n",
    "                    \"model\": \"model_id\" # grab the model_id from Cohere\n",
    "                }\n",
    "           },\n",
    "           \"properties\": [\n",
    "               {\n",
    "                   \"name\": \"content\",\n",
    "                   \"dataType\": [\"text\"],\n",
    "                   \"description\": \"Content from the blogs.\",\n",
    "               }\n",
    "           ]\n",
    "       }      \n",
    "   ]\n",
    "}\n",
    "    \n",
    "client.schema.create(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "        \"class\": \"BlogsFineTuned\",\n",
    "        \"description\": \"Weaviate blogs\",\n",
    "        \"vectorizer\": \"text2vec-cohere\",\n",
    "        \"moduleConfig\": {\n",
    "            \"reranker-cohere\": {\n",
    "                \"model\": \"reranker model path\"\n",
    "        }\n",
    "    }\n",
    "}      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data (same as above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for blog in blogs:\n",
    "    data_properties = {\n",
    "        \"content\": blog\n",
    "    }\n",
    "    client.data_object.create(\n",
    "        data_object = data_properties,\n",
    "        class_name = \"BlogsFineTuned\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
