{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "\n",
    "client = weaviate.Client(\n",
    "    embedded_options=weaviate.embedded.EmbeddedOptions(),\n",
    "    additional_headers={\n",
    "        'X-Cohere-Api-Key': \"sk-key\"\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema was created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"action\":\"hnsw_vector_cache_prefill\",\"count\":1000,\"index_id\":\"blogpost_38MqzEOUwMGM\",\"level\":\"info\",\"limit\":1000000000000,\"msg\":\"prefilled vector cache\",\"time\":\"2023-08-23T16:42:09-04:00\",\"took\":133000}\n"
     ]
    }
   ],
   "source": [
    "# resetting the schema. CAUTION: THIS WILL DELETE YOUR DATA \n",
    "client.schema.delete_all()\n",
    "\n",
    "schema = {\n",
    "   \"classes\": [\n",
    "       {\n",
    "           \"class\": \"BlogPost\",\n",
    "           \"description\": \"Blog post from the Weaviate website.\",\n",
    "           \"vectorizer\": \"text2vec-cohere\",\n",
    "           \"vectorIndexConfig\": {\n",
    "            \"distance\": \"dot\"\n",
    "           },\n",
    "           \"moduleConfig\": {\n",
    "               \"reranker-cohere\": { \n",
    "                    \"model\": \"rerank-multilingual-v2.0\"\n",
    "                }\n",
    "           },\n",
    "           \"properties\": [\n",
    "               {\n",
    "                  \"name\": \"Content\",\n",
    "                  \"dataType\": [\"text\"],\n",
    "                  \"description\": \"Content from the blog post\",\n",
    "               },\n",
    "               {\n",
    "                \"name\": \"URL\",\n",
    "                \"dataType\": [\"text\"],\n",
    "                \"description\": \"Title of the blog post\"\n",
    "               }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "client.schema.delete_all()\n",
    "\n",
    "client.schema.create(schema)\n",
    "\n",
    "print(\"Schema was created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs = ['../ranking/data/ranking-models.mdx', '../ranking/data/ref2vec-centroid.mdx']\n",
    "\n",
    "data = {}\n",
    "\n",
    "# Loop through each file path and read the file\n",
    "for blog in blogs:\n",
    "    with open(blog, 'r') as file:\n",
    "        data[blog] = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm manually chunking up the document into smaller chunks. This results in the chunks being a bit messy, but this can be improved by using an external tool like LlamaIndex, Haystack, LangChain, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with client.batch as batch:\n",
    "    for source in data.keys():\n",
    "        for i in range(0,len(data[source]), 500):\n",
    "            properties = {\n",
    "                \"source\": source,\n",
    "                \"content\": data[source][i:i+500]\n",
    "            }\n",
    "            client.batch.add_data_object(\n",
    "                properties,\n",
    "                class_name=\"BlogPost\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query without reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'Get': {'BlogPost': [{'content': 's to prompt it with:\\n`please output a relevance score on a scale of 1 to 100.`\\n\\nI think the second strategy is a bit more interesting, in which we put as many documents as we can in the input and ask the LLM to rank them. The key to making this work is the emergence of LLMs to follow instructions, especially with formatting their output. By prompting this ranking with “please output the ranking as a dictionary of IDs with the key equal to the rank and the value equal to the document id”. Also in'},\n",
       "    {'content': \"from the User's references to other vectors. And as the set of references continues to evolve, the Ref2Vec vectors will continue to evolve also, ensuring that the User vector remains up-to-date with their latest interests.\\n\\nWhether your goal is to construct a Home Feed interface for users or to pair with search queries, Ref2Vec provides a strong foundation to enable Recommendation with fairly low overhead. For example, it can achieve personalized re-ranking, also known as a session-based recomme\"},\n",
       "    {'content': '\\ntitle: Ranking Models for Better Search\\n\\nWhether searching to present information to a human, or a large language model, quality matters. One of the low hanging fruit strategies to improve search quality are ranking models. Broadly speaking, ranking models describe taking the query and each candidate document, one-by-one, as input to predict relevance. This is different from vector and lexical search where representations are computed offline and indexed for speed. Back in August, we [published'},\n",
       "    {'content': 'ask. So if a User searches for \"adidas  shoes for the summer\", not only does Weaviate need to find these particular kinds of shoes, but it also needs to rank them based on relevance to the particular user\\'s interests!\\n\\nWith Ref2Vec, this task is made easier by representing a user\\'s interests by drawing a graph of cross-references from the user to objects the user has engaged with. This gives Weaviate unique points of reference for each user that can be used to rank the search results.\\n\\nIn Weavia'},\n",
       "    {'content': 'r apps. Applying a ranking model to hybrid search results is a promising approach to keep pushing the frontier of zero-shot AI.\\n\\nImagine we want to retrieve information about the Weaviate Ref2Vec feature. If our application is using the Cohere embedding model, it has never seen this term or concept. Luckily, hybrid search comes to the rescue by combining the contextual semantics from the vector search and the keyword matching from the BM25 scoring. If the query is: `How can I use ref2vec to buil'},\n",
       "    {'content': 'ndation, without persisting user data over a long sequence of interactions. A new user could have personalization available after a few interactions on the app which will help them quickly settle in and feel at home, helping to overcome what is known as the cold-start problem.\\n\\n\\n## Representing long objects\\n\\nOne of the most outstanding problems in Search technology is finding suitable representations for long objects. In this sense, \"long\" is used to describe text documents that significantly ex'},\n",
       "    {'content': '– **Relational Structure**.\\n\\nRef2Vec-Centroid goes some way to harness the joint power of vector search **combined with** relational structure, by making it easier to derive object vectors from relationships. As you have seen above, we think Ref2Vec can add value for use cases such as recommendations, re-ranking, overcoming the cold start problem and representing long objects. We are also excited to see what you build with Ref2Vec, and excited to build on this module with its future iterations.\\n'},\n",
       "    {'content': 'hell out of that benchmark and it moves science forward… [ truncated for visibility ] |\\n| Hybrid Only            | Or, at least being able to ask follow up questions when it’s unclear about and that’s surprisingly not that difficult to do with these current systems, as long as you’re halfway decent at prompting, you can build up these follow up systems and train them over the course of a couple 1,000 examples to perform really, really well, at least to cove r90, 95% of questions that you might g'},\n",
       "    {'content': 'also power high-quality, lightning-fast recommendations. This is because Recommendation is a very similar task to Search from the perspective of a vector database. Both tasks leverage the ANN index of vector representations to search for a suitable object. The key difference is that in Search, relevance is typically contained entirely within the query. In Recommendation, relevance is additionally dependent on the user, making the query a subjective, user-dependent task rather than an objective t'},\n",
       "    {'content': 'candidate documents to rank with. These kinds of models are increasingly being used as guardrails for generative models. For example, a harmful or NSFW content detector can prevent these generations from making it through the search pipeline. An interesting idea I recently heard from Eddie Zhou on Jerry Liu’s Llama Index Fireside Chat is the idea of using Natural Language Inference models to prevent hallucination by predicting the entailment or contradiction taking as the [retrieved context, gen'}]}}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "{\n",
    "  Get {\n",
    "    BlogPost (\n",
    "      nearText: {\n",
    "        concepts: \"Low hanging fruit to improve relevance\"\n",
    "      }\n",
    "      limit: 10)\n",
    "     {\n",
    "      content\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "client.query.raw(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The first few results from the above query aren't exactly what we're looking for. Let's run the query again, but rerank the top 10 documents with the text in the content property. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query with Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'Get': {'BlogPost': [{'_additional': {'rerank': [{'score': 0.99982184}]},\n",
       "     'content': '\\ntitle: Ranking Models for Better Search\\n\\nWhether searching to present information to a human, or a large language model, quality matters. One of the low hanging fruit strategies to improve search quality are ranking models. Broadly speaking, ranking models describe taking the query and each candidate document, one-by-one, as input to predict relevance. This is different from vector and lexical search where representations are computed offline and indexed for speed. Back in August, we [published'},\n",
       "    {'_additional': {'rerank': [{'score': 3.0959105e-05}]},\n",
       "     'content': 's to prompt it with:\\n`please output a relevance score on a scale of 1 to 100.`\\n\\nI think the second strategy is a bit more interesting, in which we put as many documents as we can in the input and ask the LLM to rank them. The key to making this work is the emergence of LLMs to follow instructions, especially with formatting their output. By prompting this ranking with “please output the ranking as a dictionary of IDs with the key equal to the rank and the value equal to the document id”. Also in'},\n",
       "    {'_additional': {'rerank': [{'score': 1.9947297e-06}]},\n",
       "     'content': \"from the User's references to other vectors. And as the set of references continues to evolve, the Ref2Vec vectors will continue to evolve also, ensuring that the User vector remains up-to-date with their latest interests.\\n\\nWhether your goal is to construct a Home Feed interface for users or to pair with search queries, Ref2Vec provides a strong foundation to enable Recommendation with fairly low overhead. For example, it can achieve personalized re-ranking, also known as a session-based recomme\"},\n",
       "    {'_additional': {'rerank': [{'score': 1.0511496e-06}]},\n",
       "     'content': 'hell out of that benchmark and it moves science forward… [ truncated for visibility ] |\\n| Hybrid Only            | Or, at least being able to ask follow up questions when it’s unclear about and that’s surprisingly not that difficult to do with these current systems, as long as you’re halfway decent at prompting, you can build up these follow up systems and train them over the course of a couple 1,000 examples to perform really, really well, at least to cove r90, 95% of questions that you might g'},\n",
       "    {'_additional': {'rerank': [{'score': 6.179393e-07}]},\n",
       "     'content': 'also power high-quality, lightning-fast recommendations. This is because Recommendation is a very similar task to Search from the perspective of a vector database. Both tasks leverage the ANN index of vector representations to search for a suitable object. The key difference is that in Search, relevance is typically contained entirely within the query. In Recommendation, relevance is additionally dependent on the user, making the query a subjective, user-dependent task rather than an objective t'},\n",
       "    {'_additional': {'rerank': [{'score': 5.896418e-07}]},\n",
       "     'content': 'ask. So if a User searches for \"adidas  shoes for the summer\", not only does Weaviate need to find these particular kinds of shoes, but it also needs to rank them based on relevance to the particular user\\'s interests!\\n\\nWith Ref2Vec, this task is made easier by representing a user\\'s interests by drawing a graph of cross-references from the user to objects the user has engaged with. This gives Weaviate unique points of reference for each user that can be used to rank the search results.\\n\\nIn Weavia'},\n",
       "    {'_additional': {'rerank': [{'score': 5.8505316e-07}]},\n",
       "     'content': 'ndation, without persisting user data over a long sequence of interactions. A new user could have personalization available after a few interactions on the app which will help them quickly settle in and feel at home, helping to overcome what is known as the cold-start problem.\\n\\n\\n## Representing long objects\\n\\nOne of the most outstanding problems in Search technology is finding suitable representations for long objects. In this sense, \"long\" is used to describe text documents that significantly ex'},\n",
       "    {'_additional': {'rerank': [{'score': 5.5391723e-07}]},\n",
       "     'content': 'candidate documents to rank with. These kinds of models are increasingly being used as guardrails for generative models. For example, a harmful or NSFW content detector can prevent these generations from making it through the search pipeline. An interesting idea I recently heard from Eddie Zhou on Jerry Liu’s Llama Index Fireside Chat is the idea of using Natural Language Inference models to prevent hallucination by predicting the entailment or contradiction taking as the [retrieved context, gen'},\n",
       "    {'_additional': {'rerank': [{'score': 4.4162138e-07}]},\n",
       "     'content': '– **Relational Structure**.\\n\\nRef2Vec-Centroid goes some way to harness the joint power of vector search **combined with** relational structure, by making it easier to derive object vectors from relationships. As you have seen above, we think Ref2Vec can add value for use cases such as recommendations, re-ranking, overcoming the cold start problem and representing long objects. We are also excited to see what you build with Ref2Vec, and excited to build on this module with its future iterations.\\n'},\n",
       "    {'_additional': {'rerank': [{'score': 4.0525455e-07}]},\n",
       "     'content': 'r apps. Applying a ranking model to hybrid search results is a promising approach to keep pushing the frontier of zero-shot AI.\\n\\nImagine we want to retrieve information about the Weaviate Ref2Vec feature. If our application is using the Cohere embedding model, it has never seen this term or concept. Luckily, hybrid search comes to the rescue by combining the contextual semantics from the vector search and the keyword matching from the BM25 scoring. If the query is: `How can I use ref2vec to buil'}]}}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "{\n",
    "  Get {\n",
    "    BlogPost (\n",
    "      nearText: {\n",
    "        concepts: \"Low hanging fruit to improve relevance\"\n",
    "      },\n",
    "      limit: 10)\n",
    "     {\n",
    "      content\n",
    "      _additional {\n",
    "        rerank(\n",
    "            property: \"content\",\n",
    "            query: \"Low hanging fruit to improve relevance\"\n",
    "        ){\n",
    "          score\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "client.query.raw(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
