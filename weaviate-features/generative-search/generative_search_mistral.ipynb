{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the generative Mistral demo ðŸ¥³\n",
    "\n",
    "In this demo, we will use Cohere to generate the embeddings for the blog post and use Mistral's language model to generate content!\n",
    "\n",
    "What you will need for this demo:\n",
    "1. A Weaviate cluster (more info below)\n",
    "2. Cohere API key \n",
    "3. Mistral API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install weaviate-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Weaviate Cluster\n",
    "\n",
    "You can create a free 14-day sandbox on [WCS](https://console.weaviate.cloud/signin), or learn about our other installation options [here](https://weaviate.io/developers/weaviate/installation)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import os\n",
    "\n",
    "# Connect to a WCS instance\n",
    "client = weaviate.connect_to_wcs(\n",
    "    cluster_url=(\"cluster-url\"),  # URL to your sandbox\n",
    "    auth_credentials=weaviate.auth.AuthApiKey(\"auth-key\"), # api key to your sandbox if you set up auth\n",
    "    headers= {'X-Cohere-API-Key': \"key\", #cohere key\n",
    "    'X-Mistral-Api-key': \"key\" # mistral key\n",
    "   }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION: This will delete your schema and the objects!\n",
    "# only run the below if you need to reset your schema\n",
    "\n",
    "# client.collections.delete_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate.classes.config as wvcc\n",
    "from weaviate.classes.config import Property, DataType\n",
    "\n",
    "\n",
    "collection = client.collections.create(\n",
    "    name=\"BlogChunks\", # name of the collection\n",
    "    vectorizer_config=wvcc.Configure.Vectorizer.text2vec_cohere\n",
    "    (\n",
    "        model=\"embed-multilingual-v3.0\" # Cohere model (we support other models in Weaviate)\n",
    "    ),\n",
    "    generative_config=wvcc.Configure.Generative.mistral(\n",
    "        model=\"open-mistral-7b\" # Mistral model (we support other models in Weaviate)\n",
    "    ),\n",
    "    properties=[\n",
    "            Property(name=\"content\", data_type=DataType.TEXT) # We only have one property for our collection. It is the content within the blog posts\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk and Import Data\n",
    "\n",
    "We need to break our blog posts into smaller chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Break a list into chunks of the specified size.\"\"\"\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentences using regular expressions.\"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "def read_and_chunk_index_files(main_folder_path):\n",
    "    \"\"\"Read index.md files from subfolders, split into sentences, and chunk every 5 sentences.\"\"\"\n",
    "    blog_chunks = []\n",
    "    \n",
    "    for file_path in os.listdir(\"./data\"):\n",
    "        index_file_path = os.path.join(\"./data\", file_path)\n",
    "        with open(index_file_path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            sentences = split_into_sentences(content)\n",
    "            sentence_chunks = chunk_list(sentences, 5)\n",
    "            sentence_chunks = [' '.join(chunk) for chunk in sentence_chunks]\n",
    "            blog_chunks.extend(sentence_chunks)\n",
    "    return blog_chunks\n",
    "\n",
    "# run with:\n",
    "main_folder_path = './data'\n",
    "blog_chunks = read_and_chunk_index_files(main_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First chunk\n",
    "\n",
    "blog_chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the objects (chunks) into the Weaviate cluster\n",
    "\n",
    "from weaviate.util import get_valid_uuid\n",
    "from uuid import uuid4\n",
    "blogs = client.collections.get(\"BlogChunks\")\n",
    "\n",
    "for blog_chunk in blog_chunks:\n",
    "    random_uuid = get_valid_uuid(uuid4())\n",
    "    blogs.data.insert(\n",
    "        properties={\n",
    "            \"content\": blog_chunk\n",
    "        },\n",
    "        uuid=random_uuid\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Search Query\n",
    "\n",
    "Hybrid search combines BM25 and vector search and weighs the two algorithms depending on the `alpha` parameter. \n",
    "\n",
    "`alpha`= 0 --> pure BM25\n",
    "\n",
    "`alpha`= 0.5 --> half BM25, half vector search\n",
    "\n",
    "`alpha`= 1 --> pure vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "blogs = client.collections.get(\"BlogChunks\")\n",
    "\n",
    "response = blogs.query.hybrid(\n",
    "    query=\"What is Ref2Vec\",\n",
    "    alpha=0.5,\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "for o in response.objects:\n",
    "    print(json.dumps(o.properties, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Search Query\n",
    "\n",
    "Here is what happens in the below:\n",
    "1. We will retrieve 3 relevant chunks from our vector database (powered by the Cohere embeddings)\n",
    "2. We will pass the 3 chunks to Mistral to generate the short paragraph about Ref2Vec\n",
    "\n",
    "The first line in the output is the generated text, and the `content` pieces below it, are what was retrieved from Weaviate and passed to Mistral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs = client.collections.get(\"BlogChunks\")\n",
    "\n",
    "\n",
    "response = blogs.generate.near_text(\n",
    "    query=\"What is Ref2Vec?\",\n",
    "    single_prompt=\"Write a short paragraph about ref2vec with this content: {content}\",\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "\n",
    "for o in response.objects:\n",
    "    print(o.generated)\n",
    "    print(json.dumps(o.properties, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
