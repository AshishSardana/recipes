{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b7e219b-8ff1-4d73-9d3d-85a91c809973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hello! It's nice to meet you. Is there something I can help you with, or would you like to chat?\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to Ollama running Llama3\n",
    "import dspy\n",
    "llama3_ollama = dspy.OllamaLocal(model=\"llama3:instruct\", max_tokens=4000, timeout_s=480)\n",
    "\n",
    "dspy.settings.configure(lm=llama3_ollama)\n",
    "\n",
    "llama3_ollama(\"say hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b621b7da-62e7-4ade-be64-5869d272f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load blogs into Weaviate\n",
    "import weaviate\n",
    "\n",
    "weaviate_client = weaviate.connect_to_local()\n",
    "\n",
    "weaviate_client.collections.delete(\"WeaviateBlogChunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fbd8c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1182\n",
      "\n",
      "---\n",
      "title: Combining LangChain and Weaviate\n",
      "slug: combining-langchain-and-weaviate\n",
      "authors: [erika]\n",
      "date: 2023-02-21\n",
      "tags: ['integrations']\n",
      "image: ./img/hero.png\n",
      "description: \"LangChain is one of the most exciting new tools in AI. It helps overcome many limitations of LLMs, such as hallucination and limited input lengths.\"\n",
      "---\n",
      "![Combining LangChain and Weaviate](./img/hero.png)\n",
      "\n",
      "Large Language Models (LLMs) have revolutionized the way we interact and communicate with computers. These machines can understand and generate human-like language on a massive scale. LLMs are a versatile tool that is seen in many applications like chatbots, content creation, and much more. Despite being a powerful tool, LLMs have the drawback of being too general.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Break a list into chunks of the specified size.\"\"\"\n",
    "    return [lst[i:i + chunk_size] for i in range(0, len(lst), chunk_size)]\n",
    "\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Split text into sentences using regular expressions.\"\"\"\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "\n",
    "def read_and_chunk_index_files(main_folder_path):\n",
    "    \"\"\"Read index.md files from subfolders, split into sentences, and chunk every 5 sentences.\"\"\"\n",
    "    blog_chunks = []\n",
    "    for folder_name in os.listdir(main_folder_path):\n",
    "        subfolder_path = os.path.join(main_folder_path, folder_name)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            index_file_path = os.path.join(subfolder_path, 'index.mdx')\n",
    "            if os.path.isfile(index_file_path):\n",
    "                with open(index_file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    sentences = split_into_sentences(content)\n",
    "                    sentence_chunks = chunk_list(sentences, 5)\n",
    "                    sentence_chunks = [' '.join(chunk) for chunk in sentence_chunks]\n",
    "                    blog_chunks.extend(sentence_chunks)\n",
    "    return blog_chunks\n",
    "\n",
    "\n",
    "# Example usage\n",
    "main_folder_path = '../datasets/weaviate-blogs'\n",
    "blog_chunks = read_and_chunk_index_files(main_folder_path)\n",
    "\n",
    "\n",
    "print(f\"{len(blog_chunks)}\\n\")\n",
    "print(blog_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49acea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate.classes.config as wvcc\n",
    "\n",
    "collection = weaviate_client.collections.create(\n",
    "   name=\"WeaviateBlogChunk\",\n",
    "   vectorizer_config=wvcc.Configure.Vectorizer.text2vec_ollama\n",
    "   (\n",
    "       api_endpoint=\"http://host.docker.internal:11434\",\n",
    "       model=\"snowflake-arctic-embed:335m\"\n",
    "   ),\n",
    "   properties=[\n",
    "           wvcc.Property(name=\"content\", data_type=wvcc.DataType.TEXT),\n",
    "           wvcc.Property(name=\"query\", data_type=wvcc.DataType.TEXT, skip_vectorization=True),\n",
    "           wvcc.Property(name=\"is_high_quality_query\", data_type=wvcc.DataType.BOOL),\n",
    "     ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d90eadec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded and vectorized 1182 blog chunks in 110.77285599708557 seconds.\n"
     ]
    }
   ],
   "source": [
    "from weaviate.util import get_valid_uuid\n",
    "from uuid import uuid4\n",
    "import time\n",
    "\n",
    "blogs = weaviate_client.collections.get(\"WeaviateBlogChunk\")\n",
    "\n",
    "blog_chunk_uuids = []\n",
    "\n",
    "start = time.time()\n",
    "for idx, blog_chunk in enumerate(blog_chunks):\n",
    "    id = get_valid_uuid(uuid4())\n",
    "    blog_chunk_uuids.append(id)\n",
    "    upload = blogs.data.insert(\n",
    "        properties={\n",
    "           \"content\": blog_chunk\n",
    "        },\n",
    "        uuid=id\n",
    "    )\n",
    "\n",
    "print(f\"Uploaded and vectorized {len(blog_chunks)} blog chunks in {time.time() - start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1760ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Any\n",
    "import functools\n",
    "\n",
    "class UpdateProperty(dspy.Signature):\n",
    "    \"\"\"I need your help to generate the value of a property by following the instruction using the provided name-value property references. VERY IMPORTANT!! Please follow this next instruction carefully. It is EXTREMELY IMPORTANT that you only output the property value and nothing else. Do not start your response with something like `Sure, I can help with that!` or anything of the sort. JUST OUTPUT THE PROPERTY VALUE!!\n",
    "    \"\"\"\n",
    "\n",
    "    property_name = dspy.InputField(\n",
    "        desc=\"The name of the property that you should update.\"\n",
    "    )\n",
    "    references = dspy.InputField(\n",
    "        desc=\"The name-value property pairs that you should refer to while updating the property.\"\n",
    "    )\n",
    "    instruction = dspy.InputField(\n",
    "        desc=\"The prompt to use when generating the updated property value.\"\n",
    "    )\n",
    "    property_value = dspy.OutputField(\n",
    "        desc=\"The value of the updated property. VERY IMPORTANT!! ONLY OUTPUT THIS VALUE!! Do not output anything other than this value.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Program(dspy.Module):\n",
    "    def __init__(self, property_value_type: Any) -> None:\n",
    "        self.property_value_type = property_value_type\n",
    "        self.update_property = dspy.Predict(UpdateProperty)\n",
    " \n",
    "    def assert_property_value_type(self, property_value: str) -> bool:\n",
    "        try:\n",
    "            self.property_value_type(property_value)\n",
    "            return True\n",
    "        except (ValueError, TypeError):\n",
    "            return False\n",
    "\n",
    "\n",
    "    def failed_assertion_message(self, property_name: str) -> str:\n",
    "        return f\"\"\"\n",
    "        The value of the '{property_name}' property does not match the expected type: {self.property_value_type}.\n",
    "        Please ensure that the generated value adheres to the specified type.\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, property_name: str, references: str, instruction: str) -> Any:\n",
    "        prediction: dspy.Prediction = self.update_property(\n",
    "            property_name=property_name, references=references, instruction=instruction\n",
    "        )\n",
    "\n",
    "        dspy.Suggest(\n",
    "            self.assert_property_value_type(prediction.property_value),\n",
    "            self.failed_assertion_message(property_name),\n",
    "        )\n",
    "        \n",
    "        if self.property_value_type == bool:\n",
    "            return prediction.property_value.lower() == \"true\"\n",
    "        \n",
    "        return self.property_value_type(prediction.property_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49906dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "from dspy.primitives.assertions import assert_transform_module, backtrack_handler\n",
    "\n",
    "\n",
    "program = Program(property_value_type=int)\n",
    "program_with_assertions = assert_transform_module(\n",
    "   program, functools.partial(backtrack_handler, max_backtracks=1)\n",
    ")\n",
    "\n",
    "\n",
    "property_name = \"age\"\n",
    "references = \"name: John, occupation: Engineer\"\n",
    "instruction = \"Update the 'age' property to a random integer between 25 and 35.\"\n",
    "\n",
    "\n",
    "result = program_with_assertions(\n",
    "   property_name=property_name, references=references, instruction=instruction\n",
    ")\n",
    "print(result)\n",
    "print(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "925c30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "program = Program(property_value_type=str)\n",
    "\n",
    "instruction = \"\"\"\n",
    "Given a snippet from a blog post published by Weaviate, a Vector Database company, construct a question that delves deeply into the underlying concepts and explores new dimensions beyond the provided information. \n",
    "VERY IMPORTANT!! These queries should emphasize the advantages of semantic search with vector embeddings over traditional keyword-based methods like BM25. Ensure that the query explores related concepts, implications, or applications without directly repeating any keywords from the source document.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46bf6f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing some queries to illustrate what this is doing... \n",
      "\n",
      "What are the limitations of relying solely on LLMs for information retrieval and how can combining LangChain with Weaviate's vector embeddings overcome these limitations to provide more accurate and relevant search results?\n",
      "\n",
      "What are the limitations of LLMs and how do emerging technologies like LangChain help overcome them?\n",
      "\n",
      "What are the implications of combining sequential chains with Weaviate's vector embeddings for building more accurate and efficient LLM chatbots?\n",
      "\n",
      "What type of mammal lays the biggest eggs?\n",
      "\n",
      "What are the limitations of traditional keyword-based methods in processing and storing large sequences of tokens, and how do vector embeddings address these challenges?\n",
      "\n",
      "What are the underlying benefits of using Weaviate's vector database for semantic search, and how does it enable more accurate and relevant results compared to traditional keyword-based methods like BM25?\n",
      "\n",
      "What is the relationship between local memory in Refine and its ability to summarize documents one by one, and how does this technique enable more accurate semantic search results compared to traditional keyword-based methods?\n",
      "\n",
      "What are the implications of integrating semantic search with vector embeddings in Weaviate's Vector Database for enhancing information retrieval and recommendation systems?\n",
      "\n",
      "What is the relationship between tool use in language models and the potential for more accurate and relevant results in semantic search applications?\n",
      "\n",
      "What is the relationship between Weaviate's vector database and the concept of \"semantic search\" in the context of natural language processing? How do the vector embeddings used by Weaviate enable more accurate and relevant search results compared to traditional keyword-based methods like BM25, and what are some potential applications or implications of this technology in various industries?\n",
      "\n",
      "\n",
      "LOG: 100 queries generated in 232.20940709114075 seconds.\n",
      "\n",
      "\n",
      "LOG: 200 queries generated in 458.1437020301819 seconds.\n",
      "\n",
      "\n",
      "LOG: 300 queries generated in 708.5551929473877 seconds.\n",
      "\n",
      "\n",
      "LOG: 400 queries generated in 929.1951389312744 seconds.\n",
      "\n",
      "\n",
      "LOG: 500 queries generated in 1130.793534040451 seconds.\n",
      "\n",
      "\n",
      "LOG: 600 queries generated in 1346.9617178440094 seconds.\n",
      "\n",
      "\n",
      "LOG: 700 queries generated in 1573.8397841453552 seconds.\n",
      "\n",
      "\n",
      "LOG: 800 queries generated in 1798.2753992080688 seconds.\n",
      "\n",
      "\n",
      "LOG: 900 queries generated in 2057.152666091919 seconds.\n",
      "\n",
      "\n",
      "LOG: 1000 queries generated in 2279.9860739707947 seconds.\n",
      "\n",
      "\n",
      "LOG: 1100 queries generated in 2503.1281042099 seconds.\n",
      "\n",
      "1182 objects have been updated in 2728.35395693779 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "start_gfl = time.time()\n",
    "\n",
    "for idx, chunk_uuid in enumerate(blog_chunk_uuids):\n",
    "    if idx % 100 == 99:\n",
    "        print(f\"\\nLOG: {idx+1} queries generated in {time.time() - start_gfl} seconds.\\n\")\n",
    "  \n",
    "    # Get the object\n",
    "    obj = blogs.query.fetch_object_by_id(chunk_uuid, return_properties=\"content\")\n",
    "  \n",
    "    # Format the references\n",
    "    references = \" \".join(f\"{k}: {v}\" for k, v in obj.properties.items())\n",
    "  \n",
    "    # Run GFL\n",
    "    query = program(\n",
    "        property_name=\"query\",\n",
    "        references=references,\n",
    "        instruction=instruction,\n",
    "    )\n",
    "  \n",
    "    if idx < 10:\n",
    "        if idx == 0:\n",
    "            print(\"Printing some queries to illustrate what this is doing... \\n\")\n",
    "        print(f\"{query}\\n\")\n",
    "\n",
    "    # Update property in Weaviate\n",
    "    blogs.data.update(\n",
    "        properties={\n",
    "            \"query\": query\n",
    "        },\n",
    "        uuid=chunk_uuid\n",
    "    )\n",
    "    \n",
    "print(f\"{len(blog_chunk_uuids)} objects have been updated in {time.time() - start_gfl} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e1b025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing some queries to illustrate what this is doing... \n",
      "\n",
      "Is this a high quality query?\n",
      "What are the limitations of relying solely on LLMs for information retrieval and how can combining LangChain with Weaviate's vector embeddings overcome these limitations to provide more accurate and relevant search results?\n",
      "True\n",
      "\n",
      "Is this a high quality query?\n",
      "What are the limitations of LLMs and how do emerging technologies like LangChain help overcome them?\n",
      "True\n",
      "\n",
      "Is this a high quality query?\n",
      "What are the implications of combining sequential chains with Weaviate's vector embeddings for building more accurate and efficient LLM chatbots?\n",
      "True\n",
      "\n",
      "Is this a high quality query?\n",
      "What type of mammal lays the biggest eggs?\n",
      "False\n",
      "\n",
      "Is this a high quality query?\n",
      "What are the limitations of traditional keyword-based methods in processing and storing large sequences of tokens, and how do vector embeddings address these challenges?\n",
      "True\n",
      "\n",
      "Is this a high quality query?\n",
      "What are the underlying benefits of using Weaviate's vector database for semantic search, and how does it enable more accurate and relevant results compared to traditional keyword-based methods like BM25?\n",
      "True\n",
      "\n",
      "Is this a high quality query?\n",
      "What is the relationship between local memory in Refine and its ability to summarize documents one by one, and how does this technique enable more accurate semantic search results compared to traditional keyword-based methods?\n",
      "True\n",
      "\n",
      "Is this a high quality query?\n",
      "What are the implications of integrating semantic search with vector embeddings in Weaviate's Vector Database for enhancing information retrieval and recommendation systems?\n",
      "True\n",
      "\n",
      "Is this a high quality query?\n",
      "What is the relationship between tool use in language models and the potential for more accurate and relevant results in semantic search applications?\n",
      "True\n",
      "\n",
      "Is this a high quality query?\n",
      "What is the relationship between Weaviate's vector database and the concept of \"semantic search\" in the context of natural language processing? How do the vector embeddings used by Weaviate enable more accurate and relevant search results compared to traditional keyword-based methods like BM25, and what are some potential applications or implications of this technology in various industries?\n",
      "True\n",
      "\n",
      "\n",
      "LOG: 100 queries generated in 153.65433073043823 seconds.\n",
      "\n",
      "\n",
      "LOG: 200 queries generated in 290.9161078929901 seconds.\n",
      "\n",
      "\n",
      "LOG: 300 queries generated in 446.91282892227173 seconds.\n",
      "\n",
      "\n",
      "LOG: 400 queries generated in 581.5143098831177 seconds.\n",
      "\n",
      "\n",
      "LOG: 500 queries generated in 709.4699110984802 seconds.\n",
      "\n",
      "\n",
      "LOG: 600 queries generated in 845.3106598854065 seconds.\n",
      "\n",
      "\n",
      "LOG: 700 queries generated in 994.7649910449982 seconds.\n",
      "\n",
      "\n",
      "LOG: 800 queries generated in 1131.867527961731 seconds.\n",
      "\n",
      "\n",
      "LOG: 900 queries generated in 1306.1410517692566 seconds.\n",
      "\n",
      "\n",
      "LOG: 1000 queries generated in 1449.410636663437 seconds.\n",
      "\n",
      "\n",
      "LOG: 1100 queries generated in 1589.0269577503204 seconds.\n",
      "\n",
      "1182 objects have been updated in 1730.3037178516388 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "program = Program(property_value_type=bool)\n",
    "\n",
    "instruction = \"\"\"\n",
    "Critically examine the query. Output 'True' ONLY if ALL of the following conditions are met:\n",
    "\n",
    "The query directly asks about a technological concept (e.g., LLMs, AI, databases, programming).\n",
    "The query is not merely mentioned as an example in the content, but genuinely seeks information about technology.\n",
    "The query demonstrates an understanding of and engagement with the technological themes in the content.\n",
    "If ANY of these conditions are not met, output 'False'.\n",
    "\"\"\"\n",
    "start_gfl = time.time()\n",
    "\n",
    "for idx, chunk_uuid in enumerate(blog_chunk_uuids):\n",
    "    if idx % 100 == 99:\n",
    "        print(f\"\\nLOG: {idx+1} queries labeled in {time.time() - start_gfl} seconds.\\n\")\n",
    "  \n",
    "    # Get the object\n",
    "    obj = blogs.query.fetch_object_by_id(chunk_uuid, return_properties=[\"content\", \"query\"])\n",
    "  \n",
    "    # Format the references\n",
    "    references = \" \".join(f\"{k}: {v}\" for k, v in obj.properties.items())\n",
    "    \n",
    "    # Run GFL\n",
    "    is_high_quality_query = program(\n",
    "        property_name=\"is_high_quality_query\",\n",
    "        references=references,\n",
    "        instruction=instruction,\n",
    "    )\n",
    "  \n",
    "    if idx < 10:\n",
    "        if idx == 0:\n",
    "            print(\"Printing some queries to illustrate what this is doing... \\n\")\n",
    "        query = obj.properties[\"query\"]\n",
    "        print(f\"Is this a high quality query?\\n{query}\")\n",
    "        print(f\"{is_high_quality_query}\\n\")\n",
    "\n",
    "    # Update property in Weaviate\n",
    "    blogs.data.update(\n",
    "        properties={\n",
    "            \"is_high_quality_query\": is_high_quality_query\n",
    "        },\n",
    "        uuid=chunk_uuid\n",
    "    )\n",
    "    \n",
    "print(f\"{len(blog_chunk_uuids)} objects have been updated in {time.time() - start_gfl} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37b83f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dspy_dev)",
   "language": "python",
   "name": "dspy_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
